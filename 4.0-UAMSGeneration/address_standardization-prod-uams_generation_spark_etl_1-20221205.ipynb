{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session                                                                                                  |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X                                                                            |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0)                                |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |   Changes the session type to Glue ETL.                                                                                                                   |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer                       |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# I set my own magics (config for the job)\n%number_of_workers 20\n\n## built in AWS Spark & Glue libraries\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			},
			"execution_count": null,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.35 \nPrevious number of workers: 5\nSetting new number of workers to: 20\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::741993363917:role/AWSGlueServiceRole\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 20\nSession ID: 4c2c2e64-5a3e-4641-a572-8bc844132190\nApplying the following default arguments:\n--glue_kernel_version 0.35\n--enable-glue-datacatalog true\nWaiting for session 4c2c2e64-5a3e-4641-a572-8bc844132190 to get into ready status...\nSession 4c2c2e64-5a3e-4641-a572-8bc844132190 has been created\n\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### my import libraries\nfrom io import StringIO\nimport numpy as np\nimport pandas as pd\n# import awswrangler as wr\nimport glob\nimport re\n#import boto3\nregex_schema = \"/*.csv\"\nfrom string import printable\nst = set(printable)\nfrom datetime import datetime\n\nimport pyspark.sql.functions as f\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.functions import when\n\nfrom pyspark.sql.window import Window\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import StringType, IntegerType\n\npd.set_option('display.max_rows', 20)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\n## Setting run date & save path\nimport datetime\na = datetime.datetime.now()\n# date_key = a.strftime('%Y%m%d')\ndate_key = '20221125' # temporary\nprint(date_key)\n\n# UAMS_PySpark_save_path = 's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/' # old Qubole Zepp path\nUAMS_PySpark_save_path = 's3://astro-groupdata-prod-pipeline/address_standardization/spark_uams_generation/' ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "20221125\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## assign variable names and paths at this step to allow for easier changing\n# phase 1\nTM_P1MDU = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_mdu/UAMS_Format_stndrd_TM_P1_MDU20221014.csv'\nTM_P1SDU = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_sdu/UAMS_Format_stndrd_TM_P1_SDU20221014.csv'\nTM_P2SDU = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_sdu/UAMS_Format_stndrd_TM_P2_SDU20221014.csv'\n\nALLO_P1MDU = 's3://astro-groupdata-prod-pipeline/address_standardization/allo_uams_mdu/UAMS_Format_stndrd_Allo_P1_MDU.csv'\nALLO_P1SDU = 's3://astro-groupdata-prod-pipeline/address_standardization/allo_uams_sdu/UAMS_Format_stndrd_Allo_P1_SDU.csv'\nALLO_P2SDU = 's3://astro-groupdata-prod-pipeline/address_standardization/allo_uams_sdu/UAMS_Format_stndrd_Allo_P2_SDU.csv'\n\nMAXIS_P1MDU = 's3://astro-groupdata-prod-pipeline/address_standardization/maxis_uams_mdu/UAMS_Format_stndrd_maxis_P1_MDU.csv'\nMAXIS_P1SDU = 's3://astro-groupdata-prod-pipeline/address_standardization/maxis_uams_sdu/UAMS_Format_stndrd_maxis_P1_SDU.csv'\nMAXIS_P2SDU = 's3://astro-groupdata-prod-pipeline/address_standardization/maxis_uams_sdu/UAMS_Format_stndrd_maxis_P2_SDU.csv'\n\nCTS_P1MDU = 's3://astro-groupdata-prod-pipeline/address_standardization/cts_uams_mdu/UAMS_Format_stndrd_CTS_P1_MDU.csv'\nCTS_P1SDU = 's3://astro-groupdata-prod-pipeline/address_standardization/cts_uams_sdu/UAMS_Format_stndrd_CTS_P1_SDU.csv'\nCTS_P2SDU = 's3://astro-groupdata-prod-pipeline/address_standardization/cts_uams_sdu/UAMS_Format_stndrd_CTS_P2_SDU.csv'\n\n# phase 2\nastro_new_std_path = \"s3://astro-groupdata-prod-pipeline/address_standardization/astro_new_standardized/historical_folder/astro_new_standardized_20221025.csv\"\ntm_new_std_path = \"s3://astro-groupdata-prod-pipeline/address_standardization/tm_new_standardized/historical_folder/TM_New_Standardised_20221013.csv.gz\"\nallo_new_std_path = \"s3://astro-groupdata-prod-pipeline/address_standardization/allo_new_standardized/Allo_New_Standardised.csv\"\ncts_new_std_path = \"s3://astro-groupdata-prod-pipeline/address_standardization/cts_new_standardized/CTS_New_Standardised_202209_Reformatted-SarahLocal.csv\"\nmaxis_new_std_path = \"s3://astro-groupdata-prod-pipeline/address_standardization/maxis_new_standardized/Maxis_New_Standardised.csv\"\n\n",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# PHASE 1\n- Read and format P1/P2 mapped addresses for all the ISPs and append them\n\n----\n========================================= THIS IS THE START OF PHASE 1 ============================================\n- taken from Glue Job: address_standardization-prod-uams_generation_final_1 (Pipeline 1)\n- originally converted to PySpark in Zepp Qubole notebook: https://us.qubole.com/notebooks#home?id=141583&type=my-notebooks&view=home\nCombine all of P1P2 Mapped Addresses from each ISP\n\n<!-- Old file paths (from Zeppelin Qubole notebook):\nTM_P1MDU = 's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_TM_P1_MDU20221014.csv'\nTM_P1SDU = 's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_TM_P1_SDU20221014.csv'\nTM_P2SDU = 's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_TM_P2_SDU20221014.csv' -->\n<!-- 's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_ALLO_P1_MDU-20220812.csv'\n's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_ALLO_P1_SDU-20220812.csv'\n's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_ALLO_P2_SDU-20220812.csv' -->\n<!-- 's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_MAXIS_P1_MDU-20220812.csv'\n's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_MAXIS_P1_SDU-20220812.csv'\n's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_MAXIS_P2_SDU-20220812.csv' -->\n<!-- 's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_CTS_P1_MDU-20220320.csv'\n's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_CTS_P1_SDU-20220320.csv'\n's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/uploaded/UAMS_Format_stndrd_CTS_P2_SDU-20220320.csv' -->",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "ISP_Name = 'TM'\n\n### P1 MDU\nUAMS_MDU_P1_TM = spark.read.csv(TM_P1MDU, header=True)\n# print(UAMS_MDU_P1_TM.columns)\n\nUAMS_MDU_P1_TM = UAMS_MDU_P1_TM.withColumn('Address_Type', f.lit('MDU')) # decided to label all as MDU instead of doing an if/else or where condition\nUAMS_MDU_P1_TM = UAMS_MDU_P1_TM.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_MDU_P1_TM = UAMS_MDU_P1_TM.withColumn('P_Flag', f.lit('P1'))\nprint('UAMS MDU P1 TM is: ', UAMS_MDU_P1_TM.select('Account_No').count()) # 179663\n# print(UAMS_MDU_P1_TM.columns)\n# z.show(UAMS_MDU_P1_TM.limit(5))\n\n### P1 SDU\nUAMS_SDU_P1_TM = spark.read.csv(TM_P1SDU, header=True)\n# print(UAMS_SDU_P1_TM.columns)\n\nUAMS_SDU_P1_TM = UAMS_SDU_P1_TM.withColumn('Address_Type', f.lit('SDU'))\nUAMS_SDU_P1_TM = UAMS_SDU_P1_TM.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_SDU_P1_TM = UAMS_SDU_P1_TM.withColumn('P_Flag', f.lit('P1'))\nprint('UAMS SDU P1 TM is: ', UAMS_SDU_P1_TM.select('Account_No').count()) # 1639756\n# print(UAMS_SDU_P1_TM.columns)\n# z.show(UAMS_SDU_P1_TM.limit(5))\n\n### P2 SDU\nUAMS_SDU_P2_TM = spark.read.csv(TM_P2SDU, header=True)\n# print(UAMS_SDU_P2_TM.columns)\n\nUAMS_SDU_P2_TM = UAMS_SDU_P2_TM.withColumn('Address_Type', f.lit('SDU'))\nUAMS_SDU_P2_TM = UAMS_SDU_P2_TM.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_SDU_P2_TM = UAMS_SDU_P2_TM.withColumn('P_Flag', f.lit('P2'))\nprint('UAMS SDU P2 TM is: ', UAMS_SDU_P2_TM.select('Account_No').count()) # 1760063\nprint(UAMS_SDU_P2_TM.columns)\n# z.show(UAMS_SDU_P2_TM.limit(5))\n\n### append P1/P2 files and keep first in drop duplicate (TM)\n\n## first ensure acc_no string & no .0 in them\nUAMS_MDU_P1_TM = UAMS_MDU_P1_TM.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nUAMS_SDU_P1_TM = UAMS_SDU_P1_TM.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nUAMS_SDU_P2_TM = UAMS_SDU_P2_TM.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\n# print(UAMS_MDU_P1_TM.select('Account_No').count(), UAMS_SDU_P1_TM.select('Account_No').count(), UAMS_SDU_P2_TM.select('Account_No').count()) # 179663 1639756 1760063\n\n## union all UAMS files together\nUAMS_P1P2_TM = UAMS_MDU_P1_TM.union(UAMS_SDU_P1_TM).union(UAMS_SDU_P2_TM)\n\n## create a sequential index to make the order of importance: P1 MDU > P1 SDU > P2 SDU, but to do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nUAMS_P1P2_TM = UAMS_P1P2_TM.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\nprint('UAMS P1P2 TM {} before de-dupe:'.format(str(ISP_Name)), UAMS_P1P2_TM.select('Account_No').count()) # 3579482\n\n## de-dupe on Account_No & keep first based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Account_No']).orderBy(f.col(\"index\").asc())\nUAMS_P1P2_TM = UAMS_P1P2_TM.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row').drop('index')\nprint('UAMS P1P2 TM {} after de-dupe:'.format(str(ISP_Name)), UAMS_P1P2_TM.select('Account_No').count(), UAMS_P1P2_TM.select(f.countDistinct('Account_No')).show()) # 3579332 \n\n# z.show(UAMS_P1P2_TM.head(5))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "['_c0', 'Account_No', 'OBJID', 'House_No', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'POSTCODE', 'ASTRO_STATE', 'Standard_Building_Name', 'ServiceType', 'Servicable', 'HNUM_STRT_TM', 'Address_Type', 'P_Flag']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## assign variable names and paths at this step to allow for easier changing\nISP_Name = 'ALLO'\n\n### P1 MDU\nUAMS_MDU_P1_ALLO = spark.read.csv(ALLO_P1MDU, header=True)\n# print(UAMS_MDU_P1_ALLO.columns)\n\nUAMS_MDU_P1_ALLO = UAMS_MDU_P1_ALLO.withColumn('Address_Type', f.lit('MDU')) # decided to label all as MDU instead of doing an if/else or where condition\nUAMS_MDU_P1_ALLO = UAMS_MDU_P1_ALLO.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_MDU_P1_ALLO = UAMS_MDU_P1_ALLO.withColumn('P_Flag', f.lit('P1'))\nprint('UAMS MDU P1 ALLO is: ', UAMS_MDU_P1_ALLO.select('Account_No').count()) # 552\n# print(UAMS_MDU_P1_ALLO.columns)\n# z.show(UAMS_MDU_P1_ALLO.limit(5))\n\n### P1 SDU\nUAMS_SDU_P1_ALLO = spark.read.csv(ALLO_P1SDU, header=True)\n# print(UAMS_SDU_P1_ALLO.columns)\n\nUAMS_SDU_P1_ALLO = UAMS_SDU_P1_ALLO.withColumn('Address_Type', f.lit('SDU'))\nUAMS_SDU_P1_ALLO = UAMS_SDU_P1_ALLO.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_SDU_P1_ALLO = UAMS_SDU_P1_ALLO.withColumn('P_Flag', f.lit('P1'))\nprint('UAMS SDU P1 ALLO is: ', UAMS_SDU_P1_ALLO.select('Account_No').count()) # 46575\n# print(UAMS_SDU_P1_ALLO.columns)\n# z.show(UAMS_SDU_P1_ALLO.limit(5))\n\n### P2 SDU\nUAMS_SDU_P2_ALLO = spark.read.csv(ALLO_P2SDU, header=True)\n# print(UAMS_SDU_P2_ALLO.columns)\n\nUAMS_SDU_P2_ALLO = UAMS_SDU_P2_ALLO.withColumn('Address_Type', f.lit('SDU'))\nUAMS_SDU_P2_ALLO = UAMS_SDU_P2_ALLO.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_SDU_P2_ALLO = UAMS_SDU_P2_ALLO.withColumn('P_Flag', f.lit('P2'))\nprint('UAMS SDU P2 ALLO is: ', UAMS_SDU_P2_ALLO.select('Account_No').count()) # 48990\nprint(UAMS_SDU_P2_ALLO.columns)\n# z.show(UAMS_SDU_P2_ALLO.limit(5))\n\n### append P1/P2 files and keep first in drop duplicate (ALLO)\n\n## first ensure acc_no string & no .0 in them\nUAMS_MDU_P1_ALLO = UAMS_MDU_P1_ALLO.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nUAMS_SDU_P1_ALLO = UAMS_SDU_P1_ALLO.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nUAMS_SDU_P2_ALLO = UAMS_SDU_P2_ALLO.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\n\nprint(UAMS_MDU_P1_ALLO.select('Account_No').count(), UAMS_SDU_P1_ALLO.select('Account_No').count(), UAMS_SDU_P2_ALLO.select('Account_No').count())\n\n## union all UAMS files together\nUAMS_P1P2_ALLO = UAMS_MDU_P1_ALLO.union(UAMS_SDU_P1_ALLO).union(UAMS_SDU_P2_ALLO)\n\n# create a sequential index to make the order of importance: P1 MDU > P1 SDU > P2 SDU, but to do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nUAMS_P1P2_ALLO = UAMS_P1P2_ALLO.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\nprint('UAMS P1P2 ALLO {} before de-dupe:'.format(str(ISP_Name)), UAMS_P1P2_ALLO.select('Account_No').count()) # 96117\n\n# de-dupe on Account_No & keep first based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Account_No']).orderBy(f.col(\"index\").asc())\nUAMS_P1P2_ALLO = UAMS_P1P2_ALLO.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row').drop('index')\nprint('UAMS P1P2 ALLO {} after de-dupe:'.format(str(ISP_Name)), UAMS_P1P2_ALLO.select('Account_No').count(), UAMS_P1P2_ALLO.select(f.countDistinct('Account_No')).show()) # 96117\n\n# z.show(UAMS_P1P2_ALLO.head(5))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "UAMS MDU P1 ALLO is:  552\nUAMS SDU P1 ALLO is:  46575\nUAMS SDU P2 ALLO is:  48990\n['_c0', 'Account_No', 'OBJID', 'House_No', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'POSTCODE', 'ASTRO_STATE', 'Standard_Building_Name', 'ServiceType', 'Servicable', 'HNUM_STRT_TM', 'Address_Type', 'P_Flag']\n552 46575 48990\nUAMS P1P2 ALLO ALLO before de-dupe: 96117\n+--------------------------+\n|count(DISTINCT Account_No)|\n+--------------------------+\n|                     96117|\n+--------------------------+\n\nUAMS P1P2 ALLO ALLO after de-dupe: 96117 None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## assign variable names and paths at this step to allow for easier changing\nISP_Name = 'MAXIS'\n\n### P1 MDU\nUAMS_MDU_P1_MAXIS = spark.read.csv(MAXIS_P1MDU, header=True)\n# print(UAMS_MDU_P1_MAXIS.columns)\n\nUAMS_MDU_P1_MAXIS = UAMS_MDU_P1_MAXIS.withColumn('Address_Type', f.lit('MDU')) # decided to label all as MDU instead of doing an if/else or where condition\nUAMS_MDU_P1_MAXIS = UAMS_MDU_P1_MAXIS.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_MDU_P1_MAXIS = UAMS_MDU_P1_MAXIS.withColumn('P_Flag', f.lit('P1'))\nprint('UAMS MDU P1 MAXIS is: ', UAMS_MDU_P1_MAXIS.select('Account_No').count()) # 9279\n# print(UAMS_MDU_P1_MAXIS.columns)\n# z.show(UAMS_MDU_P1_MAXIS.limit(5))\n\n### P1 SDU\nUAMS_SDU_P1_MAXIS = spark.read.csv(MAXIS_P1SDU, header=True)\n# print(UAMS_SDU_P1_MAXIS.columns)\n\nUAMS_SDU_P1_MAXIS = UAMS_SDU_P1_MAXIS.withColumn('Address_Type', f.lit('SDU'))\nUAMS_SDU_P1_MAXIS = UAMS_SDU_P1_MAXIS.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_SDU_P1_MAXIS = UAMS_SDU_P1_MAXIS.withColumn('P_Flag', f.lit('P1'))\nprint('UAMS SDU P1 MAXIS is: ', UAMS_SDU_P1_MAXIS.select('Account_No').count()) # 12406\n# print(UAMS_SDU_P1_MAXIS.columns)\n# z.show(UAMS_SDU_P1_MAXIS.limit(5))\n\n### P2 SDU\nUAMS_SDU_P2_MAXIS = spark.read.csv(MAXIS_P2SDU, header=True)\n# print(UAMS_SDU_P2_MAXIS.columns)\n\nUAMS_SDU_P2_MAXIS = UAMS_SDU_P2_MAXIS.withColumn('Address_Type', f.lit('SDU'))\nUAMS_SDU_P2_MAXIS = UAMS_SDU_P2_MAXIS.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_SDU_P2_MAXIS = UAMS_SDU_P2_MAXIS.withColumn('P_Flag', f.lit('P2'))\nprint('UAMS SDU P2 MAXIS is: ', UAMS_SDU_P2_MAXIS.select('Account_No').count()) # 39735\nprint(UAMS_SDU_P2_MAXIS.columns)\n# z.show(UAMS_SDU_P2_MAXIS.limit(5))\n\n### append P1/P2 files and keep first in drop duplicate (MAXIS)\n\n## first ensure acc_no string & no .0 in them\nUAMS_MDU_P1_MAXIS = UAMS_MDU_P1_MAXIS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nUAMS_SDU_P1_MAXIS = UAMS_SDU_P1_MAXIS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nUAMS_SDU_P2_MAXIS = UAMS_SDU_P2_MAXIS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\n\nprint(UAMS_MDU_P1_MAXIS.select('Account_No').count(), UAMS_SDU_P1_MAXIS.select('Account_No').count(), UAMS_SDU_P2_MAXIS.select('Account_No').count())\n\n## union all UAMS files together\nUAMS_P1P2_MAXIS = UAMS_MDU_P1_MAXIS.union(UAMS_SDU_P1_MAXIS).union(UAMS_SDU_P2_MAXIS)\n\n# create a sequential index to make the order of importance: P1 MDU > P1 SDU > P2 SDU, but to do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nUAMS_P1P2_MAXIS = UAMS_P1P2_MAXIS.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\nprint('UAMS P1P2 MAXIS {} before de-dupe:'.format(str(ISP_Name)), UAMS_P1P2_MAXIS.select('Account_No').count()) # 61420\n\n# de-dupe on Account_No & keep first based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Account_No']).orderBy(f.col(\"index\").asc())\nUAMS_P1P2_MAXIS = UAMS_P1P2_MAXIS.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row').drop('index')\nprint('UAMS P1P2 MAXIS {} after de-dupe:'.format(str(ISP_Name)), UAMS_P1P2_MAXIS.select('Account_No').count(), UAMS_P1P2_MAXIS.select(f.countDistinct('Account_No')).show()) # 61417\n\n# z.show(UAMS_P1P2_MAXIS.head(5))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "UAMS MDU P1 MAXIS is:  9279\nUAMS SDU P1 MAXIS is:  12406\nUAMS SDU P2 MAXIS is:  39735\n['_c0', 'Account_No', 'OBJID', 'House_No', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'POSTCODE', 'ASTRO_STATE', 'Standard_Building_Name', 'ServiceType', 'Servicable', 'HNUM_STRT_TM', 'Address_Type', 'P_Flag']\n9279 12406 39735\nUAMS P1P2 MAXIS MAXIS before de-dupe: 61420\n+--------------------------+\n|count(DISTINCT Account_No)|\n+--------------------------+\n|                     61417|\n+--------------------------+\n\nUAMS P1P2 MAXIS MAXIS after de-dupe: 61418 None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## assign variable names and paths at this step to allow for easier changing\nISP_Name = 'CTS'\n\n### P1 MDU\nUAMS_MDU_P1_CTS = spark.read.csv(CTS_P1MDU, header=True)\n# print(UAMS_MDU_P1_CTS.columns)\n\nUAMS_MDU_P1_CTS = UAMS_MDU_P1_CTS.withColumn('Address_Type', f.lit('MDU')) # decided to label all as MDU instead of doing an if/else or where condition\nUAMS_MDU_P1_CTS = UAMS_MDU_P1_CTS.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_MDU_P1_CTS = UAMS_MDU_P1_CTS.withColumn('P_Flag', f.lit('P1'))\nprint('UAMS MDU P1 CTS is: ', UAMS_MDU_P1_CTS.select('Account_No').count()) # 0\n# print(UAMS_MDU_P1_CTS.columns)\n# z.show(UAMS_MDU_P1_CTS.limit(5))\n\n### P1 SDU\nUAMS_SDU_P1_CTS = spark.read.csv(CTS_P1SDU, header=True)\n# print(UAMS_SDU_P1_CTS.columns)\n\nUAMS_SDU_P1_CTS = UAMS_SDU_P1_CTS.withColumn('Address_Type', f.lit('SDU'))\nUAMS_SDU_P1_CTS = UAMS_SDU_P1_CTS.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_SDU_P1_CTS = UAMS_SDU_P1_CTS.withColumn('P_Flag', f.lit('P1'))\nprint('UAMS SDU P1 CTS is: ', UAMS_SDU_P1_CTS.select('Account_No').count()) # 7707\n# print(UAMS_SDU_P1_CTS.columns)\n# z.show(UAMS_SDU_P1_CTS.limit(5))\n\n### P2 SDU\nUAMS_SDU_P2_CTS = spark.read.csv(CTS_P2SDU, header=True)\n# print(UAMS_SDU_P2_CTS.columns)\n\nUAMS_SDU_P2_CTS = UAMS_SDU_P2_CTS.withColumn('Address_Type', f.lit('SDU'))\nUAMS_SDU_P2_CTS = UAMS_SDU_P2_CTS.withColumnRenamed('service_add_objid', 'OBJID')\nUAMS_SDU_P2_CTS = UAMS_SDU_P2_CTS.withColumn('P_Flag', f.lit('P2'))\nprint('UAMS SDU P2 CTS is: ', UAMS_SDU_P2_CTS.select('Account_No').count()) # 37821\nprint(UAMS_SDU_P2_CTS.columns)\n# z.show(UAMS_SDU_P2_CTS.limit(5))\n\n### append P1/P2 files and keep first in drop duplicate (CTS)\n\n## first ensure acc_no string & no .0 in them\nUAMS_MDU_P1_CTS = UAMS_MDU_P1_CTS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nUAMS_SDU_P1_CTS = UAMS_SDU_P1_CTS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nUAMS_SDU_P2_CTS = UAMS_SDU_P2_CTS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\n\nprint(UAMS_MDU_P1_CTS.select('Account_No').count(), UAMS_SDU_P1_CTS.select('Account_No').count(), UAMS_SDU_P2_CTS.select('Account_No').count())\n\n## union all UAMS files together\nUAMS_P1P2_CTS = UAMS_MDU_P1_CTS.union(UAMS_SDU_P1_CTS).union(UAMS_SDU_P2_CTS)\n\n# create a sequential index to make the order of importance: P1 MDU > P1 SDU > P2 SDU, but to do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nUAMS_P1P2_CTS = UAMS_P1P2_CTS.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\nprint('UAMS P1P2 CTS {} before de-dupe:'.format(str(ISP_Name)), UAMS_P1P2_CTS.select('Account_No').count()) # 45528\n\n# de-dupe on Account_No & keep first based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Account_No']).orderBy(f.col(\"index\").asc())\nUAMS_P1P2_CTS = UAMS_P1P2_CTS.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row').drop('index')\nprint('UAMS P1P2 CTS {} after de-dupe:'.format(str(ISP_Name)), UAMS_P1P2_CTS.select('Account_No').count(), UAMS_P1P2_CTS.select(f.countDistinct('Account_No')).show()) # 45528\n\n# z.show(UAMS_P1P2_CTS.head(5))\n\n## 31/10/2022: brief workaround because CTS file does not have HNUM_STRT_TM column so it wouldn't union properly with the other files\nUAMS_P1P2_CTS = UAMS_P1P2_CTS.withColumn('HNUM_STRT_TM', f.lit('CTS_file'))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "UAMS MDU P1 CTS is:  0\nUAMS SDU P1 CTS is:  7707\nUAMS SDU P2 CTS is:  37821\n['_c0', 'Account_No', 'OBJID', 'House_No', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'POSTCODE', 'ASTRO_STATE', 'Standard_Building_Name', 'ServiceType', 'Servicable', 'Address_Type', 'P_Flag']\n0 7707 37821\nUAMS P1P2 CTS CTS before de-dupe: 45528\n+--------------------------+\n|count(DISTINCT Account_No)|\n+--------------------------+\n|                     45527|\n+--------------------------+\n\nUAMS P1P2 CTS CTS after de-dupe: 45528 None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Append all ISP's P1/P2 mapped addresses",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## Revision - fakhrul - 28/6/22 adding these 3 lines of code below because some of them are lower case\nUAMS_P1P2_TM = UAMS_P1P2_TM.withColumn('Servicable', f.upper(f.col('Servicable').cast('string')) )\nUAMS_P1P2_ALLO = UAMS_P1P2_ALLO.withColumn('Servicable', f.upper(f.col('Servicable').cast('string')) )\nUAMS_P1P2_CTS = UAMS_P1P2_CTS.withColumn('Servicable', f.upper(f.col('Servicable').cast('string')) )\n\n## checking servicable of each ISPs they have to be capital except for maxis\nprint('checking servicable column (TM) :', UAMS_P1P2_TM.select('Servicable').distinct().show()) # null, TM\nprint('checking servicable column (ALLO) :', UAMS_P1P2_ALLO.select('Servicable').distinct().show()) # ALLO\nprint('checking servicable column (Maxis) :', UAMS_P1P2_MAXIS.select('Servicable').distinct().show()) # null, maxis\nprint('checking servicable column (CTS) :', UAMS_P1P2_CTS.select('Servicable').distinct().show()) # null, CTS\n\n## revision - fakhrul - 8/9/22 saving these files to test them locally\nUAMS_P1P2_TM.coalesce(1).write.csv(UAMS_PySpark_save_path+'localP1P2/historical_folder/TM_LOCAL_{}.csv'.format(str(date_key)), header=True, mode='overwrite')\nUAMS_P1P2_ALLO.coalesce(1).write.csv(UAMS_PySpark_save_path+'localP1P2/historical_folder/ALLO_LOCAL_{}.csv'.format(str(date_key)), header=True, mode='overwrite')\nUAMS_P1P2_MAXIS.coalesce(1).write.csv(UAMS_PySpark_save_path+'localP1P2/historical_folder/MAXIS_LOCAL_{}.csv'.format(str(date_key)), header=True, mode='overwrite')\nUAMS_P1P2_CTS.coalesce(1).write.csv(UAMS_PySpark_save_path+'localP1P2/historical_folder/CTS_LOCAL_{}.csv'.format(str(date_key)), header=True, mode='overwrite')\n\n## union the 4 ISP DF's together then clean some columns\nUAMS_P1P2 = UAMS_P1P2_TM.union(UAMS_P1P2_MAXIS).union(UAMS_P1P2_ALLO).union(UAMS_P1P2_CTS)\nUAMS_P1P2 = UAMS_P1P2.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nUAMS_P1P2 = UAMS_P1P2.withColumn(\"OBJID\", f.regexp_replace(f.col('OBJID').cast('string'), '\\.0', '') )\n\n## after unioning, delete the files to free up RAM\ndel UAMS_P1P2_TM\ndel UAMS_P1P2_ALLO\ndel UAMS_P1P2_MAXIS\ndel UAMS_P1P2_CTS\n\n## create a sequential index to make the index: TM > MAXIS > ALLO > CTS, but to do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nUAMS_P1P2 = UAMS_P1P2.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\nprint('UAMS P1P2 all ISPs before remove ISP|Z (now ISP|null):', UAMS_P1P2.select('Account_No').count()) # 3782395\nprint('Unique acc_no in UAMS P1P2 all ISPs before remove ISP|Z (now ISP|null):', UAMS_P1P2.dropDuplicates(subset=['Account_No']).select('Account_No').count()) # 3600655\n\n# z.show(UAMS_P1P2.head(5))\n\n## Fill '' for nans & filter out cases without ServiceType (after upper case & trim)\nUAMS_P1P2 = UAMS_P1P2.fillna('')\nUAMS_P1P2 = UAMS_P1P2.withColumn('ServiceType', f.upper(f.trim(f.col('ServiceType'))) ).filter(f.col('ServiceType') != '').filter(f.col('ServiceType').isNotNull()).filter(f.col('ServiceType') != 'NAN')\n\n## create the Serviceable column\nUAMS_P1P2 = UAMS_P1P2.withColumn('Serviceable', f.concat_ws('|', f.col(\"Servicable\"), f.col(\"ServiceType\")) )\nprint('checking UAMS P1P2 count after checking unique serviceable : ', UAMS_P1P2.select('Account_No').count()) # 3780562\nprint('checking unique acc_no in UAMS P1P2 count after checking unique serviceable : ', UAMS_P1P2.dropDuplicates(subset=['Account_No']).select('Account_No').count()) # 3600103\nprint('checking all unique serviceable here!!!', UAMS_P1P2.dropDuplicates(subset=['Account_No']).groupBy('Servicable').count().orderBy('count', ascending=False).show()) # 3600103\n\n## clean OBJID column (again)\nUAMS_P1P2 = UAMS_P1P2.withColumn('OBJID',  f.regexp_replace(f.col('OBJID').cast('string'), '\\.0', '') )",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+\n|Servicable|\n+----------+\n|      null|\n|        TM|\n+----------+\n\nchecking servicable column (TM) : None\n+----------+\n|Servicable|\n+----------+\n|      ALLO|\n+----------+\n\nchecking servicable column (ALLO) : None\n+----------+\n|Servicable|\n+----------+\n|      null|\n|     maxis|\n+----------+\n\nchecking servicable column (Maxis) : None\n+----------+\n|Servicable|\n+----------+\n|      null|\n|       CTS|\n+----------+\n\nchecking servicable column (CTS) : None\nUAMS P1P2 all ISPs before remove ISP|Z (now ISP|null): 3782395\nUnique acc_no in UAMS P1P2 all ISPs before remove ISP|Z (now ISP|null): 3600655\nchecking UAMS P1P2 count after checking unique serviceable :  3780562\nchecking unique acc_no in UAMS P1P2 count after checking unique serviceable :  3600103\n+----------+-------+\n|Servicable|  count|\n+----------+-------+\n|        TM|3557269|\n|     maxis|  19157|\n|      ALLO|  17451|\n|       CTS|   6226|\n+----------+-------+\n\nchecking all unique serviceable here!!! None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Combine all Serviceable values for each Acc_No",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## GroupBy Acc_No & get ALL Serviceable values for that Acc_No\nUAMS_P1P2_1 = UAMS_P1P2.groupBy('Account_No').agg(f.collect_set('Serviceable').alias('Serviceable_New'))\nUAMS_P1P2_1 = UAMS_P1P2_1.withColumn(\"Serviceable_New\", f.concat_ws(',', f.col('Serviceable_New')) )\n# UAMS_P1P2_1.select('Serviceable_New').distinct().show() ## this code looks correct\n\nprint('checking uams p1p2_1 count: ', UAMS_P1P2_1.count()) # 3600103\nprint('checking uams p1p2 count: ', UAMS_P1P2.count()) # 3780562\n# z.show(UAMS_P1P2.select('Account_No').head(10))\n\n## JOIN UAMS_P1P2_1 to UAMS_P1P2\nUAMS_P1P2_Merg = UAMS_P1P2_1.join(UAMS_P1P2, on='Account_No', how='left')\nprint('Count of UAMS P1P2 Merg before drop duplicates: ', UAMS_P1P2_Merg.select('Account_No').count() ) # 3780562\n# print('checking head uams p1p2 merg before drop duplicates: ', UAMS_P1P2_Merg[['Serviceable_New']].show(10))\n\n## de-dupe on Acc_No & Serviceable_New\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.dropDuplicates(subset=['Account_No','Serviceable_New'])\nprint('Count of UAMS P1P2 Merg after drop duplicates: ', UAMS_P1P2_Merg.select('Account_No').count() ) # 3600103\n# print('checking head uams p1p2 merg after drop duplicates: ', UAMS_P1P2_Merg[['Serviceable_New']].show(10))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "checking uams p1p2_1 count:  3600103\nchecking uams p1p2 count:  3780562\nCount of UAMS P1P2 Merg before drop duplicates:  3780562\nCount of UAMS P1P2 Merg after drop duplicates:  3600103\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Cleaning and standardizing State, then cleaning the address columns",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.upper(f.trim(f.col('ASTRO_STATE').cast('string'))) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'FEDERAL TERRITORY OF KUALA LUMPUR','WIL'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'WILAYAH PERSEKUTUAN KUALA LUMPUR','WIL'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'WIL KL','WIL'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'KL','WIL'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'LKG','KEDAH'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'SELANGOR','SEL'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'SEL','SELANGOR'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'JOHOR','JOH'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'JOH','JOHOR'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'MELAKA','MEL'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'MEL','MELAKA'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'PULAU PINANG','PNG'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'PENANG','PNG'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'PINANG','PNG'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'PNG','PULAU PINANG'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'PERAK','PRK'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'PRK','PERAK'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'PERLIS','PLS'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'PLS','PERLIS'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'SABAH','SAB'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'SAB','SABAH'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'SARAWAK','SAR'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'SAR','SARAWAK'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'TERENGGANU','TRG'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'TRG','TERENGGANU'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'PAHANG','PHG'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'PHG','PAHANG'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'KEDAH','KED'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'KED','KEDAH'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'NEGERI SEMBILAN','NEG'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'NEG','NEGERI SEMBILAN'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'KELANTAN','KEL'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'KEL','KELANTAN'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'WILAYAH PERSEKUTUAN PUTRAJAYA','PUTRAJAYA'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'WIL PUTRAJAYA','PUTRAJAYA'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'WILAYAH PERSEKUTUAN PUTRAJAYA','PUTRAJAYA'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'WILAYAH PERSEKUTUAN LABUAN','LAB'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'WILAYAH LABUAN','LAB'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'WIL LABUAN','LAB'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'LABUAN','LAB'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'LAB','LABUAN'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'WILAYAH PERSEKUTUAN','WIL'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'WIL','WILAYAH PERSEKUTUAN KUALA LUMPUR'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'LGK','KEDAH'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.regexp_replace('ASTRO_STATE', 'SIN','SINGAPORE'))\n\n## SAVE an INTERMEDIATE TABLE ###\n# mainly coz the above step (cleaning ASTRO_STATE) took 32 mins & I'm worried the notebook might crash\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate1.orc\".format(date_key) , mode='overwrite') # 20/11/22: renamed the savepath",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# del UAMS_P1P2_Merg # -- DELETE previous df if required\n# read back in the intermediate table\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate1.orc\".format(date_key))\n\n# _list = ['#',',',\"'\" ]\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"House_No\", f.regexp_replace(\"House_No\", '|'.join(_list), ''))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"House_No\", f.regexp_replace(\"House_No\", '#',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"House_No\", f.regexp_replace(\"House_No\", ',',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"House_No\", f.regexp_replace(\"House_No\", \"'\",''))\n\n# _list = ['#',',', '/','-', 'No Name', '\\.', '\\*', '=', ':','\\)', '\\(', '`', '_']\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '|'.join(_list), ''))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '#',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", ',',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '/',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '-',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", 'No Name',''))\n# UAMS_P1P2_Merg[\"Combined_Building\"]= np.where( UAMS_P1P2_Merg[\"Combined_Building\"]=='0', '', UAMS_P1P2_Merg[\"Combined_Building\"])\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '\\.',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '\\*',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '=',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", ':',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '\\)',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '\\(',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '`',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '_',''))\n#revision - zohreh - 5/8/22 - uams complain about carrot\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '\\^',''))\n#UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '  ',''))\n#UAMS_P1P2_Merg[\"Combined_Building\"] = UAMS_P1P2_Merg[\"Combined_Building\"].str.strip()\n#UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '(^|\\s)($|\\s)','', case = False, regex = True)\n\n## assigning SDU, MDU to Address_Type\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('Address_Type', when( ((f.col('Combined_Building').isNull()) | (f.col('Combined_Building') == '')), 'SDU').otherwise('MDU') )\n\n# _list = ['#',',', '/','-', 'No Name']\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_1\", f.regexp_replace(\"Street_Type_1\", '|'.join(_list), ''))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_1\", f.regexp_replace(\"Street_Type_1\", '#',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_1\", f.regexp_replace(\"Street_Type_1\", ',',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_1\", f.regexp_replace(\"Street_Type_1\", '/',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_1\", f.regexp_replace(\"Street_Type_1\", '-',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_1\", f.regexp_replace(\"Street_Type_1\", 'No Name',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_1\", f.regexp_replace(\"Street_Type_1\", 'JLN','JALAN'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_1\", f.regexp_replace(\"Street_Type_1\", 'LRG','LORONG'))\n\n# _list = ['#',',', '/','-', 'No Name']\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_2\", f.regexp_replace(\"Street_Type_2\", '|'.join(_list), ''))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_2\", f.regexp_replace(\"Street_Type_2\", '#',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_2\", f.regexp_replace(\"Street_Type_2\", ',',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_2\", f.regexp_replace(\"Street_Type_2\", '/',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_2\", f.regexp_replace(\"Street_Type_2\", '-',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_2\", f.regexp_replace(\"Street_Type_2\", 'No Name',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_2\", f.regexp_replace(\"Street_Type_2\", 'JLN','JALAN'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_2\", f.regexp_replace(\"Street_Type_2\", 'LRG','LORONG'))\n\n# _list = ['#',',', 'No Name']\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(\"Street_1_New\", '|'.join(_list), ''))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(\"Street_1_New\", '#',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(\"Street_1_New\", ',',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(\"Street_1_New\", 'No Name',''))\n\n# _list = ['#',',', 'No Name']\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(\"Street_2_New\",'|'.join(_list), ''))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(\"Street_2_New\", '#',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(\"Street_2_New\", ',',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(\"Street_2_New\", 'No Name',''))\n\n# _list = ['#',',', '/','-', 'No Name']\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"AREA\", f.regexp_replace(\"AREA\", '|'.join(_list), ''))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"AREA\", f.regexp_replace(\"AREA\", '#',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"AREA\", f.regexp_replace(\"AREA\", ',',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"AREA\", f.regexp_replace(\"AREA\", '/',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"AREA\", f.regexp_replace(\"AREA\", '-',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"AREA\", f.regexp_replace(\"AREA\", 'No Name',''))\n\n# _list = ['#',',', '/','-', '=', ':','\\)', '\\(', 'No Name','\\[\\]']\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", '|'.join(_list), ''))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", '#',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", ',',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", '/',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", '-',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", '=',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", ':',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", '\\)',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", '\\(',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", 'No Name',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", '\\[\\]',''))\n\n# _list = ['#',',', '/','-', 'No Name']\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"ASTRO_STATE\", f.regexp_replace(\"ASTRO_STATE\", '|'.join(_list), ''))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"ASTRO_STATE\", f.regexp_replace(\"ASTRO_STATE\", '#',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"ASTRO_STATE\", f.regexp_replace(\"ASTRO_STATE\", ',',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"ASTRO_STATE\", f.regexp_replace(\"ASTRO_STATE\", '/',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"ASTRO_STATE\", f.regexp_replace(\"ASTRO_STATE\", '-',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"ASTRO_STATE\", f.regexp_replace(\"ASTRO_STATE\", 'No Name',''))\n\n# _list = ['#',',', '/','-', 'No Name']\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"POSTCODE\", f.regexp_replace(\"POSTCODE\", '|'.join(_list), ''))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"POSTCODE\", f.regexp_replace(\"POSTCODE\", '#',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"POSTCODE\", f.regexp_replace(\"POSTCODE\", ',',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"POSTCODE\", f.regexp_replace(\"POSTCODE\", '/',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"POSTCODE\", f.regexp_replace(\"POSTCODE\", '-',''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"POSTCODE\", f.regexp_replace(\"POSTCODE\", 'No Name',''))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## trim the columns\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Account_No\", f.trim(f.col(\"Account_No\")) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"OBJID\", f.trim(f.col(\"OBJID\")) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"House_No\", f.trim(f.col(\"House_No\")) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.trim(f.col(\"Combined_Building\")) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.trim(f.col(\"Street_1_New\")) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.trim(f.col(\"Street_2_New\")) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_1\", f.trim(f.col(\"Street_Type_1\")) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_2\", f.trim(f.col(\"Street_Type_2\")) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"POSTCODE\", f.trim(f.col(\"POSTCODE\")) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"AREA\", f.trim(f.col(\"AREA\")) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.trim(f.col(\"STD_CITY\")) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"ASTRO_STATE\", f.trim(f.col(\"ASTRO_STATE\")) )\n\n## don't know how to do below ones in pyspark\n# UAMS_P1P2_Merg[\"Account_No\"] = UAMS_P1P2_Merg[\"Account_No\"].str.strip('\\n\\t')\n# UAMS_P1P2_Merg[\"OBJID\"] = UAMS_P1P2_Merg[\"OBJID\"].str.strip('\\n\\t')\n# UAMS_P1P2_Merg[\"House_No\"] = UAMS_P1P2_Merg[\"House_No\"].str.strip('\\n\\t')\n# UAMS_P1P2_Merg[\"Combined_Building\"] = UAMS_P1P2_Merg[\"Combined_Building\"].str.strip('\\n\\t')\n# UAMS_P1P2_Merg[\"Street_1_New\"] = UAMS_P1P2_Merg[\"Street_1_New\"].str.strip('\\n\\t')\n# UAMS_P1P2_Merg[\"Street_2_New\"] = UAMS_P1P2_Merg[\"Street_2_New\"].str.strip('\\n\\t')\n# UAMS_P1P2_Merg[\"Street_Type_1\"] = UAMS_P1P2_Merg[\"Street_Type_1\"].str.strip('\\n\\t')\n# UAMS_P1P2_Merg[\"Street_Type_2\"] = UAMS_P1P2_Merg[\"Street_Type_2\"].str.strip('\\n\\t')\n# UAMS_P1P2_Merg[\"POSTCODE\"] = UAMS_P1P2_Merg[\"POSTCODE\"].str.strip('\\n\\t')\n# UAMS_P1P2_Merg[\"AREA\"] = UAMS_P1P2_Merg[\"AREA\"].str.strip('\\n\\t')\n# UAMS_P1P2_Merg[\"STD_CITY\"] = UAMS_P1P2_Merg[\"STD_CITY\"].str.strip('\\n\\t')\n# UAMS_P1P2_Merg[\"ASTRO_STATE\"] = UAMS_P1P2_Merg[\"ASTRO_STATE\"].str.strip('\\n\\t')\n\n## fill any blanks\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.fillna('')\n\n## keep only those with non-null & non-blank postcode, city, street type & street name\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter( (f.col('POSTCODE').isNotNull()) & (f.col('POSTCODE') != ''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter( (f.col('STD_CITY').isNotNull()) & (f.col('STD_CITY') != ''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter( (f.col('Street_Type_1').isNotNull()) & (f.col('Street_Type_1') != ''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter( (f.col('Street_1_New').isNotNull()) & (f.col('Street_1_New') != ''))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter( (f.col('ASTRO_STATE').isNotNull()) & (f.col('ASTRO_STATE') != ''))\n\n## return blanks for any AREA value that starts with 'AA' (null by GAPI)\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"AREA\", f.when(f.col('AREA').startswith(\"AA\"), '').otherwise(f.col('AREA')) )\n\n## filter out cases where it starts with 'AA' (null by GAPI) in important columns: city, state, postcode and if there are non-digits in postcode --> should we consider instead of filtering out at this stage, maybe coalesce to get the non-GAPI address component at an earlier Step (maybe Step 2.1 & 2.2)\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter(~f.col('STD_CITY').cast('string').startswith(\"AA\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter(~f.col('ASTRO_STATE').cast('string').startswith(\"AA\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter(~f.col('POSTCODE').cast('string').startswith(\"AA\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter(~f.col('POSTCODE').cast('string').rlike(\"[a-zA-Z]\"))\n\n## filter out any '\\[\\]' coz sometimes Excel handles blanks in cells by returning this\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter(f.col('POSTCODE') != '\\[\\]')\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter(f.col('STD_CITY') != '\\[\\]')\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter(f.col('Street_Type_1') != '\\[\\]')\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter(f.col('Street_1_New') != '\\[\\]')\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter(f.col('ASTRO_STATE') != '\\[\\]')\n\n## ensure POSTCODE is only 5 digits long\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"POSTCODE\", f.regexp_replace(f.col('POSTCODE').cast('string'), '\\.0', '') )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"POSTCODE\", f.substring(f.col('POSTCODE'), 1, 5) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Post_length\", f.length(f.col('POSTCODE')) )\nprint(UAMS_P1P2_Merg.select(\"Post_length\").distinct().show())\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.drop(*['Post_length'])\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('POSTCODE', f.lpad(f.col('POSTCODE').cast('string'), 5, '0') )\n\n## encode then decode each column into ascii\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"House_No\", f.decode(f.encode(f.col('House_No'), 'ascii'), 'ascii') )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Combined_Building\", f.decode(f.encode(f.col('Combined_Building'), 'ascii'), 'ascii') )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.decode(f.encode(f.col('Street_1_New'), 'ascii'), 'ascii') )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.decode(f.encode(f.col('Street_2_New'), 'ascii'), 'ascii') )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_2\", f.decode(f.encode(f.col('Street_Type_2'), 'ascii'), 'ascii') )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_Type_1\", f.decode(f.encode(f.col('Street_Type_1'), 'ascii'), 'ascii') )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"POSTCODE\", f.decode(f.encode(f.col('POSTCODE'), 'ascii'), 'ascii') )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.decode(f.encode(f.col('STD_CITY'), 'ascii'), 'ascii') )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"AREA\", f.decode(f.encode(f.col('AREA'), 'ascii'), 'ascii') )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"ASTRO_STATE\", f.decode(f.encode(f.col('ASTRO_STATE'), 'ascii'), 'ascii') )\n\n### Save an intermediate table\n# coz above steps (from cleaning ASTRO_STATE up to encode/decode) took 30ish mins total & I'm worried the notebook might crash\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate2.orc\".format(date_key) , mode='overwrite')",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+\n|Post_length|\n+-----------+\n|          1|\n|          4|\n|          5|\n+-----------+\n\nNone\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### Not sure what code below does but I think it's to replace anything in string printable (from string import printable) with ''. This cell was mainly to study the codes\n\n## Code in question:\n# UAMS_P1P2_Merg[\"House_No\"] = UAMS_P1P2_Merg[\"House_No\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# UAMS_P1P2_Merg[\"Combined_Building\"] = UAMS_P1P2_Merg[\"Combined_Building\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# UAMS_P1P2_Merg[\"Street_1_New\"] = UAMS_P1P2_Merg[\"Street_1_New\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# UAMS_P1P2_Merg[\"Street_2_New\"] = UAMS_P1P2_Merg[\"Street_2_New\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# UAMS_P1P2_Merg[\"Street_Type_1\"] = UAMS_P1P2_Merg[\"Street_Type_1\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# UAMS_P1P2_Merg[\"Street_Type_2\"] = UAMS_P1P2_Merg[\"Street_Type_2\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# UAMS_P1P2_Merg[\"POSTCODE\"] = UAMS_P1P2_Merg[\"POSTCODE\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# UAMS_P1P2_Merg[\"AREA\"] = UAMS_P1P2_Merg[\"AREA\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# UAMS_P1P2_Merg[\"STD_CITY\"] = UAMS_P1P2_Merg[\"STD_CITY\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# UAMS_P1P2_Merg[\"ASTRO_STATE\"] = UAMS_P1P2_Merg[\"ASTRO_STATE\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n\n## some experimentation I did to try & figure out what is happening - doesn't look like anything really changes...\n# UAMS_P1P2_Merg_pd_test = UAMS_P1P2_Merg.select('Account_No', 'Street_1_New').sample(0.005).toPandas()\n# UAMS_P1P2_Merg_pd_test[\"Street_1_New_1\"] = UAMS_P1P2_Merg_pd_test[\"Street_1_New\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# print(st)\n# UAMS_P1P2_Merg_pd_test.head(5)\n\n## attempt to convert to PySpark:\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"House_No\", f.concat_ws(\"\", f.array_f.col('House_No')) )\n\n## some codes taken from the MDU enhancement notebook that may be useful:\n# astro_kv_clean_2 = astro_kv_clean_2.withColumn('sorted_set', f.array_sort(f.array_distinct(f.split(f.col('new_block_building_name'), pattern=' '))) )\n#  astro_kv_clean_2 = astro_kv_clean_2.withColumn('joined_set', f.concat_ws(\" \", col(\"sorted_set\")) )",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# del UAMS_P1P2_Merg # -- DELETE previous df if required\n# read back in the intermediate table\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate2.orc\".format(date_key))\n\n## remove NANs in Combined_Building\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('Combined_Building', f.regexp_replace(f.upper(f.col('Combined_Building')), 'NAN', '') )\n\n## below step was already done above, looks like it's a repeat\n# UAMS_P1P2_Merg[\"Address_Type\"] = np.where(UAMS_P1P2_Merg[\"Combined_Building\"].isnull(), 'SDU', 'MDU')\n# UAMS_P1P2_Merg[\"Address_Type\"]= np.where(UAMS_P1P2_Merg[\"Combined_Building\"]=='', 'SDU', 'MDU')\n\n## return blank if OBJID is not length 8\n# print(UAMS_P1P2_Merg['OBJID'].map(str).map(len).unique())\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('OBJID', when( (f.length(f.col('OBJID').cast('string')) == 8), f.col('OBJID') ).otherwise(''))\n# UAMS_P1P2_Merg['OBJID'].map(str).map(len).unique()\n\n# UAMS_P1P2_Merg[UAMS_P1P2_Merg[\"STD_CITY\"].astype(str).str.startswith(\"AA\")]\n\n## keep only those with account_no length of 8 or 10\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('Account_len', f.length(f.col('Account_No').cast('string')) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.filter( (f.col('Account_len') == 8) | (f.col('Account_len') == 10) )\n# print(UAMS_P1P2_Merg['Account_len'].unique())\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.drop(*['Account_len'])\n\n# create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## Cleaning up STD_CITY, POSTCODE and ASTRO_STATE MANUALLY (code originally from Maryam/Zohreh)\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", f.regexp_replace(f.col(\"STD_CITY\"), 'KUALA LUMPUR','KL') )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"STD_CITY\", when(f.col('STD_CITY') == 'KL', 'KUALA LUMPUR')\n                                .when(f.col(\"STD_CITY\") == 'WILAYAH PERSEKUTUANERSEKUTUAN',  'WILAYAH PERSEKUTUAN')\n                                .when(f.col(\"STD_CITY\") == 'MENGGATALKOTA KINABALU',  'MENGGATAL')\n                                .when(f.col(\"STD_CITY\") == 'TUARANKOTA KINABALU',  'TUARAN')\n                                .when(f.col(\"ASTRO_STATE\") == 'KUANTAN', 'KUANTAN')\n                                .otherwise(f.col('STD_CITY')) )\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"POSTCODE\", when(f.col(\"POSTCODE\") == '96000', 'SARAWAK').otherwise(f.col(\"POSTCODE\")) )\n\n## save intermediate table\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate3.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel UAMS_P1P2_Merg # -- if required\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate3.orc\".format(date_key)) ## read in ORC version\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"ASTRO_STATE\", when(f.col(\"ASTRO_STATE\") == 'WILAYAH PERSEKUTUAN KUALA LUMPURANG', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") == 'W\\.P\\.', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") == 'PHI', 'PULAU PINANG')\n                                .when(f.col(\"ASTRO_STATE\") == 'SEREMBAN', 'NEGERI SEMBILAN')\n                                .when(f.col(\"ASTRO_STATE\") == 'KUANTAN', 'PAHANG')\n                                .when(f.col(\"ASTRO_STATE\") == 'NEGERI SEMBILANERISEMBILAN', 'NEGERI SEMBILAN')\n                                .when(f.col(\"ASTRO_STATE\") == 'SENAWANG', 'NEGERI SEMBILAN')\n                                .when(f.col(\"ASTRO_STATE\") == 'SUNGAI PETANI', 'KEDAH')\n                                .when(f.col(\"ASTRO_STATE\") == 'PORT DICKSON', 'NEGERI SEMBILAN')\n                                .when(f.col(\"ASTRO_STATE\") == 'PENNSYLVANIA', 'PULAU PINANG')\n                                .when(f.col(\"ASTRO_STATE\") == 'CHERAS', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .otherwise(f.col(\"ASTRO_STATE\")) )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate4.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel UAMS_P1P2_Merg # -- if required\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate4.orc\".format(date_key)) ## read in ORC version\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"ASTRO_STATE\", when(f.col(\"ASTRO_STATE\") == 'GEORGETOWN', 'PULAU PINANG')\n                                .when(f.col(\"ASTRO_STATE\") == 'WP KUALA LUMPUR', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") == 'KUALA TERENGGANU', 'TERENGGANU')\n                                .when(f.col(\"ASTRO_STATE\") == 'IPOH', 'PERAK')\n                                .when(f.col(\"ASTRO_STATE\") == 'LABUAN FEDERAL TERRITORY', 'LABUAN')\n                                .when(f.col(\"ASTRO_STATE\") == 'PASIR PUTEH', 'KELANTAN')\n                                .when(f.col(\"ASTRO_STATE\") == 'ALOR SETAR', 'KEDAH')\n                                .when(f.col(\"ASTRO_STATE\") == 'BATU CAVES', 'SELANGOR')\n                                .when(f.col(\"ASTRO_STATE\") == 'PETALING JAYA', 'SELANGOR')\n                                .when(f.col(\"ASTRO_STATE\") == 'BANTING', 'SELANGOR')\n                                .when(f.col(\"ASTRO_STATE\") == 'PEKAN NANAS', 'JOHOR')\n                                .when(f.col(\"ASTRO_STATE\") == 'KUALA KANGSARAWAK', 'PERAK')\n                                .when(f.col(\"ASTRO_STATE\") == 'WP', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") == 'BANGSARAWAK', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .otherwise(f.col(\"ASTRO_STATE\")) )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate5.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel UAMS_P1P2_Merg # -- if required\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate5.orc\".format(date_key)) ## read in ORC version\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"ASTRO_STATE\", when(f.col(\"ASTRO_STATE\") == 'SRI HARTAMAS', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") == 'SENTUL', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") == 'SEGAMBUT', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") == 'KUALA LUMPUR', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") == 'BATU CAVES', 'SELANGOR')\n                                .when(f.col(\"ASTRO_STATE\") ==  'WILAYAH PERSEKUTUAN KUALA LUMPUR WILAYAH PERSEKUTUAN KUALA LUMPURAYAHL 0 PERSEKUTUAN KUALA LUMPUR', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") ==  'SEPANGOR', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") ==  'SELANGORAGOR', 'SELANGOR')\n                                .when(f.col(\"ASTRO_STATE\") ==  '  SHAH ALAM', 'SELANGOR')\n                                .when(f.col(\"ASTRO_STATE\") == 'W\\.P\\.', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") == '   ', '')\n                                .when(f.col(\"ASTRO_STATE\") == '[]', '')\n                                .when(f.col(\"ASTRO_STATE\") == 'MALACCA', 'MELAKA')\n                                .when(f.col(\"ASTRO_STATE\") == 'WILAYAH PERSEKUTUAN KUALA LUMPURANG', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"ASTRO_STATE\") == 'W.P.', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .otherwise(f.col(\"ASTRO_STATE\")) )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate6.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel UAMS_P1P2_Merg # -- if required\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate6.orc\".format(date_key)) ## read in ORC version\n\n# ## filter out Postcode_length != 5\n# print('before filtering out Post_length != 5', UAMS_P1P2_Merg.select('POSTCODE').count()) # \n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Post_length\", f.length(f.col('POSTCODE')) )\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.filter(f.col(\"Post_length\") == 5 )\n# print('after filtering out Post_length != 5', UAMS_P1P2_Merg.select('POSTCODE').count()) # \n\n## fill nulls again\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.fillna('')\n\n## make all columns upper case\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('House_No', f.upper(f.col('House_No')) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('Combined_Building', f.upper(f.col('Combined_Building')) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('Street_Type_1', f.upper(f.col('Street_Type_1')) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('Street_1_New', f.upper(f.col('Street_1_New')) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('Street_Type_2', f.upper(f.col('Street_Type_2')) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('Street_2_New', f.upper(f.col('Street_2_New')) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('AREA', f.upper(f.col('AREA')) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('STD_CITY', f.upper(f.col('STD_CITY')) )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('ASTRO_STATE', f.upper(f.col('ASTRO_STATE')) )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate7.orc\".format(date_key), mode='overwrite', compression='snappy')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Clean HouseNo & Street Names that were converted to Dates",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "del UAMS_P1P2_Merg # -- if required\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate7.orc\".format(date_key)) ## read in ORC version\n\n## Fix HouseNo that are converted to date. Pyspark code taken from P2 MDU Mapping Test Qubole Zepp notebooks\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.col('House_No'))\n\n# UAMS_P1P2_Merg['HouseNo'] = UAMS_P1P2_Merg['HouseNo'].str.replace({'HouseNo': { \n#     \"JAN-\":\"01-\",\"-JAN\":\"-01\", \"FEB-\":\"02-\", \"-FEB\":'-02',\"MAR-\":'03-',\"-MAR\":\"-03\",\n#     \"APR-\":\"04-\",\"-APR\":\"-04\", \"MAY-\":\"05-\",\"-MAY\":\"-05\", \"JUN-\":\"06-\", \"-JUN\":\"-06\",\n#     \"JUL-\":\"07-\", \"-JUL\":\"-07\",\"AUG-\":'08-', \"-AUG\":\"-08\", \"SEP-\":\"09-\", \"-SEP\":\"-09\",\n#    \"OCT-\":\"10-\", \"-OCT\":\"-10\",  \"NOV-\":\"11-\",\"-NOV\":\"-11\", \"DEC-\":\"12-\",\"-DEC\":\"-12\"  }}, case = False)\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JAN-\",\"01-\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JAN\",\"-01\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"FEB-\",\"02-\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-FEB\",'-02'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"MAR-\",'03-'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-MAR\",\"-03\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"APR-\",\"04-\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-APR\",\"-04\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"MAY-\",\"05-\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-MAY\",\"-05\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JUN-\",\"06-\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JUN\",\"-06\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JUL-\",\"07-\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JUL\",\"-07\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"AUG-\",'08-'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-AUG\",\"-08\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"SEP-\",\"09-\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-SEP\",\"-09\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"OCT-\",\"10-\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-OCT\",\"-10\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"NOV-\",\"11-\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-NOV\",\"-11\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"DEC-\",\"12-\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-DEC\",\"-12\"))\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate8.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel UAMS_P1P2_Merg # -- if required\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate8.orc\".format(date_key)) ## read in ORC version\n\n## Fix HouseNo that are converted to date (DD/MM/YYYY format). Pyspark code taken from P2 MDU Mapping Test Qubole Zepp notebooks\n# Filter date HouseNo\ndate_house = UAMS_P1P2_Merg.filter(f.regexp_extract('HouseNo', '^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$', 0) != '' ) \n# Spliting the HouseNo\ndate_house = date_house.withColumn('block_date',  f.substring(date_house.HouseNo, 1, 2))\ndate_house = date_house.withColumn('floor',  f.substring(date_house.HouseNo, 4, 2))\ndate_house = date_house.withColumn('unit',  f.substring(date_house.HouseNo, 9, 2))\n# Combine the split HouseNo with dashes: '-'\ndate_house = date_house.withColumn('HOUSE_NO_ASTRO', f.concat_ws('-', date_house.block_date, date_house.floor, date_house.unit))\n# Remove additional column created to combine HouseNo\ndate_house = date_house.drop(*['block_date','floor','unit'])\n# print('date_house:', date_house.select('ACCOUNT_NO').count(), date_house.select(f.countDistinct('ACCOUNT_NO')).show()) #  rows,  unique acc_no\n\n# Filter not date HouseNo\nnot_date_house = UAMS_P1P2_Merg.filter( f.regexp_extract('HouseNo', '^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$', 0) == '' )\nnot_date_house = not_date_house.withColumn('HOUSE_NO_ASTRO', f.col('HouseNo'))\n# print('not_date_house:', not_date_house.select('ACCOUNT_NO').count(), not_date_house.select(f.countDistinct('ACCOUNT_NO')).show()) #  rows,  unique acc_no\n\n# Append the 2 dfs (date_house, not_date_house)\nUAMS_P1P2_Merg = date_house.union(not_date_house)\nprint('after reunioning the 2:', UAMS_P1P2_Merg.select('ACCOUNT_NO').count(), UAMS_P1P2_Merg.select(f.countDistinct('ACCOUNT_NO')).show()) #  rows,  unique acc_no\n# create a sequential index as Zohreh did a pandas reset_index at this step again. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## rename final house_no column & drop extra house no columns\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('House_No', f.lpad(f.col('HOUSE_NO_ASTRO').cast('string'), 10, ' ') )\nUAMS_P1P2_Merg= UAMS_P1P2_Merg.drop(*['HOUSE_NO_ASTRO', 'HouseNo'])\n\n### Save an intermediate table\n## coz above steps (from cleaning ASTRO_STATE up to encode/decode) took 30ish mins total & I'm worried the notebook might crash\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate9.orc\".format(date_key) , mode='overwrite')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel UAMS_P1P2_Merg # -- DELETE previous df if required\n# read back in the intermediate table\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate9.orc\".format(date_key))\n\n## Fix Street_1_New that got converted to date format then pad with spaces\n# UAMS_P1P2_Merg[\"Street_1_New\"] = UAMS_P1P2_Merg[\"Street_1_New\"].str.replace({'Street_1_New': \n#           {\"JAN-\":\"1/\", \"-JAN\":\"/1\", \"FEB-\":\"2/\", \"-FEB\":'/2',\"MAR-\":'3/',\"-MAR\":\"/3\", \"APR-\":\"4/\",\"-APR\":\"/4\", \"MAY-\":\"5/\",\"-MAY\":\"/5\", \"JUN-\":\"6/\", \"-JUN\":\"/6\",\n#              \"JUL-\":\"7/\", \"-JUL\":\"/7\",\"AUG-\":'8/', \"-AUG\":\"/8\", \"SEP-\":\"9/\", \"-SEP\":\"/9\", \"OCT-\":\"10/\", \"-OCT\":\"/10\",  \"NOV-\":\"11/\",\"-NOV\":\"/11\", \"DEC-\":\"12/\",\"-DEC\":\"/12\"  }}, case = False)\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"JAN-\",\"1/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-JAN\",\"/1\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), 'FEB-','2/'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), '-FEB','/2'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"MAR-\",'3/'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-MAR\",\"/3\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"APR-\",\"4/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-APR\",\"/4\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"MAY-\",\"5/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-MAY\",\"/5\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"JUN-\",\"6/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-JUN\",\"/6\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"JUL-\",\"7/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-JUL\",\"/7\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"AUG-\",'8/'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-AUG\",\"/8\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"SEP-\",\"9/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-SEP\",\"/9\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"OCT-\",\"10/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-OCT\",\"/10\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"NOV-\",\"11/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-NOV\",\"/11\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"DEC-\",\"12/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-DEC\",\"/12\"))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('Street_1_New', f.lpad(f.col('Street_1_New').cast('string'), 10, ' ') )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate10.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel UAMS_P1P2_Merg # -- if required\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate10.orc\".format(date_key)) ## read in ORC version\n\n## Fix Street_2_New that got converted to date format then pad with spaces\n# UAMS_P1P2_Merg[\"Street_2_New\"] = UAMS_P1P2_Merg[\"Street_2_New\"].str.replace({'Street_2_New': \n#           {\"JAN-\":\"1/\", \"-JAN\":\"/1\", \"FEB-\":\"2/\", \"-FEB\":'/2',\"MAR-\":'3/',\"-MAR\":\"/3\", \"APR-\":\"4/\",\"-APR\":\"/4\", \"MAY-\":\"5/\",\"-MAY\":\"/5\", \"JUN-\":\"6/\", \"-JUN\":\"/6\",\n#              \"JUL-\":\"7/\", \"-JUL\":\"/7\",\"AUG-\":'8/', \"-AUG\":\"/8\", \"SEP-\":\"9/\", \"-SEP\":\"/9\", \"OCT-\":\"10/\", \"-OCT\":\"/10\",  \"NOV-\":\"11/\",\"-NOV\":\"/11\", \"DEC-\":\"12/\",\"-DEC\":\"/12\" }}, case = False)\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"JAN-\",\"1/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-JAN\",\"/1\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), 'FEB-','2/'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), '-FEB','/2'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"MAR-\",'3/'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-MAR\",\"/3\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"APR-\",\"4/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-APR\",\"/4\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"MAY-\",\"5/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-MAY\",\"/5\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"JUN-\",\"6/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-JUN\",\"/6\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"JUL-\",\"7/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-JUL\",\"/7\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"AUG-\",'8/'))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-AUG\",\"/8\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"SEP-\",\"9/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-SEP\",\"/9\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"OCT-\",\"10/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-OCT\",\"/10\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"NOV-\",\"11/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-NOV\",\"/11\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"DEC-\",\"12/\"))\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-DEC\",\"/12\"))\n\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn('Street_2_New', f.lpad(f.col('Street_2_New').cast('string'), 10, ' ') )\n\n## these steps seems to be duplicate steps as above...\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.fillna('')\n# UAMS_P1P2_Merg['Combined_Building'] = UAMS_P1P2_Merg['Combined_Building'].str.upper()\n# UAMS_P1P2_Merg['Street_Type_1'] = UAMS_P1P2_Merg['Street_Type_1'].str.upper()\n# UAMS_P1P2_Merg['Street_Type_2'] = UAMS_P1P2_Merg['Street_Type_2'].str.upper()\n# UAMS_P1P2_Merg['Street_1_New'] = UAMS_P1P2_Merg['Street_1_New'].str.upper()\n# UAMS_P1P2_Merg['Street_2_New'] = UAMS_P1P2_Merg['Street_2_New'].str.upper()\n# UAMS_P1P2_Merg['AREA'] = UAMS_P1P2_Merg['AREA'].str.upper()\n# UAMS_P1P2_Merg['STD_CITY'] = UAMS_P1P2_Merg['STD_CITY'].str.upper()\n# UAMS_P1P2_Merg['ASTRO_STATE'] = UAMS_P1P2_Merg['ASTRO_STATE'].str.upper()\n\n## rename state column\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumnRenamed('ASTRO_STATE','STATE')\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate11.orc\".format(date_key), mode='overwrite', compression='snappy')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                         0|\n+--------------------------+\n\ndate_house: 0 None\n+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                   3328852|\n+--------------------------+\n\nnot_date_house: 3328852 None\n+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                   3328852|\n+--------------------------+\n\nafter reunioning the 2: 3328852 None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "del UAMS_P1P2_Merg # -- if required\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-intermediate11.orc\".format(date_key)) ## read in ORC version\n\n## Create the \"Key\" column (is this the unique identifier in UAMS? Or just the full address?). Then upper case & replace spaces with blanks\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Key\", f.concat_ws(\" ,\", \"House_No\", \"Combined_Building\", \"Street_Type_1\", \"Street_1_New\", \"Street_Type_2\", \"Street_2_New\", \"STD_CITY\", \"AREA\", \"POSTCODE\", \"STATE\") )\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Key\", f.regexp_replace(f.upper(f.col(\"Key\")), \" \", \"\") )\n\n## Creating Address_ID based on address_key by converting the 'categorical' values to numerical values. In PySpark: https://stackoverflow.com/questions/45507803/pyspark-dataframe-how-to-convert-one-column-from-categorical-values-to-int\n# UAMS_P1P2_Merg = UAMS_P1P2_Merg.withColumn(\"Key_cat\", f.col('Key').cast('category'))\nfrom pyspark.ml.feature import StringIndexer\nindexer = StringIndexer(inputCol='Key', outputCol='Address_ID')\nUAMS_P1P2_Merg_index = indexer.fit(UAMS_P1P2_Merg).transform(UAMS_P1P2_Merg)\nUAMS_P1P2_Merg_index = UAMS_P1P2_Merg_index.withColumn('Address_ID', f.col('Address_ID') + 1)\n# UAMS_P1P2_Merg['Address_ID'] += 1\n\n\n## TEMPORARY line (this should've been run earlier in this notebook but coz I forgot to add this line earlier, I am adding it here so I can save to CSV. When re-running, can remove this bottom line\nUAMS_P1P2_Merg_index = UAMS_P1P2_Merg_index.withColumn(\"Serviceable_New\", f.concat_ws(',', f.col('Serviceable_New')) )\n\nprint('end of pipeline, uams p1p2 merged columns & count is:')\nprint(UAMS_P1P2_Merg_index.columns)\nprint(UAMS_P1P2_Merg_index.count()) # 3328852\n\n## SAVE files!\nUAMS_P1P2_Merg_index.write.orc(UAMS_PySpark_save_path+'phase_1/{}/UAMS_P1P2_Merg-final.orc'.format(str(date_key)), mode='overwrite') ## also save an orc version just in case\nUAMS_P1P2_Merg_index.coalesce(1).write.csv(UAMS_PySpark_save_path+'phase_1/{}/UAMS_P1P2_Merg-final.csv.gz'.format(str(date_key)), header=True, mode='overwrite', compression='gzip')\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megaytes):')\n# print(usage)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "end of pipeline, uams p1p2 merged columns & count is:\n['Account_No', 'Serviceable_New', '_c0', 'OBJID', 'House_No', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'POSTCODE', 'STATE', 'Standard_Building_Name', 'ServiceType', 'Servicable', 'HNUM_STRT_TM', 'Address_Type', 'P_Flag', 'index', 'Serviceable', 'Key', 'Address_ID']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# PHASE 2\n- Preparing UAMS format for all standardised base - Astro and ISPs\n- to automate!\n\n---\n ===================================== THIS IS THE START OF PIPELINE 2 - BACKUP  ===================================\n- taken from Glue Job: address_standardization-prod-uams_generation_final_2_backup\n- according to Fakhrul, the order for pipeline 2 is final_2_backup -> final_2_backup_2 -> final_2\n- Original Zepp Qub notebook: https://us.qubole.com/notebooks#recent?id=141792&type=my-notebooks&view=recent",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# ### Preparing UAMS format for all standardised base - Astro and ISPs\n\n### ASTRO\n\n## read in the data & relevant columns\n# Astro_Standard = spark.read.csv(\"s3://astro-datalake-prod-sandbox/amzar/BB/other_adhoc/uploaded/address_checking/astro_new_standardized_20221025.csv.gz\", header=True)\nAstro_Standard = spark.read.csv(astro_new_std_path, header=True)  # to automate\nAstro_Standard = Astro_Standard.select(['service_add_objid', 'ACCOUNT_NO0','HOUSE_NO', 'AREA', 'STD_CITY', 'ASTRO_STATE', 'POSTCODE', 'Combined_Building','Street_1', 'Street_2','Standard_Building_Name','match'])\n## rename columns that got changed at some point to fit the script that Fakhrul has on Glue job\nAstro_Standard = Astro_Standard.withColumnRenamed('ACCOUNT_NO0', 'ACCOUNT_NO')\n\nprint('this is astro new std :', Astro_Standard.select('ACCOUNT_NO').count()) # 4733318\n\n## create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nAstro_Standard = Astro_Standard.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## turn 'AA' in Street_1 into blanks and same for Street_2 cases where match == 'Match'\nAstro_Standard = Astro_Standard.withColumn('Street_1', when(f.col('Street_1').cast('string').startswith('AA'), '').otherwise(f.col('Street_1')))\nAstro_Standard = Astro_Standard.withColumn('Street_2', when(f.col('match').cast('string') == 'Match', '').otherwise(f.col('Street_2')))\n\n# STRT_P1.head()\n\n## uppercase\nAstro_Standard = Astro_Standard.withColumn('Street_1', f.upper(f.col('Street_1')))\nAstro_Standard = Astro_Standard.withColumn('Street_2', f.upper(f.col('Street_2')))\n\n## PySpark equivalent of defined extract_street function\nr1 = \"JALAN|JLN|LORONG|LRG|CHANGKAT|LAMAN|LAHAT|LEBUH|LEBUHRAYA|LENGKOK|LINGKARAN|PERSIARAN\"\nAstro_Standard = Astro_Standard.withColumn('Street_Type_1', f.regexp_extract(f.col('Street_1'), r1, 0) )\nAstro_Standard = Astro_Standard.withColumn('Street_Type_2', f.regexp_extract(f.col('Street_2'), r1, 0) )\n\n# STRT_P1.head()\n\n## extract the Street Name without any of the words in the Street Type List\nstreet_type_list = ['JLN','JALAN','LRG', 'LORONG','CHANGKAT', 'LAMAN', 'LAHAT', 'LEBUH', 'LEBUHRAYA', 'LENGKOK ','LINGKARAN', 'PERSIARAN' ]\nAstro_Standard = Astro_Standard.withColumn(\"Street_1_New\", f.regexp_replace(f.col(\"Street_1\"), '|'.join(street_type_list), '') )\nAstro_Standard = Astro_Standard.withColumn(\"Street_2_New\", f.regexp_replace(f.col(\"Street_2\"), '|'.join(street_type_list), '') )\n\nprint('this is astro new std :', Astro_Standard.count()) # 4733318\n\n## select relevant columns & do some renaming\nAstro_Standard = Astro_Standard.select(['service_add_objid', 'ACCOUNT_NO','HOUSE_NO', 'AREA', 'STD_CITY', 'ASTRO_STATE', 'POSTCODE', 'Combined_Building', 'Street_Type_1','Street_1_New','Street_Type_2',\n                                         'Street_2_New','Standard_Building_Name'])\nAstro_Standard = Astro_Standard.withColumnRenamed('service_add_objid', 'OBJID').withColumnRenamed('ASTRO_STATE','STATE').withColumnRenamed('HOUSE_NO','HouseNo')\n\n## ensure string type & no .0\nAstro_Standard = Astro_Standard.withColumn('OBJID', f.regexp_replace(f.col('OBJID').cast('string'), '\\.0', '') )\nAstro_Standard = Astro_Standard.withColumn('ACCOUNT_NO', f.regexp_replace(f.col('ACCOUNT_NO').cast('string'), '\\.0', '') )\n\n# print(Astro_Standard.info())\nprint(Astro_Standard.columns)\n\n## cast all columns as string\nAstro_Standard = Astro_Standard.select([f.col(column).cast('string') for column in Astro_Standard.columns])\n\n## save to path \nAstro_Standard.write.orc(UAMS_PySpark_save_path+'phase_2/{}/astro_temp_standard.orc'.format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_parquet(df = Astro_Standard,compression ='snappy', schema_evolution = False, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_parquet/final_2_astro_standard.snappy.parquet')\n\ndel Astro_Standard # delete to free up running memory ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "['OBJID', 'ACCOUNT_NO', 'HouseNo', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'Standard_Building_Name']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### TM\n# -- did a check in the 20221013 file: no values where STD_CITY or CITY_coalesced are blank. BUT CITY_coalesced had 10668091 nulls whereas STD_CITY only had 121887 nulls. So on 10/11/2022, switched from CITY_coalesced to STD_CITY\n# ----> this led to 6,523,217 STD_CITY nulls/blanks in the All file created in the \"Concatenate into All and further cleaning\" step\n# -- found that ServiceType has around 145 records which aren't FTTH or VDSL. Most are ERROR, a few are null or something else like TUARAN, SHAH ALAM, PASARAYA GIANT etc\n\n## read in the data & relevant columns\n# TM_Standard = spark.read.csv(\"s3://astro-datalake-prod-sandbox/amzar/BB/other_adhoc/uploaded/address_checking/TM_New_Standardised_20221013.csv.gz\", header=True)\nTM_Standard = spark.read.csv(tm_new_std_path, header=True) # to automate\nTM_Standard = TM_Standard.select(['Combined_Building','HouseNo', 'Street_1','Street_2','AREA','STD_CITY','STATE10', 'POSTCODE12', 'ServiceType','Standard_Building_Name','match'])\n## rename columns that got changed at some point (either due to how Spark handles duplicate column names or me coalescing addr components) to fit the script that Fakhrul has on Glue job\nTM_Standard = TM_Standard.withColumnRenamed('STATE10', 'STATE').withColumnRenamed('POSTCODE12', 'POSTCODE') \n# .withColumnRenamed('CITY_coalesced', 'STD_CITY') --> originally used CITY_coalesced but apparently it had 10668091 nulls whereas STD_CITY only had 121887 nulls so changed to STD_CITY on 10/11/2022\n# .withColumnRenamed('Street_coalesced', 'Street_1') --> originally used Street_coalesced but apparently it had 10668085 nulls whereas Street_1 only had 81969 nulls so changed to STD_CITY on 10/11/2022\n\nprint('this is TM new std :', TM_Standard.count()) # 11050724\n\n## create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nTM_Standard = TM_Standard.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## turn 'AA' in Street_1 into blanks and same for Street_2 cases where match == 'Match'\nTM_Standard = TM_Standard.withColumn('Street_1', when(f.col('Street_1').cast('string').startswith('AA'), '').otherwise(f.col('Street_1')))\nTM_Standard = TM_Standard.withColumn('Street_2', when(f.col('match').cast('string') == 'Match', '').otherwise(f.col('Street_2')))\n\n# STRT_P1.head()\n\n## uppercase\nTM_Standard = TM_Standard.withColumn('Street_1', f.upper(f.col('Street_1'))).withColumn('Street_2', f.upper(f.col('Street_2')))\n\n## PySpark equivalent of defined extract_street function\nr1 = \"JALAN|JLN|LORONG|LRG|CHANGKAT|LAMAN|LAHAT|LEBUH|LEBUHRAYA|LENGKOK|LINGKARAN|PERSIARAN\"\nTM_Standard = TM_Standard.withColumn('Street_Type_1', f.regexp_extract(f.col('Street_1'), r1, 0) )\nTM_Standard = TM_Standard.withColumn('Street_Type_2', f.regexp_extract(f.col('Street_2'), r1, 0) )\n\n# STRT_P1.head()\n\n## extract the Street Name without any of the words in the Street Type List\nstreet_type_list = ['JLN','JALAN','LRG', 'LORONG','CHANGKAT', 'LAMAN', 'LAHAT', 'LEBUH', 'LEBUHRAYA', 'LENGKOK ','LINGKARAN', 'PERSIARAN' ]\nTM_Standard = TM_Standard.withColumn(\"Street_1_New\", f.regexp_replace(f.col(\"Street_1\"), '|'.join(street_type_list), '') )\nTM_Standard = TM_Standard.withColumn(\"Street_2_New\", f.regexp_replace(f.col(\"Street_2\"), '|'.join(street_type_list), '') )\n\n# print('this is TM new std :', TM_Standard.count()) # 11050724\n\n## select relevant columns & create \"Serviceable\"\nTM_Standard = TM_Standard.select(['Combined_Building','HouseNo','Street_Type_1', 'Street_1_New','Street_Type_2','Street_2_New','AREA','STD_CITY','STATE', 'POSTCODE', 'ServiceType','Standard_Building_Name'])\nTM_Standard = TM_Standard.withColumn(\"Servicable\", f.lit('TM'))\n\n## --> Revision - fakhrul - zohreh - 1/7/22 - remove nan and error records\nprint('TM_Standard before removing nan & error records :', TM_Standard.select('HouseNo').count()) # 11050724\n\n# order ServiceType values to keep 'FTTH' over 'VDSL' when de-duping (if it exists). To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Combined_Building','HouseNo','Street_Type_1', 'Street_1_New','Street_Type_2','Street_2_New','AREA','STD_CITY','STATE','POSTCODE','Standard_Building_Name']).orderBy(f.col(\"ServiceType\").asc())\nTM_Standard = TM_Standard.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\n# this is the pandas de-dupe code: TM_Standard = TM_Standard.sort_values(['ServiceType']).drop_duplicates( keep = 'first')\n\nTM_Standard = TM_Standard.filter(f.col(\"ServiceType\") != \"\")\nTM_Standard = TM_Standard.filter(f.col(\"ServiceType\").isNotNull())\nTM_Standard = TM_Standard.filter(f.col(\"ServiceType\") != \"NAN\")\nTM_Standard = TM_Standard.filter(f.col(\"ServiceType\") != \"ERROR\")\nprint('TM_Standard after removing nan & error records :', TM_Standard.select('HouseNo').count()) # 9641239\n\n## convert Serviceable to \"TM|FTTH\" or \"TM|VDSL\"\nTM_Standard = TM_Standard.withColumn('Serviceable', f.concat_ws(\"|\", f.col(\"Servicable\"), f.col(\"ServiceType\")) )\n\n# print(TM_Standard.info())\nprint(TM_Standard.columns)\n## cast all columns as string\nTM_Standard = TM_Standard.select([f.col(column).cast('string') for column in TM_Standard.columns])\nprint(TM_Standard.columns)\n\n## save to path\nTM_Standard.write.orc(UAMS_PySpark_save_path+'phase_2/{}/tm_temp_standard.orc'.format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_parquet(df = TM_Standard,compression ='snappy', schema_evolution = False, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_parquet/final_2_tm_standard.snappy.parquet')\n\ndel TM_Standard",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "TM_Standard after removing nan & error records : 9641239\n['Combined_Building', 'HouseNo', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'ServiceType', 'Standard_Building_Name', 'Servicable', 'Serviceable']\n['Combined_Building', 'HouseNo', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'ServiceType', 'Standard_Building_Name', 'Servicable', 'Serviceable']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### Allo\n\n## read in the data & relevant columns\n# Allo_Standard = spark.read.csv(\"s3://astro-datalake-prod-sandbox/amzar/BB/other_adhoc/uploaded/address_checking/Allo_New_Standardised_20220707.csv\", header=True)\nAllo_Standard = spark.read.csv(allo_new_std_path, header=True) # to automate\nAllo_Standard = Allo_Standard.select(['Combined_Building','HouseNo', 'Street_1','Street_2','AREA','STD_CITY','STATE12', 'POSTCODE14', 'ServiceType','Standard_Building_Name','match'])\n## rename columns that got changed at some point (either due to how Spark handles duplicate column names or me coalescing addr components) to fit the script that Fakhrul has on Glue job\nAllo_Standard = Allo_Standard.withColumnRenamed('STATE12', 'STATE').withColumnRenamed('POSTCODE14', 'POSTCODE')\n\n# print('this is Allo new std :', Allo_Standard.count()) # 371708\n\n## create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nAllo_Standard = Allo_Standard.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## turn 'AA' in Street_1 into blanks and same for Street_2 cases where match == 'Match'\nAllo_Standard = Allo_Standard.withColumn('Street_1', when(f.col('Street_1').cast('string').startswith('AA'), '').otherwise(f.col('Street_1')))\nAllo_Standard = Allo_Standard.withColumn('Street_2', when(f.col('match').cast('string') == 'Match', '').otherwise(f.col('Street_2')))\n\n# STRT_P1.head()\n\n## uppercase\nAllo_Standard = Allo_Standard.withColumn('Street_1', f.upper(f.col('Street_1'))).withColumn('Street_2', f.upper(f.col('Street_2')))\n\n## PySpark equivalent of defined extract_street function\nr1 = \"JALAN|JLN|LORONG|LRG|CHANGKAT|LAMAN|LAHAT|LEBUH|LEBUHRAYA|LENGKOK|LINGKARAN|PERSIARAN\"\nAllo_Standard = Allo_Standard.withColumn('Street_Type_1', f.regexp_extract(f.col('Street_1'), r1, 0) )\nAllo_Standard = Allo_Standard.withColumn('Street_Type_2', f.regexp_extract(f.col('Street_2'), r1, 0) )\n\n# STRT_P1.head()\n\n## extract the Street Name without any of the words in the Street Type List\nstreet_type_list = ['JLN','JALAN','LRG', 'LORONG','CHANGKAT', 'LAMAN', 'LAHAT', 'LEBUH', 'LEBUHRAYA', 'LENGKOK ','LINGKARAN', 'PERSIARAN' ]\nAllo_Standard = Allo_Standard.withColumn(\"Street_1_New\", f.regexp_replace(f.col(\"Street_1\"), '|'.join(street_type_list), '') )\nAllo_Standard = Allo_Standard.withColumn(\"Street_2_New\", f.regexp_replace(f.col(\"Street_2\"), '|'.join(street_type_list), '') )\n\nprint('this is Allo new std :', Allo_Standard.count()) # 371708\n\n## select relevant columns & create \"Serviceable\"\nAllo_Standard = Allo_Standard.select(['Combined_Building','HouseNo','Street_Type_1', 'Street_1_New','Street_Type_2','Street_2_New','AREA','STD_CITY','STATE', 'POSTCODE', 'ServiceType','Standard_Building_Name'])\nAllo_Standard = Allo_Standard.withColumn(\"Servicable\", f.lit('ALLO'))\n\n## --> #revision - fakhrul - 1/7/22 - remove allo nan\nprint('Allo_Standard before removing nan & error records :', Allo_Standard.select('HouseNo').count()) # 371708\n\n# order ServiceType values to keep 'FTTH' over 'VDSL' when de-duping (if it exists). To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Combined_Building','HouseNo','Street_Type_1', 'Street_1_New','Street_Type_2','Street_2_New','AREA','STD_CITY','STATE','POSTCODE','Standard_Building_Name']).orderBy(f.col(\"ServiceType\").asc())\nAllo_Standard = Allo_Standard.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\n# this is the pandas de-dupe code: Allo_Standard = Allo_Standard.sort_values(['ServiceType']).drop_duplicates( keep = 'first')\nAllo_Standard = Allo_Standard.filter(f.col(\"ServiceType\") != \"\")\nAllo_Standard = Allo_Standard.filter(f.col(\"ServiceType\").isNotNull())\nAllo_Standard = Allo_Standard.filter(f.col(\"ServiceType\") != \"NAN\")\nAllo_Standard = Allo_Standard.filter(f.col(\"ServiceType\") != \"ERROR\")\nprint('Allo_Standard after removing nan & error records :', Allo_Standard.select('HouseNo').count()) # 157457\n\n## convert Serviceable to \"Allo|FTTH\" or \"Allo|VDSL\"\nAllo_Standard = Allo_Standard.withColumn('Serviceable', f.concat_ws(\"|\", f.col(\"Servicable\"), f.col(\"ServiceType\")) )\n\n# print(Allo_Standard.info())\nprint(Allo_Standard.columns)\n## cast all columns as string\nAllo_Standard = Allo_Standard.select([f.col(column).cast('string') for column in Allo_Standard.columns])\n\n## save to path\nAllo_Standard.write.orc(UAMS_PySpark_save_path+'phase_2/{}/allo_temp_standard.orc'.format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_parquet(df = Allo_Standard,compression ='snappy', schema_evolution = False, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_parquet/final_2_allo_standard.snappy.parquet')\n\ndel Allo_Standard",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "this is Allo new std : 371708\nAllo_Standard before removing nan & error records : 371708\nAllo_Standard after removing nan & error records : 157457\n['Combined_Building', 'HouseNo', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'ServiceType', 'Standard_Building_Name', 'Servicable', 'Serviceable']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### CTS\n\n## read in the data & relevant columns\n# CTS_Standard = spark.read.csv(\"s3://astro-datalake-prod-sandbox/amzar/BB/other_adhoc/uploaded/address_checking/CTS_New_Standardised_202209_Reformatted-SarahLocal.csv\", header=True)\nCTS_Standard = spark.read.csv(cts_new_std_path, header=True) # to automate\n# ORI Glue job code had this argument: dtype = {'Combined_Building':object})\nCTS_Standard = CTS_Standard.select(['Combined_Building','HouseNo', 'Street_1','Street_2','AREA','STD_CITY','STATE11', 'POSTCODE20', 'ServiceType','Standard_Building_Name','match'])\n## rename columns that got changed at some point (either due to how Spark handles duplicate column names or me coalescing addr components) to fit the script that Fakhrul has on Glue job\nCTS_Standard = CTS_Standard.withColumnRenamed('STATE11', 'STATE').withColumnRenamed('POSTCODE20', 'POSTCODE')\n\n# print('this is CTS new std :', CTS_Standard.count()) # 79329\n\n## create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nCTS_Standard = CTS_Standard.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## turn 'AA' in Street_1 into blanks and same for Street_2 cases where match == 'Match'\nCTS_Standard = CTS_Standard.withColumn('Street_1', when(f.col('Street_1').cast('string').startswith('AA'), '').otherwise(f.col('Street_1')))\nCTS_Standard = CTS_Standard.withColumn('Street_2', when(f.col('match').cast('string') == 'Match', '').otherwise(f.col('Street_2')))\n\n# STRT_P1.head()\n\n## uppercase\nCTS_Standard = CTS_Standard.withColumn('Street_1', f.upper(f.col('Street_1'))).withColumn('Street_2', f.upper(f.col('Street_2')))\n\n## PySpark equivalent of defined extract_street function\nr1 = \"JALAN|JLN|LORONG|LRG|CHANGKAT|LAMAN|LAHAT|LEBUH|LEBUHRAYA|LENGKOK|LINGKARAN|PERSIARAN\"\nCTS_Standard = CTS_Standard.withColumn('Street_Type_1', f.regexp_extract(f.col('Street_1'), r1, 0) )\nCTS_Standard = CTS_Standard.withColumn('Street_Type_2', f.regexp_extract(f.col('Street_2'), r1, 0) )\n\n# STRT_P1.head()\n\n## extract the Street Name without any of the words in the Street Type List\nstreet_type_list = ['JLN','JALAN','LRG', 'LORONG','CHANGKAT', 'LAMAN', 'LAHAT', 'LEBUH', 'LEBUHRAYA', 'LENGKOK ','LINGKARAN', 'PERSIARAN' ]\nCTS_Standard = CTS_Standard.withColumn(\"Street_1_New\", f.regexp_replace(f.col(\"Street_1\"), '|'.join(street_type_list), '') )\nCTS_Standard = CTS_Standard.withColumn(\"Street_2_New\", f.regexp_replace(f.col(\"Street_2\"), '|'.join(street_type_list), '') )\n\nprint('this is CTS new std :', CTS_Standard.count()) # 79329\n\n## select relevant columns & create \"Serviceable\"\nCTS_Standard = CTS_Standard.select(['Combined_Building','HouseNo','Street_Type_1', 'Street_1_New','Street_Type_2','Street_2_New','AREA','STD_CITY','STATE', 'POSTCODE', 'ServiceType','Standard_Building_Name'])\nCTS_Standard = CTS_Standard.withColumn(\"Servicable\", f.lit('CTS'))\n\n## convert Serviceable to \"CTS|FTTH\" or \"CTS|VDSL\"\nCTS_Standard = CTS_Standard.withColumn('Serviceable', f.concat_ws(\"|\", f.col(\"Servicable\"), f.col(\"ServiceType\")) )\n\n# print(CTS_Standard.info())\nprint(CTS_Standard.columns)\n## cast all columns as string\nCTS_Standard = CTS_Standard.select([f.col(column).cast('string') for column in CTS_Standard.columns])\n\n## save to path\nCTS_Standard.write.orc(UAMS_PySpark_save_path+'phase_2/{}/cts_temp_standard.orc'.format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_parquet(df = CTS_Standard,compression ='snappy', schema_evolution = False, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_parquet/final_2_cts_standard.snappy.parquet')\n\ndel CTS_Standard",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "this is CTS new std : 79329\nthis is CTS new std : 79329\n['Combined_Building', 'HouseNo', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'ServiceType', 'Standard_Building_Name', 'Servicable', 'Serviceable']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### Maxis NGBB\n\n## read in the data & relevant columns\n# Maxis_Standard = spark.read.csv(\"s3://astro-datalake-prod-sandbox/amzar/BB/other_adhoc/uploaded/address_checking/Maxis_New_Standardised_20220321.csv\", header=True)\nMaxis_Standard = spark.read.csv(maxis_new_std_path, header=True) # to automate\n# ORI Glue job code had this argument: dtype = {'Combined_Building':object})\nMaxis_Standard = Maxis_Standard.select(['Combined_Building','HouseNo', 'Street_1','Street_2','AREA','STD_CITY','STATE27', 'POSTCODE15', 'ServiceType','Standard_Building_Name','match'])\n## rename columns that got changed at some point (either due to how Spark handles duplicate column names or me coalescing addr components) to fit the script that Fakhrul has on Glue job\nMaxis_Standard = Maxis_Standard.withColumnRenamed('STATE27', 'STATE').withColumnRenamed('POSTCODE15', 'POSTCODE')\n\n# print('this is Maxis new std :', Maxis_Standard.count()) # 130435\n\n## create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nMaxis_Standard = Maxis_Standard.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## turn 'AA' in Street_1 into blanks and same for Street_2 cases where match == 'Match'\nMaxis_Standard = Maxis_Standard.withColumn('Street_1', when(f.col('Street_1').cast('string').startswith('AA'), '').otherwise(f.col('Street_1')))\nMaxis_Standard = Maxis_Standard.withColumn('Street_2', when(f.col('match').cast('string') == 'Match', '').otherwise(f.col('Street_2')))\n\n# STRT_P1.head()\n\n## uppercase\nMaxis_Standard = Maxis_Standard.withColumn('Street_1', f.upper(f.col('Street_1'))).withColumn('Street_2', f.upper(f.col('Street_2')))\n\n## PySpark equivalent of defined extract_street function\nr1 = \"JALAN|JLN|LORONG|LRG|CHANGKAT|LAMAN|LAHAT|LEBUH|LEBUHRAYA|LENGKOK|LINGKARAN|PERSIARAN\"\nMaxis_Standard = Maxis_Standard.withColumn('Street_Type_1', f.regexp_extract(f.col('Street_1'), r1, 0) )\nMaxis_Standard = Maxis_Standard.withColumn('Street_Type_2', f.regexp_extract(f.col('Street_2'), r1, 0) )\n\n# STRT_P1.head()\n\n## extract the Street Name without any of the words in the Street Type List\nstreet_type_list = ['JLN','JALAN','LRG', 'LORONG','CHANGKAT', 'LAMAN', 'LAHAT', 'LEBUH', 'LEBUHRAYA', 'LENGKOK ','LINGKARAN', 'PERSIARAN' ]\nMaxis_Standard = Maxis_Standard.withColumn(\"Street_1_New\", f.regexp_replace(f.col(\"Street_1\"), '|'.join(street_type_list), '') )\nMaxis_Standard = Maxis_Standard.withColumn(\"Street_2_New\", f.regexp_replace(f.col(\"Street_2\"), '|'.join(street_type_list), '') )\n\nprint('this is Maxis new std :', Maxis_Standard.count()) # 130435\n\n## select relevant columns & create \"Serviceable\"\nMaxis_Standard = Maxis_Standard.select(['Combined_Building','HouseNo','Street_Type_1', 'Street_1_New','Street_Type_2','Street_2_New','AREA','STD_CITY','STATE', 'POSTCODE', 'ServiceType','Standard_Building_Name'])\nMaxis_Standard = Maxis_Standard.withColumn(\"Servicable\", f.lit('Maxis'))\n\n## convert Serviceable to \"Maxis|FTTH\" or \"Maxis|VDSL\"\nMaxis_Standard = Maxis_Standard.withColumn('Serviceable', f.concat_ws(\"|\", f.col(\"Servicable\"), f.col(\"ServiceType\")) )\n\n##just to see if serviceable is actually there for maxis\nprint('checking maxis serviceable: ', Maxis_Standard.select(\"Serviceable\").head(5))\n\n# print(Maxis_Standard.info())\nprint(Maxis_Standard.columns)\n## cast all columns as string\nMaxis_Standard = Maxis_Standard.select([f.col(column).cast('string') for column in Maxis_Standard.columns])\n\n## save to path\nMaxis_Standard.write.orc(UAMS_PySpark_save_path+'phase_2/{}/maxis_temp_standard.orc'.format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_parquet(df = Maxis_Standard,compression ='snappy', schema_evolution = False, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_parquet/final_2_maxis_standard.snappy.parquet')\n\ndel Maxis_Standard",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "this is Maxis new std : 130435\nthis is Maxis new std : 130435\nchecking maxis serviceable:  [Row(Serviceable='Maxis|FTTH'), Row(Serviceable='Maxis|FTTH'), Row(Serviceable='Maxis|FTTH'), Row(Serviceable='Maxis|FTTH'), Row(Serviceable='Maxis|FTTH')]\n['Combined_Building', 'HouseNo', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'ServiceType', 'Standard_Building_Name', 'Servicable', 'Serviceable']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Concatenate into All and further cleaning",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## Read in the saved Standard files:\nAstro_Standard = spark.read.orc(UAMS_PySpark_save_path+'phase_2/{}/astro_temp_standard.orc'.format(date_key))\nTM_Standard = spark.read.orc(UAMS_PySpark_save_path+'phase_2/{}/tm_temp_standard.orc'.format(date_key)) \nMaxis_Standard = spark.read.orc(UAMS_PySpark_save_path+'phase_2/{}/maxis_temp_standard.orc'.format(date_key))\nAllo_Standard = spark.read.orc(UAMS_PySpark_save_path+'phase_2/{}/allo_temp_standard.orc'.format(date_key))\nCTS_Standard = spark.read.orc(UAMS_PySpark_save_path+'phase_2/{}/cts_temp_standard.orc'.format(date_key))\n\n## create \"F\" column\nAstro_Standard = Astro_Standard.withColumn('F', f.lit('A_MB'))\nTM_Standard = TM_Standard.withColumn('F', f.lit('TM_MB'))\nMaxis_Standard = Maxis_Standard.withColumn('F', f.lit('MAXIS_MB'))\nAllo_Standard = Allo_Standard.withColumn('F', f.lit('ALLO_MB'))\nCTS_Standard = CTS_Standard.withColumn('F', f.lit('CTS_MB'))\n\nprint('checking unique tm serviceable', TM_Standard.select('Serviceable').distinct().show())\n\n## checking the info/columns\nprint('checking astro info: ', Astro_Standard.columns)\nprint('checking tm info: ', TM_Standard.columns)\nprint('checking maxis info: ', Maxis_Standard.columns)\nprint('checking allo info: ', Allo_Standard.columns)\nprint('checking cts info: ', CTS_Standard.columns)\n\n## just to see if serviceable is actually there for maxis and tm before becoming frame\nprint('checking maxis serviceable: ', Maxis_Standard.select('Serviceable').head(5))\nprint('checking tm serviceable: ', TM_Standard.select('Serviceable').head(5))\n\n## checking the MB here\nprint('checking tm mb here: ', TM_Standard.select('F').head(5))\nprint('checking astro mb here: ', Astro_Standard.select('F').head(5))\nprint('checking maxis mb here: ', Maxis_Standard.select('F').head(5))\n\n## Union/concat the ISP DFs (have to do the below method as Astro & ISP Std Bases don't have exactly the same columns) --> priority is important ## KIV for TIME ISP\nAll_ISP = TM_Standard.union(Maxis_Standard).union(Allo_Standard).union(CTS_Standard) ## concat ISPs first as they have the same column names\n# print(All_ISP.select('STD_CITY').filter(f.col('STD_CITY') == '').count()) # 0 blank STD_CITY\n\n## generating columns that ISPs have which Astro Std Base does not have & vice versa\nfor column in [column for column in All_ISP.columns if column not in Astro_Standard.columns]:\n    Astro_Standard = Astro_Standard.withColumn(column, f.lit(None))\nfor column in [column for column in Astro_Standard.columns if column not in All_ISP.columns]:\n    All_ISP = All_ISP.withColumn(column, f.lit(None))\n\n## rearranging columns to enable smooth Union-ing\nAstro_Standard = Astro_Standard.select(['OBJID', 'ACCOUNT_NO', 'HouseNo', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'Standard_Building_Name', 'F', 'ServiceType', 'Servicable', 'Serviceable'])\nAll_ISP = All_ISP.select(['OBJID', 'ACCOUNT_NO', 'HouseNo', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'Standard_Building_Name', 'F', 'ServiceType', 'Servicable', 'Serviceable'])\nAll = Astro_Standard.union(All_ISP)\nprint('All count after UNION-ing all Std Bases:', All.select('HouseNo').count())\n\n## validation to show that the above UNION step works ok\n# count of blank/null STD_CITY --> \nprint('no of rows with null std_city', All.filter(f.col('STD_CITY').isNull()).select('STATE').count()) # 88692\nprint('no of rows with blank std_city', All.filter(f.col('STD_CITY') == '').select('STATE').count()) # 0\n# count of blank AREA --> print(All.filter(f.col('AREA') == '').count()) # 0\n# count of blank STATE --> print(All.filter(f.col('STATE') == '').count()) # 0\n# count of blank POSTCODE --> print(All.filter(f.col('POSTCODE') == '').count()) # 0\n# count of blank Street_Type_1 --> print(All.filter(f.col('Street_Type_1') == '').count()) # 608635\n# print(All_ISP.filter(f.col('Street_Type_1') == '').select('STATE').count()) # 40196\n# print(Astro_Standard.filter(f.col('Street_Type_1') == '').select('STATE').count()) # 568439\n\n# OLD CODE: All = Astro_Standard.unionByName(All_ISP) --> found this code leads to a lot of blanks in many columns. So switched to new method where I rearrange the columns using select() then use normal union()\n# print(All.select('STD_CITY').filter(f.col('STD_CITY') == '').count()) # 6523217 blank STD_CITY --> means something is wrong with the unionByName function\n\n# print('count of All after union/concat', All.select('F').count()) ## 11,937,295\n# print('checking weird housenumbers here : ', All.filter(f.col('HouseNo') == '*').head(5))\n# print('All serviceable unique here :', All.select('Serviceable').distinct().show())\n# print('Checking All here after concat: ', All.select('Serviceable').head(5))\n\n## create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nAll = All.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\nprint('Checking All df after reset index: ', All.select('Serviceable').count()) ## looks ok\nprint('checking all info here: ', All.columns)\n\n# print(All.shape)\n\n# print('checking serviceable values here for All before code below: ', All.select('Serviceable').head(10))\n\n## cleaning any weird values like '\\\\|'\nAll = All.withColumn('Serviceable', when(f.col('Serviceable').cast('string') == '\\\\|', '').otherwise(f.col('Serviceable')))\nprint('checking serviceable values here for All: ', All.select('Serviceable').distinct().show())\nprint('no of rows with blank std_city', All.filter(f.col('STD_CITY') == '').select('STATE').count()) # 0\n\n## clean POSTCODE & fill all nulls\nAll = All.withColumn('POSTCODE', f.regexp_replace(f.col('POSTCODE').cast('string'), '\\.0', '') )\nAll = All.fillna('')\nprint('no of rows with blank std_city', All.filter(f.col('STD_CITY') == '').select('STATE').count()) # 88692\n\n## Cleaning STATE values\nAll = All.withColumn('STATE', f.upper(f.trim(f.col('STATE').cast('string'))))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'FEDERAL TERRITORY OF KUALA LUMPUR','WIL'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'WILAYAH PERSEKUTUAN KUALA LUMPUR','WIL'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'WIL KL','WIL'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'KL','WIL'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'LKG','KEDAH'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'SELANGOR','SEL'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'SEL','SELANGOR'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'JOHOR','JOH'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'JOH','JOHOR'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'MELAKA','MEL'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'MEL','MELAKA'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'PULAU PINANG','PNG'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'PENANG','PNG'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'PINANG','PNG'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'PNG','PULAU PINANG'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'PERAK','PRK'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'PRK','PERAK'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'PERLIS','PLS'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'PLS','PERLIS'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'SABAH','SAB'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'SAB','SABAH'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'SARAWAK','SAR'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'SAR','SARAWAK'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'TERENGGANU','TRG'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'TRG','TERENGGANU'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'PAHANG','PHG'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'PHG','PAHANG'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'KEDAH','KED'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'KED','KEDAH'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'NEGERI SEMBILAN','NEG'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'NEG','NEGERI SEMBILAN'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'KELANTAN','KEL'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'KEL','KELANTAN'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'WILAYAH PERSEKUTUAN PUTRAJAYA','PUTRAJAYA'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'WIL PUTRAJAYA','PUTRAJAYA'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'WILAYAH PERSEKUTUAN PUTRAJAYA','PUTRAJAYA'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'WILAYAH PERSEKUTUAN LABUAN','LAB'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'WILAYAH LABUAN','LAB'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'WIL LABUAN','LAB'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'LABUAN','LAB'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'LAB','LABUAN'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'WILAYAH PERSEKUTUAN','WIL'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'WIL','WILAYAH PERSEKUTUAN KUALA LUMPUR'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'LGK','KEDAH'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'SIN','SINGAPORE'))\nAll = All.withColumn('STATE', f.regexp_replace(f.col('STATE'), 'PJY','PUTRAJAYA'))\n\nprint('no of rows with blank std_city', All.filter(f.col('STD_CITY') == '').select('STATE').count()) # 88692\n\n## save intermediate table: mainly coz the above step (cleaning STATE) took 2.5 mins & I'm worried the notebook might crash\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate1.orc\".format(date_key) , mode='overwrite', compression='snappy')\nprint(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate1.orc\".format(date_key))\n\nprint('no of rows with blank std_city', All.filter(f.col('STD_CITY') == '').select('STATE').count()) # 88692\nprint('All count after some filtering + cleaning STATE column', All.select('STATE').count()) # \n\n\n### ------------- more codes (mainly for checking numbers) --------------\n## checking how many nulls are in some columns of different DFs\n# print(Astro_Standard.filter(f.col('Street_Type_1').isNull()).select('STATE').count()) # 38946\n# print(All_ISP.filter(f.col('Street_Type_1').isNull()).select('STATE').count()) # 6552534\n# print(TM_Standard.filter(f.col('Street_Type_1').isNull()).select('STATE').count()) # 6545104\n## looking at no of STATE values with 'AA'\n# All.select('STATE').distinct().show(100) ## --> a lot of values that start with 'AA'. Also some non-STATE values e.g MERLIMAU, IPOH, NILAI\n# print(All.filter(f.col(\"STATE\").startswith('AA')).select('STATE').count()) # 21140\n## using old unionByName function:\n# count of blank AREA --> print(All.filter(f.col('AREA') == '').count()) # 566840\n# count of blank STATE --> print(All.filter(f.col('STATE') == '').count()) # 1797568\n# count of blank POSTCODE --> print(All.filter(f.col('POSTCODE') == '').count()) # 68948\n# count of blank Street_Type_1 --> print(All.filter(f.col('Street_Type_1') == '').count()) # 7147449\n\n## seeing table where STD_CITY is blank\n# z.show(All.filter(f.col('STD_CITY') == '').head(100))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+\n|         Serviceable|\n+--------------------+\n|TM|1-G-8 JALAN PU...|\n|        TM|SHAH ALAM|\n| TM|BLOK KOMERSIAL A|\n|TM|5425 JALAN LUB...|\n|TM|LOT 411 JALAN ...|\n|           TM|TUARAN|\n|     TM|KUALA LUMPUR|\n|    TM|KUBOR PANJANG|\n|             TM|VDSL|\n|             TM|FTTH|\n+--------------------+\n\nchecking unique tm serviceable None\nchecking astro info:  ['OBJID', 'ACCOUNT_NO', 'HouseNo', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'Standard_Building_Name', 'F']\nchecking tm info:  ['Combined_Building', 'HouseNo', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'ServiceType', 'Standard_Building_Name', 'Servicable', 'Serviceable', 'F']\nchecking maxis info:  ['Combined_Building', 'HouseNo', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'ServiceType', 'Standard_Building_Name', 'Servicable', 'Serviceable', 'F']\nchecking allo info:  ['Combined_Building', 'HouseNo', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'ServiceType', 'Standard_Building_Name', 'Servicable', 'Serviceable', 'F']\nchecking cts info:  ['Combined_Building', 'HouseNo', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'ServiceType', 'Standard_Building_Name', 'Servicable', 'Serviceable', 'F']\nchecking maxis serviceable:  [Row(Serviceable='Maxis|FTTH'), Row(Serviceable='Maxis|FTTH'), Row(Serviceable='Maxis|FTTH'), Row(Serviceable='Maxis|FTTH'), Row(Serviceable='Maxis|FTTH')]\nchecking tm serviceable:  [Row(Serviceable='TM|FTTH'), Row(Serviceable='TM|FTTH'), Row(Serviceable='TM|FTTH'), Row(Serviceable='TM|FTTH'), Row(Serviceable='TM|FTTH')]\nchecking tm mb here:  [Row(F='TM_MB'), Row(F='TM_MB'), Row(F='TM_MB'), Row(F='TM_MB'), Row(F='TM_MB')]\nchecking astro mb here:  [Row(F='A_MB'), Row(F='A_MB'), Row(F='A_MB'), Row(F='A_MB'), Row(F='A_MB')]\nchecking maxis mb here:  [Row(F='MAXIS_MB'), Row(F='MAXIS_MB'), Row(F='MAXIS_MB'), Row(F='MAXIS_MB'), Row(F='MAXIS_MB')]\nno of rows with null std_city 135692\nno of rows with blank std_city 0\nchecking all info here:  ['OBJID', 'ACCOUNT_NO', 'HouseNo', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'Standard_Building_Name', 'F', 'ServiceType', 'Servicable', 'Serviceable', 'index']\n+--------------------+\n|         Serviceable|\n+--------------------+\n|          Maxis|FTTH|\n|       ALLO|JELUTONG|\n|                null|\n|TM|1-G-8 JALAN PU...|\n|        TM|SHAH ALAM|\n|   ALLO|PULAU PINANG|\n|          Maxis|VDSL|\n| TM|BLOK KOMERSIAL A|\n|            CTS|FTTH|\n|TM|5425 JALAN LUB...|\n|TM|LOT 411 JALAN ...|\n|           TM|TUARAN|\n|           ALLO|FTTH|\n|     TM|KUALA LUMPUR|\n|    TM|KUBOR PANJANG|\n|             TM|VDSL|\n|             TM|FTTH|\n+--------------------+\n\nchecking serviceable values here for All:  None\nno of rows with blank std_city 0\nno of rows with blank std_city 135692\nno of rows with blank std_city 135692\ns3://astro-groupdata-prod-pipeline/address_standardization/spark_uams_generation/phase_2/20221120/All-intermediate1.orc\n14741778\nno of rows with blank std_city 135692\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "del All # -- DELETE previous df if required\n# read back in the intermediate table\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate1.orc\".format(date_key))\n\n## HouseNo\nAll = All.withColumn(\"HouseNo\", f.regexp_replace(\"HouseNo\", \"#|,|'\",'')) ## this seems to cover multiple cases & runs faster than having multiple lines for each symbol to regexp_replace\n\n## Combined_Building\n_list = ['#', ',', '/', '-', 'No Name', '\\.', '\\*', '=', ':','\\)', '\\(', '`', '_', '\\^'] #revision - zohreh - 5/8/22 - uams complain about carrot\nAll = All.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '|'.join(_list), '')) ## this seems to cover multiple cases & runs faster than having multiple lines for each symbol to regexp_replace\n\n## extra code kept here just in case we need it again\n# All[\"Combined_Building\"]= np.where( All[\"Combined_Building\"]=='0', '', All[\"Combined_Building\"])\n#All = All.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '  ',''))\n#All[\"Combined_Building\"] = All[\"Combined_Building\"].str.strip()\n#All = All.withColumn(\"Combined_Building\", f.regexp_replace(\"Combined_Building\", '(^|\\s)($|\\s)','', case = False, regex = True)\n\n## assigning SDU, MDU to Address_Type\nAll = All.withColumn('Address_Type', when( ((f.col('Combined_Building').isNull()) | (f.col('Combined_Building') == '')), 'SDU').otherwise('MDU') )\n\n## Street_Type_1\n_list = ['#', ',', '/', '-', 'No Name']\nAll = All.withColumn(\"Street_Type_1\", f.regexp_replace(\"Street_Type_1\", '|'.join(_list), ''))\nAll = All.withColumn(\"Street_Type_1\", f.regexp_replace(\"Street_Type_1\", 'JLN','JALAN'))\nAll = All.withColumn(\"Street_Type_1\", f.regexp_replace(\"Street_Type_1\", 'LRG','LORONG'))\n\n## Street_Type_2\n_list = ['#', ',', '/', '-', 'No Name']\nAll = All.withColumn(\"Street_Type_2\", f.regexp_replace(\"Street_Type_2\", '|'.join(_list), ''))\nAll = All.withColumn(\"Street_Type_2\", f.regexp_replace(\"Street_Type_2\", 'JLN','JALAN'))\nAll = All.withColumn(\"Street_Type_2\", f.regexp_replace(\"Street_Type_2\", 'LRG','LORONG'))\n\n## Street_1_New\n_list = ['#', ',', 'No Name']\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(\"Street_1_New\", '|'.join(_list), ''))\n\n## Street_2_New\n_list = ['#',',', 'No Name']\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(\"Street_2_New\",'|'.join(_list), ''))\n\n## AREA\n_list = ['#',',', '/','-', 'No Name']\nAll = All.withColumn(\"AREA\", f.regexp_replace(\"AREA\", '|'.join(_list), ''))\n\n## STD_CITY -- there seemed to be a lot of nulls or blanks after this step -- to check before & after\nprint('Distinct count of STD_CITY before converting to blanks: ', All.select(f.countDistinct('STD_CITY')).show()) # 10914\n# print(All.select('STD_CITY').filter(f.col('STD_CITY').isNull()).count()) # 0\nprint('No of rows where STD CITY is blank from All df', All.select('STD_CITY').filter(f.col('STD_CITY') == '').count()) # 6523217 --> lots of blanks to begin with... with this number, it's probably due to TM (6mil) --> FIXED\n# print(All.select('STD_CITY').filter(f.col('STD_CITY') == '\\[\\]').count()) # 0\n\n_list = ['#',',', '/','-', '=', ':','\\)', '\\(', 'No Name','\\[','\\]']\nAll = All.withColumn(\"STD_CITY\", f.regexp_replace(\"STD_CITY\", '|'.join(_list), ''))\n\nprint('Distinct count of STD_CITY AFTER converting to blanks: ', All.select(f.countDistinct('STD_CITY')).show()) # 10898 --> only reduced by 16 from before this cleaning step\n# print(All.select('STD_CITY').filter(f.col('STD_CITY').isNull()).count()) # 0\nprint(All.select('STD_CITY').filter(f.col('STD_CITY') == '').count()) # 6523228\n# print(All.select('STD_CITY').filter(f.col('STD_CITY') == '\\[\\]').count()) # 0\n\n## STATE\n_list = ['#',',', '/','-', 'No Name']\nAll = All.withColumn(\"STATE\", f.regexp_replace(\"STATE\", '|'.join(_list), ''))\n\n## POSTCODE\n_list = ['#',',', '/','-', 'No Name']\nAll = All.withColumn(\"POSTCODE\", f.regexp_replace(\"POSTCODE\", '|'.join(_list), ''))\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\nprint('All count at end of pipeline 2 backup 1:', All.select('STATE').count()) # 11937295\n\nprint(All.columns)\n## cast all columns as string\nAll = All.select([f.col(column).cast('string') for column in All.columns])\n\n## Save before next step of Pipeline 2\n# Fakhrul: will probably split here for pipeline 2 backup\nAll.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate2.csv.gz\".format(date_key), mode='overwrite', header=True, compression='gzip')\n# wr.s3.to_csv(df = All, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/all_temp_final_2_backup_before_2.csv')\n\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate2.orc\".format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_parquet(df = All,compression ='snappy', schema_evolution = False, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_parquet/all_from_final_2_backup_target_1.snappy.parquet')\n\n\n### --- further codes for checking outputs -----\n## 7/11/2022: found that STD_CITY has a LOT of blanks (around 6 mil). To analyze it a bit:\n# All.filter(f.col('STD_CITY') =='').select('Serviceable').distinct().show(100) ## seems like these many blank STD_CITY come from TM, Maxis & Allo file only\n# All.filter(f.col('STD_CITY') =='').select('Servicable').distinct().show(100)\n## see how many cases from each ISP\n# print(All.filter(f.col('STD_CITY') =='').filter(f.col('Servicable') == 'TM').select('Serviceable').count()) # 6492440 --> NEED TO FIX this one especially!!\n# print(All.filter(f.col('STD_CITY') =='').filter(f.col('Servicable') == 'ALLO').select('Serviceable').count()) # 62\n# print(All.filter(f.col('STD_CITY') =='').filter(f.col('Servicable') == 'Maxis').select('Serviceable').count()) # 486",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------------------+\n|count(DISTINCT STD_CITY)|\n+------------------------+\n|                   11111|\n+------------------------+\n\nDistinct count of STD_CITY before converting to blanks:  None\n135692\n+------------------------+\n|count(DISTINCT STD_CITY)|\n+------------------------+\n|                   11090|\n+------------------------+\n\nDistinct count of STD_CITY AFTER converting to blanks:  None\n136742\n14741778\n['OBJID', 'ACCOUNT_NO', 'HouseNo', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'Standard_Building_Name', 'F', 'ServiceType', 'Servicable', 'Serviceable', 'index', 'Address_Type']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF PIPELINE 2 - BACKUP 2 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_2_backup_2\n### Preparing UAMS format for all standardised base - TM and ISPs\n\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate2.orc\".format(date_key))\n# All = spark.read.csv(UAMS_PySpark_save_path+\"all_temp_final_2_backup_before_2_{}.csv\".format(date_key), header=True)\n# All = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/all_temp_final_2_backup_before_2.csv')\n\n## trim the columns\nAll = All.withColumn(\"HouseNo\", f.trim(f.col(\"HouseNo\")) )\nAll = All.withColumn(\"Combined_Building\", f.trim(f.col(\"Combined_Building\")) )\nAll = All.withColumn(\"Street_1_New\", f.trim(f.col(\"Street_1_New\")) )\nAll = All.withColumn(\"Street_2_New\", f.trim(f.col(\"Street_2_New\")) )\nAll = All.withColumn(\"Street_Type_1\", f.trim(f.col(\"Street_Type_1\")) )\nAll = All.withColumn(\"Street_Type_2\", f.trim(f.col(\"Street_Type_2\")) )\nAll = All.withColumn(\"POSTCODE\", f.trim(f.col(\"POSTCODE\")) )\nAll = All.withColumn(\"AREA\", f.trim(f.col(\"AREA\")) )\nAll = All.withColumn(\"STD_CITY\", f.trim(f.col(\"STD_CITY\")) )\nAll = All.withColumn(\"STATE\", f.trim(f.col(\"STATE\")) )\n\nprint('All count at end of Pipeline 2 Backup 2:', All.select('STATE').count()) # 11937295\n\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate3.orc\".format(date_key), mode='overwrite', compression='snappy')\n# All.coalesce(1).write.csv(UAMS_PySpark_save_path+\"all_temp_final_2_backup_before_2_2_{}.csv\".format(date_key), mode='overwrite', header=True)\n# wr.s3.to_csv(df = All, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/all_temp_final_2_backup_before_2_2.csv')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "14741778\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF PIPELINE 2 - FINAL ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_2\n### Preparing UAMS format for all standardised base - TM and ISPs\n\ndel All # -- if required\n# All = spark.read.csv(UAMS_PySpark_save_path+\"all_temp_final_2_backup_before_2_2_{}.csv\".format(date_key), header=True)\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate3.orc\".format(date_key))\n\n### Not sure what code below does but I think it's to replace anything in string printable (from string import printable) with ''\n## Code in question:\n# All[\"HouseNo\"] = All[\"HouseNo\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# All[\"Combined_Building\"] = All[\"Combined_Building\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# All[\"Street_1_New\"] = All[\"Street_1_New\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# All[\"Street_Type_1\"] = All[\"Street_Type_1\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# All[\"Street_2_New\"] = All[\"Street_2_New\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# All[\"Street_Type_2\"] = All[\"Street_Type_2\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n\n# All[\"POSTCODE\"] = All[\"POSTCODE\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# All[\"AREA\"] = All[\"AREA\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# All[\"STD_CITY\"] = All[\"STD_CITY\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n# All[\"STATE\"] = All[\"STATE\"].map(str).apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n\n# wr.s3.to_csv(df = All, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/all_checking_after_encrypt_test.csv')\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\n## fill nulls\nAll = All.fillna('')\n\n# print('All shape here check')\n# print(All.shape)\n\n# test = All [All['Area']=='AAEFF']\n# test\n\nprint('All count before filtering out blanks : ', All.count()) # 11937295\n\n## Filter out null values in POSTCODE, STD_CITY, Street_Type_1, Street_1_New, STATE\nAll = All.filter((f.col('POSTCODE').isNotNull()) & (f.col('POSTCODE') != '') & (f.col('POSTCODE') != '\\[\\]'))\nprint('All count AFTER filtering out blank postcode : ', All.select('POSTCODE').count()) #  11868347\nAll = All.filter((f.col('STD_CITY').isNotNull()) & (f.col('STD_CITY') != '') & (f.col('STD_CITY') != '\\[\\]'))\nprint('All count AFTER filtering out blank city : ', All.select('POSTCODE').count()) # 5372327\nAll = All.filter((f.col('Street_Type_1').isNotNull()) & (f.col('Street_Type_1') != '') & (f.col('Street_Type_1') != '\\[\\]'))\nprint('All count AFTER filtering out blank Street Type 1 : ', All.select('POSTCODE').count()) # 4738152\nAll = All.filter((f.col('Street_1_New').isNotNull()) & (f.col('Street_1_New') != '') & (f.col('Street_1_New') != '\\[\\]'))\nprint('All count AFTER filtering out blank Street 1 New : ', All.select('POSTCODE').count()) # 4710896\nAll = All.filter((f.col('STATE').isNotNull()) & (f.col('STATE') != '') & (f.col('STATE') != '\\[\\]'))\n\nprint('All count AFTER filtering out all blanks : ', All.count()) # 4615662 \n\n## replace 'AA' AREA with blanks\nAll = All.withColumn('AREA', when(f.col('AREA').cast('string').startswith('AA'), '').otherwise(f.col('AREA')))\n\n## filter out 'AA' STD_CITY, STATE and POSTCODE with alphabets\nAll = All.filter(~f.col('STD_CITY').startswith('AA')).filter(~f.col('STATE').startswith('AA')).filter(~f.col('POSTCODE').rlike('[a-zA-Z]'))\n\nprint('All count AFTER filtering out \"AA\" values : ', All.count()) # 4475248\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes): before ending 2nd')\n# print(usage)\n\n# print(All.shape)\n\n## ensure POSTCODE is only 5 digits long\nAll = All.withColumn(\"POSTCODE\", f.regexp_replace(f.col('POSTCODE').cast('string'), '\\.0', '') )\nAll = All.withColumn(\"POSTCODE\", f.substring(f.col('POSTCODE'), 1, 5) )\nAll = All.withColumn('POSTCODE', f.lpad(f.col('POSTCODE').cast('string'), 5, '0') )\n\n# print(All.shape)\n\n## check that POSTCODE is only len = 5\nprint(All.select(f.length(f.col('POSTCODE'))).distinct().show())\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\nprint(All.columns)\nprint('All count at end of Pipeline 2 Final: ', All.select('POSTCODE').count()) # 4475248\n\n## save as csv\nAll.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate4.orc\".format(date_key), mode='overwrite', header=True, compression='gzip')\n# All.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_2/{}/All_temp_final_2_{}.csv\".format(date_key), mode='overwrite', header=True)\n# wr.s3.to_csv(df = All, path = all_temp_path + 'all_temp_final_2.csv')\n\n## cast all columns as string\nAll = All.select([f.col(column).cast('string') for column in All.columns])\n\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate4.orc\".format(date_key), mode='overwrite', compression='snappy')\n# All.write.orc(UAMS_PySpark_save_path+\"all_temp_2_final_target_{}.orc\".format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_parquet(df = All,compression ='snappy', schema_evolution = False, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_parquet/all_temp_2_final_target.snappy.parquet')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "All count before filtering out blanks :  14741778\nAll count AFTER filtering out blank postcode :  14660249\nAll count AFTER filtering out blank city :  14547295\nAll count AFTER filtering out blank Street Type 1 :  13574972\nAll count AFTER filtering out blank Street 1 New :  13540186\nAll count AFTER filtering out all blanks :  10815879\nAll count AFTER filtering out \"AA\" values :  10574543\n+----------------+\n|length(POSTCODE)|\n+----------------+\n|               5|\n+----------------+\n\nNone\n['OBJID', 'ACCOUNT_NO', 'HouseNo', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'Standard_Building_Name', 'F', 'ServiceType', 'Servicable', 'Serviceable', 'index', 'Address_Type']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Pipeline 3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF PIPELINE 3 - BACKUP 1 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_3_backup\n### Preparing UAMS format for all standardised base - Astro and ISPs (still doing this). Eventhough we're in Pipeline 3, we are still in PHASE 2 according to Zohreh's documentation) \n\n# all_final_2_temp_path = args['all_final_2_temp_path'] = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/all_temp_final_2.csv\n\n#Revision - 29/6/22 - fakhrul - remove dtype for OBJID as object\n# All = wr.s3.read_csv(path = all_final_2_temp_path, usecols = ['Address_Type','Standard_Building_Name',\"OBJID\",\"ACCOUNT_NO\",\"HouseNo\",\"Combined_Building\",\"Street_2_New\",\"Street_Type_2\",'Street_Type_1','Street_1_New','POSTCODE','STD_CITY','AREA','STATE',\"Combined_Building\",'Serviceable','Servicable','ServiceType'], dtype = {'OBJID':object, 'ACCOUNT_NO':object}, engine = 'c')\n#wr.s3.to_parquet(df = All,compression ='snappy', path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_parquet/all_temp_3_step1_source.snappy.parquet')\n\ndel All\n# All = spark.read.csv(UAMS_PySpark_save_path+\"all_temp_final_2_{}.csv\".format(date_key), header=True)\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate4.orc\".format(date_key))\n\n#REVISION -21/6/22 - fakhrul testing this one out to see if it works\n#All['HouseNo'] = All['HouseNo'].str.upper()\n#All[\"HouseNo\"] = All[\"HouseNo\"].str.replace(\"JAN-\",\"1/\", case = False)\n# ... can copy this code from other addr std notebooks\n\n#revision - 20/6/22 - fakhrul - postcode might be affected so have to str pad again\nAll = All.withColumn(\"POSTCODE\", f.regexp_replace(f.col('POSTCODE').cast('string'), '\\.0', '') )\nAll = All.withColumn(\"POSTCODE\", f.substring(f.col('POSTCODE'), 1, 5) )\nAll = All.withColumn('POSTCODE', f.lpad(f.col('POSTCODE').cast('string'), 5, '0') )\n## check that POSTCODE is only len = 5\n# print(All.select(f.length(f.col('POSTCODE'))).distinct().show())\n\n## encode then decode each column into ascii\nAll = All.withColumn(\"HouseNo\", f.decode(f.encode(f.col('HouseNo'), 'ascii'), 'ascii') )\nAll = All.withColumn(\"Combined_Building\", f.decode(f.encode(f.col('Combined_Building'), 'ascii'), 'ascii') )\nAll = All.withColumn(\"Street_1_New\", f.decode(f.encode(f.col('Street_1_New'), 'ascii'), 'ascii') )\nAll = All.withColumn(\"Street_2_New\", f.decode(f.encode(f.col('Street_2_New'), 'ascii'), 'ascii') )\nAll = All.withColumn(\"Street_Type_2\", f.decode(f.encode(f.col('Street_Type_2'), 'ascii'), 'ascii') )\nAll = All.withColumn(\"Street_Type_1\", f.decode(f.encode(f.col('Street_Type_1'), 'ascii'), 'ascii') )\nAll = All.withColumn(\"POSTCODE\", f.decode(f.encode(f.col('POSTCODE').cast('string'), 'ascii'), 'ascii') )\nAll = All.withColumn(\"STD_CITY\", f.decode(f.encode(f.col('STD_CITY'), 'ascii'), 'ascii') )\nAll = All.withColumn(\"AREA\", f.decode(f.encode(f.col('AREA'), 'ascii'), 'ascii') )\nAll = All.withColumn(\"STATE\", f.decode(f.encode(f.col('STATE'), 'ascii'), 'ascii') )\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\n## save as csv\nAll.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate5.csv.gz\".format(date_key), mode='overwrite', header=True, compression='gzip')\n# wr.s3.to_csv(df = All, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/all_temp_3_backup_before_3.csv')\n\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate5.orc\".format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_parquet(df = All,compression ='snappy', path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_parquet/all_temp_3_step1_target.snappy.parquet')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF PIPELINE 3 - BACKUP 2 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_3_backup_2\n### Preparing UAMS format for all standardised base - TM and ISPs (still doing this). Eventhough we're in pipeline 3, it's still part of PHASE 2 according to Zohreh's documentation\n\ndel All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate5.orc\".format(date_key)) ## read in ORC version\n# All = spark.read.csv(UAMS_PySpark_save_path+\"all_temp_3_backup_before_3_{}.csv\".format(date_key), header=True) ## read in CSV version\n# All = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/all_temp_3_backup_before_3.csv')\n\n#revision - fakhrul - 1/7/22 - add these codes so that its proper now \nAll = All.withColumn(\"POSTCODE\", f.regexp_replace(f.col('POSTCODE').cast('string'), '\\.0', '') )\nAll = All.withColumn(\"POSTCODE\", f.substring(f.col('POSTCODE'), 1, 5) )\nAll = All.withColumn('POSTCODE', f.lpad(f.col('POSTCODE').cast('string'), 5, '0') )\n\n## change nan to blanks\nAll = All.withColumn('Combined_Building', f.regexp_replace(f.upper(f.trim(f.col('Combined_Building'))), 'NAN', ''))\n\n## assigning SDU, MDU to Address_Type\nAll = All.withColumn('Address_Type', when( ((f.col('Combined_Building').isNull()) | (f.col('Combined_Building') == '')), 'SDU').otherwise('MDU') )\n\n## create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nAll = All.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n# print('checking unique postcode values: ', All.POSTCODE.value_counts())\n\n## Cleaning up STD_CITY, POSTCODE and STATE MANUALLY (code originally from Maryam/Zohreh)\nAll = All.withColumn(\"STD_CITY\", f.regexp_replace(f.col(\"STD_CITY\"), 'KUALA LUMPUR','KL') )\nAll = All.withColumn(\"STD_CITY\", when(f.col('STD_CITY') == 'KL', 'KUALA LUMPUR')\n                                .when(f.col(\"STD_CITY\") == 'WILAYAH PERSEKUTUANERSEKUTUAN',  'WILAYAH PERSEKUTUAN')\n                                .when(f.col(\"STD_CITY\") == 'MENGGATALKOTA KINABALU',  'MENGGATAL')\n                                .when(f.col(\"STD_CITY\") == 'TUARANKOTA KINABALU',  'TUARAN')\n                                .when(f.col(\"STATE\") == 'KUANTAN', 'KUANTAN')\n                                .otherwise(f.col('STD_CITY')) )\n\nAll = All.withColumn(\"POSTCODE\", when(f.col(\"POSTCODE\") == '96000', 'SARAWAK').otherwise(f.col(\"POSTCODE\")) )\n\n## save intermediate table\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate6.orc\".format(date_key), mode='overwrite', compression='snappy')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "del All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate6.orc\".format(date_key)) ## read in ORC version\n\nAll = All.withColumn(\"STATE\", when(f.col(\"STATE\") == 'WILAYAH PERSEKUTUAN KUALA LUMPURANG', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") == 'W\\.P\\.', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") == 'PHI', 'PULAU PINANG')\n                                .when(f.col(\"STATE\") == 'SEREMBAN', 'NEGERI SEMBILAN')\n                                .when(f.col(\"STATE\") == 'KUANTAN', 'PAHANG')\n                                .when(f.col(\"STATE\") == 'NEGERI SEMBILANERISEMBILAN', 'NEGERI SEMBILAN')\n                                .when(f.col(\"STATE\") == 'SENAWANG', 'NEGERI SEMBILAN')\n                                .when(f.col(\"STATE\") == 'SUNGAI PETANI', 'KEDAH')\n                                .when(f.col(\"STATE\") == 'PORT DICKSON', 'NEGERI SEMBILAN')\n                                .when(f.col(\"STATE\") == 'PENNSYLVANIA', 'PULAU PINANG')\n                                .when(f.col(\"STATE\") == 'CHERAS', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .otherwise(f.col(\"STATE\")) )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate7.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate7.orc\".format(date_key)) ## read in ORC version\n\nAll = All.withColumn(\"STATE\", when(f.col(\"STATE\") == 'GEORGETOWN', 'PULAU PINANG')\n                                .when(f.col(\"STATE\") == 'WP KUALA LUMPUR', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") == 'KUALA TERENGGANU', 'TERENGGANU')\n                                .when(f.col(\"STATE\") == 'IPOH', 'PERAK')\n                                .when(f.col(\"STATE\") == 'LABUAN FEDERAL TERRITORY', 'LABUAN')\n                                .when(f.col(\"STATE\") == 'PASIR PUTEH', 'KELANTAN')\n                                .when(f.col(\"STATE\") == 'ALOR SETAR', 'KEDAH')\n                                .when(f.col(\"STATE\") == 'BATU CAVES', 'SELANGOR')\n                                .when(f.col(\"STATE\") == 'PETALING JAYA', 'SELANGOR')\n                                .when(f.col(\"STATE\") == 'BANTING', 'SELANGOR')\n                                .when(f.col(\"STATE\") == 'PEKAN NANAS', 'JOHOR')\n                                .when(f.col(\"STATE\") == 'KUALA KANGSARAWAK', 'PERAK')\n                                .when(f.col(\"STATE\") == 'WP', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") == 'BANGSARAWAK', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .otherwise(f.col(\"STATE\")) )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate8.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate8.orc\".format(date_key)) ## read in ORC version\n\nAll = All.withColumn(\"STATE\", when(f.col(\"STATE\") == 'SRI HARTAMAS', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") == 'SENTUL', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") == 'SEGAMBUT', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") == 'KUALA LUMPUR', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") == 'BATU CAVES', 'SELANGOR')\n                                .when(f.col(\"STATE\") ==  'WILAYAH PERSEKUTUAN KUALA LUMPUR WILAYAH PERSEKUTUAN KUALA LUMPURAYAHL 0 PERSEKUTUAN KUALA LUMPUR', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") ==  'SEPANGOR', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") ==  'SELANGORAGOR', 'SELANGOR')\n                                .when(f.col(\"STATE\") ==  '  SHAH ALAM', 'SELANGOR')\n                                .when(f.col(\"STATE\") == 'W\\.P\\.', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") == '   ', '')\n                                .when(f.col(\"STATE\") == '[]', '')\n                                .when(f.col(\"STATE\") == 'MALACCA', 'MELAKA')\n                                .when(f.col(\"STATE\") == 'WILAYAH PERSEKUTUAN KUALA LUMPURANG', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .when(f.col(\"STATE\") == 'W.P.', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                .otherwise(f.col(\"STATE\")) )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate9.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate9.orc\".format(date_key)) ## read in ORC version\n\n## filter out Postcode_length != 5\nprint('before filtering out Post_length != 5', All.select('POSTCODE').count()) # 4475248\nAll = All.withColumn(\"Post_length\", f.length(f.col('POSTCODE')) )\nAll = All.filter(f.col(\"Post_length\") == 5 )\nprint('after filtering out Post_length != 5', All.select('POSTCODE').count()) # 4439727\n\n## fill nulls again\nAll = All.fillna('')\n\n## make all columns upper case\nAll = All.withColumn('HouseNo', f.upper(f.col('HouseNo')) )\nAll = All.withColumn('Combined_Building', f.upper(f.col('Combined_Building')) )\nAll = All.withColumn('Street_Type_1', f.upper(f.col('Street_Type_1')) )\nAll = All.withColumn('Street_1_New', f.upper(f.col('Street_1_New')) )\nAll = All.withColumn('Street_Type_2', f.upper(f.col('Street_Type_2')) )\nAll = All.withColumn('Street_2_New', f.upper(f.col('Street_2_New')) )\nAll = All.withColumn('AREA', f.upper(f.col('AREA')) )\nAll = All.withColumn('STD_CITY', f.upper(f.col('STD_CITY')) )\nAll = All.withColumn('STATE', f.upper(f.col('STATE')) )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate10.orc\".format(date_key), mode='overwrite', compression='snappy')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "before filtering out Post_length != 5 10574543\nafter filtering out Post_length != 5 10489664\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "del All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate10.orc\".format(date_key)) ## read in ORC version\n\n## Fix HouseNo that are converted to date. Pyspark code taken from P2 MDU Mapping Test Qubole Zepp notebooks\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JAN-\",\"01-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JAN\",\"-01\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"FEB-\",\"02-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-FEB\",'-02'))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"MAR-\",'03-'))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-MAR\",\"-03\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"APR-\",\"04-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-APR\",\"-04\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"MAY-\",\"05-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-MAY\",\"-05\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JUN-\",\"06-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JUN\",\"-06\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JUL-\",\"07-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JUL\",\"-07\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"AUG-\",'08-'))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-AUG\",\"-08\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"SEP-\",\"09-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-SEP\",\"-09\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"OCT-\",\"10-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-OCT\",\"-10\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"NOV-\",\"11-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-NOV\",\"-11\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"DEC-\",\"12-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-DEC\",\"-12\"))\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate11.orc\".format(date_key), mode='overwrite', compression='snappy')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "del All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate11.orc\".format(date_key)) ## read in ORC version\nprint('Total count of All before splitting to date_house & not_date_house:', All.select('HouseNo').count()) # 4439727\n\n## Fix HouseNo that are converted to date (DD/MM/YYYY format). Pyspark code taken from P2 MDU Mapping Test Qubole Zepp notebooks\n# Filter date HouseNo\ndate_house = All.filter(f.regexp_extract('HouseNo', '^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$', 0) != '' ) \n# Spliting the HouseNo\ndate_house = date_house.withColumn('block_date',  f.substring(date_house.HouseNo, 1, 2))\ndate_house = date_house.withColumn('floor',  f.substring(date_house.HouseNo, 4, 2))\ndate_house = date_house.withColumn('unit',  f.substring(date_house.HouseNo, 9, 2))\n# Combine the split HouseNo with dashes: '-'\ndate_house = date_house.withColumn('HOUSE_NO_ASTRO', f.concat_ws('-', date_house.block_date, date_house.floor, date_house.unit))\n# Remove additional column created to combine HouseNo\ndate_house = date_house.drop(*['block_date','floor','unit'])\nprint('date_house count:', date_house.select('HouseNo').count()) # 8644\n    \n# Filter not date HouseNo\nnot_date_house = All.filter( f.regexp_extract('HouseNo', '^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$', 0) == '' )\nnot_date_house = not_date_house.withColumn('HOUSE_NO_ASTRO', f.col('HouseNo'))\nprint('not_date_house count:', not_date_house.select('ACCOUNT_NO').count(), 'not_date_house unique acc_no:',  not_date_house.select(f.countDistinct('ACCOUNT_NO')).show()) #  rows,  unique acc_no\n# print('not_date_house count:', not_date_house.select('HouseNo').count()) # 4431083\n\n# Append the 2 dfs (date_house, not_date_house) --> originally this was in 'final_3' but I've moved it here to consolidate all the parts of this HouseNo date cleaning step in 1 phase\nAll = date_house.union(not_date_house)\nprint('Total count of All after re-appending date_house & not_date_house (end of Pipeline 2 Final):', All.select('HouseNo').count()) # 4439727\n# create a sequential index as Zohreh did a pandas reset_index at this step again. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nAll = All.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes) before concat frame below :')\n# print(usage)\n\n## save as ORC & csv\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate12.orc\".format(date_key), mode='overwrite', compression='snappy')\n\nAll.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate12.csv.gz\".format(date_key), mode='overwrite', header=True, compression='gzip')\n# wr.s3.to_csv(df = date_house, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/date_house_3_backup_before_3_2.csv')\n# wr.s3.to_csv(df = not_date_house, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/not_date_house_3_backup_before_3_2.csv')\n\n### ------ more codes to check outputs -------\n### checking size of the All dataframe before saving. Refer: https://stackoverflow.com/questions/46228138/how-to-find-pyspark-dataframe-memory-usage\n# Need to cache the table (and force the cache to happen)\n# sample = All.sample(fraction=0.01)\n# pdf = sample.toPandas() # convert to pd DF\n# pdf.info() # 1% is 54 kB, so 100% is around 54 x 100 = 5400 kb = 5.4MB",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "Total count of All before splitting to date_house & not_date_house: 10489664\ndate_house count: 8644\n+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                   3521310|\n+--------------------------+\n\nnot_date_house count: 10481020 not_date_house unique acc_no: None\nTotal count of All after re-appending date_house & not_date_house: 10489664\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF PIPELINE 3 - FINAL ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_3\n### Preparing UAMS format for all standardised base - TM and ISPs (still doing this). Eventhough this is pipeline 3, it's still part of Phase 2 according to Zohreh's documentation\n\n# all_temp_path = args['all_temp_path'] = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/\n                          \n# date_house = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/date_house_3_backup_before_3_2.csv', usecols = ['OBJID','ACCOUNT_NO','HouseNo','AREA','STD_CITY','STATE','POSTCODE','Combined_Building','Street_Type_1','Street_1_New','Street_Type_2','Street_2_New','Standard_Building_Name','ServiceType','Servicable','Serviceable','Address_Type','Post_length','block','floor','unit','HOUSE_NO_ASTRO'], dtype = {'ACCOUNT_NO':object, 'OBJID':object})\n# not_date_house = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/not_date_house_3_backup_before_3_2.csv', usecols = ['OBJID','ACCOUNT_NO','HouseNo','AREA','STD_CITY','STATE','POSTCODE','Combined_Building','Street_Type_1','Street_1_New','Street_Type_2','Street_2_New','Standard_Building_Name','ServiceType','Servicable','Serviceable','Address_Type','Post_length','HOUSE_NO_ASTRO'], dtype = {'ACCOUNT_NO':object, 'OBJID':object})\n\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate12.orc\".format(date_key)) ## ORC version\n# All = spark.read.orc(UAMS_PySpark_save_path+\"all_date_house_3_backup_before_3_2_{}.orc\".format(date_key)) ## ORC version\n# All = spark.read.csv(UAMS_PySpark_save_path+\"all_date_house_3_backup_before_3_2_{}.csv\".format(date_key), header=True) ## CSV version\n\n#revision - 20/6/22 - fakhrul - postcode might be affected so have to str pad again\nAll = All.withColumn(\"POSTCODE\", f.regexp_replace(f.col('POSTCODE').cast('string'), '\\.0', '') )\nAll = All.withColumn(\"POSTCODE\", f.substring(f.col('POSTCODE'), 1, 5) )\nAll = All.withColumn('POSTCODE', f.lpad(f.col('POSTCODE').cast('string'), 5, '0') )\n\n## Fix Street_1_New that got converted to date format then pad with spaces\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"JAN-\",\"1/\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-JAN\",\"/1\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), 'FEB-','2/'))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), '-FEB','/2'))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"MAR-\",'3/'))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-MAR\",\"/3\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"APR-\",\"4/\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-APR\",\"/4\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"MAY-\",\"5/\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-MAY\",\"/5\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"JUN-\",\"6/\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-JUN\",\"/6\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"JUL-\",\"7/\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-JUL\",\"/7\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"AUG-\",'8/'))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-AUG\",\"/8\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"SEP-\",\"9/\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-SEP\",\"/9\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"OCT-\",\"10/\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-OCT\",\"/10\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"NOV-\",\"11/\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-NOV\",\"/11\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"DEC-\",\"12/\"))\nAll = All.withColumn(\"Street_1_New\", f.regexp_replace(f.col('Street_1_New'), \"-DEC\",\"/12\"))\n\nAll = All.withColumn('Street_1_New', f.lpad(f.col('Street_1_New').cast('string'), 10, ' ') )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate13.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate13.orc\".format(date_key)) ## read in ORC version\n\n## Fix Street_2_New that got converted to date format then pad with spaces\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"JAN-\",\"1/\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-JAN\",\"/1\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), 'FEB-','2/'))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), '-FEB','/2'))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"MAR-\",'3/'))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-MAR\",\"/3\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"APR-\",\"4/\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-APR\",\"/4\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"MAY-\",\"5/\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-MAY\",\"/5\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"JUN-\",\"6/\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-JUN\",\"/6\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"JUL-\",\"7/\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-JUL\",\"/7\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"AUG-\",'8/'))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-AUG\",\"/8\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"SEP-\",\"9/\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-SEP\",\"/9\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"OCT-\",\"10/\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-OCT\",\"/10\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"NOV-\",\"11/\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-NOV\",\"/11\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"DEC-\",\"12/\"))\nAll = All.withColumn(\"Street_2_New\", f.regexp_replace(f.col('Street_2_New'), \"-DEC\",\"/12\"))\n\nAll = All.withColumn('Street_2_New', f.lpad(f.col('Street_2_New').cast('string'), 10, ' ') )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate14.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-intermediate14.orc\".format(date_key)) ## read in ORC version\n\nAll = All.withColumn('House_No', f.lpad(f.col('HOUSE_NO_ASTRO').cast('string'), 10, ' ') )\nAll = All.drop(*['HOUSE_NO_ASTRO', 'HouseNo'])\n\n## these steps seems to be duplicate steps as above...\n# All = All.fillna('')\n# All['Combined_Building'] = All['Combined_Building'].str.upper()\n# All['Street_Type_1'] = All['Street_Type_1'].str.upper()\n# All['Street_Type_2'] = All['Street_Type_2'].str.upper()\n# All['Street_1_New'] = All['Street_1_New'].str.upper()\n# All['Street_2_New'] = All['Street_2_New'].str.upper()\n# All['AREA'] = All['AREA'].str.upper()\n# All['STD_CITY'] = All['STD_CITY'].str.upper()\n# All['ASTRO_STATE'] = All['ASTRO_STATE'].str.upper()\n\n# All[All['POSTCODE'].str.contains(\"AA\")]\n\nAll = All.withColumn(\"Key\", f.concat_ws(\" ,\", \"House_No\", \"Combined_Building\", \"Street_Type_1\", \"Street_1_New\", \"Street_Type_2\", \"Street_2_New\", \"STD_CITY\", \"AREA\", \"POSTCODE\", \"STATE\") )\nAll = All.withColumn(\"Key\", f.regexp_replace(f.upper(f.col(\"Key\")), \" \", \"\") )\n\nprint('checking on keys: ', All.select('Key').head(10))\n\n## check that POSTCODE is only len = 5\nprint('Checking postcode length of All here :', All.select(f.length(f.col('POSTCODE'))).distinct().show())\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes): after all processing')\n# print(usage)\n\n# create a sequential index as Zohreh did a pandas reset_index at this step again. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nAll = All.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\nprint('checking all account no if we have one :', All.select('ACCOUNT_NO').head(10))\n\n## de-dupe on Key & Serviceable, & keep first based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nprint('All count before de-dupe on Key & Serviceable', All.count()) # 10493617\nwindow = Window.partitionBy(['Key','Serviceable']).orderBy(f.col(\"index\").asc())\nAll = All.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\nprint('All count AFTER de-dupe on Key & Serviceable', All.count()) # 8586359\n\n# print('all shape')\n# print(All.shape)\n\n#revision - fakhrul - 4/7/22 - changing serviceable and key to str to avoid float error\nAll = All.withColumn('Key', f.col('Key').cast('string')).withColumn('Serviceable', f.col('Serviceable').cast('string'))\n\n## groupby to create All_1 --> Combine all Serviceable values for each Key\nAll_1 = All.groupBy('Key').agg(f.collect_set('Serviceable').alias('Serviceable_New'))\nAll_1 = All_1.withColumn(\"Serviceable_New\", f.concat_ws(',', f.col('Serviceable_New')) )\n# create a sequential index as Zohreh did a pandas reset_index at this step again. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nAll_1 = All_1.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\nprint('All size at end of Pipeline 3', All.count()) # 8586359\nprint('All_1 size at end of Pipeline 3:', All_1.select('Serviceable_New').count()) # 7705263\nAll_1.select('Serviceable_New').distinct().show() ## this code looks correct\n\n# print('Checking all 1 head : ', All_1.head())\n# print('Checking all info :', All.info())\n# print('Checking all head :', All.head())\n# print('Checking all shape:', All.shape)\n# print('all 1 info')\n# print(All_1.info())\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\n## save to ORC & CSV\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-final.orc\".format(date_key), mode='overwrite', compression='snappy')\nAll_1.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_1-final.orc\".format(date_key), mode='overwrite', compression='snappy')\n\nAll.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_2/{}/All-final.csv.gz\".format(date_key), mode='overwrite', header=True, compression='gzip')\nAll_1.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_2/{}/All_1-final.csv.gz\".format(date_key), mode='overwrite', header=True, compression='gzip')\n\n# All.write.orc(UAMS_PySpark_save_path+\"all_temp_final_3_{}.orc\".format(date_key), mode='overwrite', compression='snappy')\n# All_1.write.orc(UAMS_PySpark_save_path+\"all_1_temp_final_3_{}.orc\".format(date_key), mode='overwrite', compression='snappy')\n# All.coalesce(1).write.csv(UAMS_PySpark_save_path+\"all_temp_final_3_{}.csv\".format(date_key), mode='overwrite', header=True)\n# All_1.coalesce(1).write.csv(UAMS_PySpark_save_path+\"all_1_temp_final_3_{}.csv\".format(date_key), mode='overwrite', header=True)\n\n# z.show(All_1.head(100))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "checking on keys:  [Row(Key='02-03-11,PANGSAPURISUCI,JALAN,PUCHONG,,,KUALALUMPUR,TAMANSRIJATI,58200,WILAYAHPERSEKUTUANKUALALUMPUR'), Row(Key='02-08-03,PANGSAPURISUCI,JALAN,PUCHONG,,,KUALALUMPUR,TAMANSRIJATI,58200,WILAYAHPERSEKUTUANKUALALUMPUR'), Row(Key='02-10-15,PANGSAPURISUCI,JALAN,PUCHONG,,,KUALALUMPUR,TAMANSRIJATI,58200,WILAYAHPERSEKUTUANKUALALUMPUR'), Row(Key='21-01-02,KELUMPUKCAMAR,JALAN,AU2/8,,,KUALALUMPUR,TAMANSEPAKAT,54200,WILAYAHPERSEKUTUANKUALALUMPUR'), Row(Key='02-01-12,VISTAANGKASAAPARTMENT,JALAN,KERINCHI,,,KUALALUMPUR,PANTAIDALAM,59200,WILAYAHPERSEKUTUANKUALALUMPUR'), Row(Key='10-02-05,VISTAANGKASAAPARTMENT,JALAN,KERINCHI,,,KUALALUMPUR,PANTAIDALAM,59200,WILAYAHPERSEKUTUANKUALALUMPUR'), Row(Key='08-03-05,MENARAMERAKKAYANGAN,JALAN,6/56,,,KUALALUMPUR,TAMANKERAMAT,54200,WILAYAHPERSEKUTUANKUALALUMPUR'), Row(Key='26-03-02,FLATSRIMELAKA,JALAN,SIAKAP,,,KUALALUMPUR,TAMANIKHSAN,?5600,WILAYAHPERSEKUTUANKUALALUMPUR'), Row(Key='08-04-02,FLATSRIMELAKA,JALAN,SIAKAP,,,KUALALUMPUR,TAMANIKHSAN,?5600,WILAYAHPERSEKUTUANKUALALUMPUR'), Row(Key='09-04-13,CHINWOOCOURT,JALAN,GELUGOR,,,KUALALUMPUR,PUDU,55200,WILAYAHPERSEKUTUANKUALALUMPUR')]\n+----------------+\n|length(POSTCODE)|\n+----------------+\n|               5|\n+----------------+\n\nChecking postcode length of All here : None\nchecking all account no if we have one : [Row(ACCOUNT_NO='96106003'), Row(ACCOUNT_NO='90854884'), Row(ACCOUNT_NO='93182848'), Row(ACCOUNT_NO='84027448'), Row(ACCOUNT_NO='83835995'), Row(ACCOUNT_NO='95865635'), Row(ACCOUNT_NO='81422591'), Row(ACCOUNT_NO='94294664'), Row(ACCOUNT_NO='90094616'), Row(ACCOUNT_NO='90189079')]\ncount before de-dupe on Key & Serviceable 10489664\ncount AFTER de-dupe on Key & Serviceable 8582404\nAll_1 size: 7675870\n+--------------------+\n|     Serviceable_New|\n+--------------------+\n|             TM|FTTH|\n|                    |\n|             TM|VDSL|\n|            ,TM|FTTH|\n|    TM|VDSL,,TM|FTTH|\n|            TM|VDSL,|\n|           ALLO|FTTH|\n|   ALLO|FTTH,TM|FTTH|\n|          ALLO|FTTH,|\n|     TM|VDSL,TM|FTTH|\n|  ALLO|FTTH,,TM|FTTH|\n|   CTS|FTTH,,TM|FTTH|\n|            CTS|FTTH|\n|          Maxis|VDSL|\n|          Maxis|FTTH|\n|TM|LOT 411 JALAN ...|\n|           CTS|FTTH,|\n|  Maxis|FTTH,TM|FTTH|\n| Maxis|FTTH,,TM|FTTH|\n|    CTS|FTTH,TM|FTTH|\n+--------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Extra codes that Fakhrul commented out (before Nov 22) from the PIPELINE 3 - FINAL Job (open this cell to view)\n<!-- ##Astro_Standard['F']= 'A_MB'\n##TM_Standard['F']='TM_MB'\n##Maxis_Standard['F']='MAXIS_MB'\n##Allo_Standard['F']='ALLO_MB'\n##CTS_Standard['F']='CTS_MB'\n\n\n## ------------------------- Note - fakhrul 15/6/22 - these codes below does not seem efficient at all. if it dies i will drop the columns from the get go -------------------------\n#Edit - ok i removed it from the usecols in the beginning - 15/6/22\n#A_MB = All[(All['F']=='A_MB')]\n\n#wr.s3.to_csv(df = A_MB, path = 's3://astro-groupdata-prod-target/address_standardization/a_mb.csv')\n\n#A_MB = A_MB.drop(['F'], axis = 1)\n#print('a_mb is')\n#print(A_MB.shape)\n#A_MB.reset_index(inplace=True, drop=True)\n#A_MB = A_MB.drop_duplicates(subset=['Key'], keep='first')\n#print(A_MB.shape)\n#\n#\n#TM_MB = All[(All['F']=='TM_MB')]\n\n#wr.s3.to_csv(df = TM_MB, path = 's3://astro-groupdata-prod-target/address_standardization/tm_mb.csv')\n\n#TM_MB = TM_MB.drop(['F'], axis = 1)\n#print('tm_mb is')\n#print(TM_MB.shape)\n#TM_MB.reset_index(inplace=True, drop=True)\n#TM_MB = TM_MB.drop_duplicates(subset=['Key'], keep='first')\n#print(TM_MB.shape)\n#\n#MAXIS_MB = All[(All['F']=='MAXIS_MB')]\n#MAXIS_MB = MAXIS_MB.drop(['F'], axis = 1)\n#print('maxis_mb is')\n#print(MAXIS_MB.shape)\n#MAXIS_MB.reset_index(inplace=True, drop=True)\n#MAXIS_MB = MAXIS_MB.drop_duplicates(subset=['Key'], keep='first')\n#print(MAXIS_MB.shape)\n#\n#ALLO_MB = All[(All['F']=='ALLO_MB')]\n#ALLO_MB =ALLO_MB.drop(['F'], axis = 1)\n#print('allo_mb is')\n#print(ALLO_MB.shape)\n#ALLO_MB.reset_index(inplace=True, drop=True)\n#ALLO_MB = ALLO_MB.drop_duplicates(subset=['Key'], keep='first')\n#print(ALLO_MB.shape)\n## \n#CTS_MB = All[(All['F']=='CTS_MB')]\n#CTS_MB = CTS_MB.drop(['F'], axis = 1)\n#print('cts_mb is')\n#print(CTS_MB.shape)\n#CTS_MB.reset_index(inplace=True, drop=True)\n#CTS_MB = CTS_MB.drop_duplicates(subset=['Key'], keep='first')\n#print(CTS_MB.shape)\n#\n#\n#Frame = [TM_MB, MAXIS_MB, A_MB, ALLO_MB, CTS_MB]\n#\n#All = pd.concat(Frame) -->",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "## Pipeline 4\n- Put multiple ISP in one line\n- Original Zepp Qubole Notebook: Converting Step 4.0_UAMS Generation to PySpark_Part3 (https://us.qub)ole.com/notebooks#recent?id=141821&type=my-notebooks&view=home",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## =========================================================== THIS IS THE START OF Pipeline 4 - BACKUP 1 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_4_backup\n# according to Fakhrul, the order for pipeline 4 is final_4_backup -> final_4\n \n## read in the files\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All-final.orc\".format(date_key))\nAll_1 = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_1-final.orc\".format(date_key))\n# All = spark.read.csv(UAMS_PySpark_save_path+\"all_temp_final_3_{}.csv\".format(date_key), header=True)\n# All_1 = spark.read.csv(UAMS_PySpark_save_path+\"all_1_temp_final_3_{}.csv\".format(date_key), header=True)\n\n#revision - 20/6/22 - fakhrul - postcode might have problems reading as it would read 08000 as 8000 instead here\nAll = All.withColumn(\"POSTCODE\", f.regexp_replace(f.col('POSTCODE').cast('string'), '\\.0', '') )\nAll = All.withColumn(\"POSTCODE\", f.substring(f.col('POSTCODE'), 1, 5) )\nAll = All.withColumn('POSTCODE', f.lpad(f.col('POSTCODE').cast('string'), 5, '0') )\n## check that POSTCODE is only len = 5\nprint(All.select(f.length(f.col('POSTCODE'))).distinct().show())\n\n# ### Put multiple ISP in one line\n\n#All = All.drop_duplicates(subset=['Key','Serviceable'], keep='first')\n\n#this new line below is an added code to prevent float error\n#All['Key'] = All['Key'].astype('object')\n#All['Serviceable'] = All['Serviceable'].astype('str')\n\n#All_1 = All.groupby(['Key'])['Serviceable'].agg(','.join).reset_index()\n#All_1 = All_1.rename({'Serviceable':'Serviceable_New' }, axis=1)\n\n\nAll_1 = All_1.withColumn(\"Serviceable_New\", f.regexp_replace(f.col(\"Serviceable_New\"), ',,' , ',') )\n\n## 10/11/2022: Amzar added to avoid duplicate 'index' column when joining later on\nAll = All.withColumnRenamed('index', 'index_nongrouped')\n\n## save intermediate files before the big JOIN below:\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate1.orc\".format(date_key), mode='overwrite', compression='snappy')\nAll_1.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_1_pipeline4-intermediate1.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ----------------------------------------------------------------------------------------------------------------------------------------\n\n## delete files to clear up memory\ndel All\ndel All_1\n\n## read intermediate file back in\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate1.orc\".format(date_key))\nAll_1 = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_1_pipeline4-intermediate1.orc\".format(date_key))\n\n## join All_1 to All on \"Key\"\nAll_Final_Merg =  All_1.join(All, on ='Key', how = 'left')\n# print('chceking all final merge :', All_Final_Merg.info())\nprint('checking all final merge :', All_Final_Merg.columns)\nprint('checking all final merge count:', All_Final_Merg.select('ACCOUNT_NO').count()) # 8586359\nprint('checking all final merge account no:', All_Final_Merg.select('ACCOUNT_NO').head(10))\n\n## 11/11/2022 Amzar: save intermediate file\nAll_Final_Merg.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate2.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ----------------------------------------------------------------------------------------------------------------------------------------\n\n## delete files to clear up memory\ndel All\ndel All_1\ndel All_Final_Merg\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------------+\n|length(POSTCODE)|\n+----------------+\n|               5|\n+----------------+\n\nNone\nchecking all final merge : ['Key', 'Serviceable_New', 'index', 'OBJID', 'ACCOUNT_NO', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'Standard_Building_Name', 'F', 'ServiceType', 'Servicable', 'Serviceable', 'index_nongrouped', 'Address_Type', 'Post_length', 'House_No']\nchecking all final merge count: 8582404\nchecking all final merge account no: [Row(ACCOUNT_NO='92244949'), Row(ACCOUNT_NO='91658904'), Row(ACCOUNT_NO='91368965'), Row(ACCOUNT_NO='91321254'), Row(ACCOUNT_NO='91066518'), Row(ACCOUNT_NO='91356100'), Row(ACCOUNT_NO='98478545'), Row(ACCOUNT_NO='97768024'), Row(ACCOUNT_NO='96075820'), Row(ACCOUNT_NO='95707097')]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## read intermediate file back in\nAll_Final_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate2.orc\".format(date_key))\n\n## de-dupe on Key & Serviceable, & keep first based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Key','Serviceable']).orderBy(f.col(\"index\").asc())\nAll_Final_Merg = All_Final_Merg.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\n# print('All_Final_Merg count AFTER de-dupe on Key & Serviceable:', All_Final_Merg.select('Key').count()) # 8586359\n\nAll = All_Final_Merg\n\n## de-dupe on Key, & keep first based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Key']).orderBy(f.col(\"index\").asc())\nAll = All.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\n# print('All count AFTER de-dupe on Key:', All.select('Key').count()) # 7705263\n\n## 12/11/2022 Amzar: save intermediate file\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate3.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ----------------------------------------------------------------------------------------------------------------------------------------\n\n# ## delete files to clear up memory\ndel All\n\n## read intermediate file back in\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate3.orc\".format(date_key)) \n## moved the reading of UAMS_P1P2_Merg from start of file to down here to save on memory\nUAMS_P1P2_Merg = spark.read.orc(UAMS_PySpark_save_path+\"phase_1/{}/UAMS_P1P2_Merg-final.orc\".format(date_key))\n# UAMS_P1P2_Merg = spark.read.csv(UAMS_PySpark_save_path+\"uams_p1p2_merg_temp-20221031.csv.gz\", header=True)\n\n## work on UAMS_P1P2_Merg now, i.e remove any \"Key\" existing in UAMS_P1P2_Merg from All (use left anti-join)\n# print(UAMS_P1P2_Merg.select('Key').count()) # 3328852\n# print('count of unique p1p2 Keys from UAMS_P1P2_Merg:', UAMS_P1P2_Merg.select(f.countDistinct('Key')).show()) # 2427513\n\nP1_P2_removed = All.join(UAMS_P1P2_Merg, All.Key == UAMS_P1P2_Merg.Key, 'leftanti')\nprint('P1_P2_removed count (not in p1p2_list):', P1_P2_removed.select('Key').count()) # 5621584\n\n## OLD --> work on UAMS_P1P2_Merg now # 12/11/2022 --> Amzar commented below codes out as it was inefficient (filter based on list). Created new 'join' codes to speed up this filtering process\n# P1_P2_removed = pd.merge(All,UAMS_P1P2_Merg, on = ['Key'], indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n# p1p2_list = UAMS_P1P2_Merg.agg(f.collect_list(f.col(\"Key\"))).collect()[0][0] \n# P1_P2_removed = All.filter(~f.col('Key').isin(p1p2_list))\n\n## filter out nulls\nP1_P2_removed = P1_P2_removed.filter(f.col('POSTCODE').isNotNull())\nP1_P2_removed = P1_P2_removed.filter(f.col('STD_CITY').isNotNull())\nP1_P2_removed = P1_P2_removed.filter(f.col('Street_Type_1').isNotNull())\nP1_P2_removed = P1_P2_removed.filter(f.col('Street_1_New').isNotNull())\nprint('P1_P2_removed count after removing nulls:', P1_P2_removed.select('Key').count()) # 5621584\n\n## get the current MAX value of Address_ID then reset_index & make Address_ID equal to the new index + j\n# i = Naresh_P1_P2_Base_Final_Merg['Address_ID'].max()\ni = UAMS_P1P2_Merg.select(f.max(f.col('Address_ID').cast('float'))).first()[0]\nprint(i) # 2427515.0\nj = int(i)+2\n# create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nP1_P2_removed = P1_P2_removed.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\nP1_P2_removed = P1_P2_removed.withColumn('Address_ID', f.col('index') + j).drop(*['index'])\nprint(P1_P2_removed.columns)\nprint(UAMS_P1P2_Merg.columns)\n\n## drop some columns in P1_P2_removed (Amzar added on 12/11/2022) & UAMS_P1P2_Merg, then rename some other columns\nP1_P2_removed = P1_P2_removed.drop(*['Serviceable', 'Servicable', 'ServiceType']).withColumnRenamed('Serviceable_New', 'Serviceable')\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.drop(*['Serviceable', 'Servicable', 'ServiceType']).withColumnRenamed('Serviceable_New', 'Serviceable')\n# print('UAMS_P1P2_Merg count:', UAMS_P1P2_Merg.count(), 'P1_P2_removed count:', P1_P2_removed.count()) # UAMS_P1P2_Merg count: 3328852 P1_P2_removed count: 5621584\n\n## save intermediate files before the UNION below:\nP1_P2_removed.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/P1P2_removed_pipeline4.orc\".format(date_key), mode='overwrite', compression='snappy')\nUAMS_P1P2_Merg.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/UAMS_P1P2_Merg_pipeline4.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ----------------------------------------------------------------------------------------------------------------------------------------\n\n## delete files to clear up memory\ndel P1_P2_removed\ndel UAMS_P1P2_Merg\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "P1_P2_removed count (not in p1p2_list): 5590572\nP1_P2_removed count after removing nulls: 5590572\n2427515.0\n['Key', 'Serviceable_New', 'OBJID', 'ACCOUNT_NO', 'AREA', 'STD_CITY', 'STATE', 'POSTCODE', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'Standard_Building_Name', 'F', 'ServiceType', 'Servicable', 'Serviceable', 'index_nongrouped', 'Address_Type', 'Post_length', 'House_No', 'Address_ID']\n['Account_No', 'Serviceable_New', '_c0', 'OBJID', 'House_No', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'POSTCODE', 'STATE', 'Standard_Building_Name', 'ServiceType', 'Servicable', 'HNUM_STRT_TM', 'Address_Type', 'P_Flag', 'index', 'Serviceable', 'Key', 'Address_ID']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## read ORC files back in\nP1_P2_removed = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/P1P2_removed_pipeline4.orc\".format(date_key))\nUAMS_P1P2_Merg= spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/UAMS_P1P2_Merg_pipeline4.orc\".format(date_key))\n\n## rename columns, create blank columns & rearrange column order for smooth UNION-ing\nP1_P2_removed = P1_P2_removed.withColumnRenamed('ACCOUNT_NO', 'Account_No').withColumnRenamed('index_nongrouped', 'index')\nP1_P2_removed = P1_P2_removed.withColumn('HNUM_STRT_TM', f.lit('')).withColumn('P_Flag', f.lit(None))\nP1_P2_removed = P1_P2_removed.select(['Account_No', 'Serviceable', 'OBJID', 'House_No', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'POSTCODE', 'STATE', 'Standard_Building_Name', 'HNUM_STRT_TM', 'Address_Type', 'P_Flag', 'index', 'Key', 'Address_ID'])\nUAMS_P1P2_Merg = UAMS_P1P2_Merg.select(['Account_No', 'Serviceable', 'OBJID', 'House_No', 'Combined_Building', 'Street_Type_1', 'Street_1_New', 'Street_Type_2', 'Street_2_New', 'AREA', 'STD_CITY', 'POSTCODE', 'STATE', 'Standard_Building_Name', 'HNUM_STRT_TM', 'Address_Type', 'P_Flag', 'index', 'Key', 'Address_ID'])\n\n## Union the 2 DFs then reset index\nAll = UAMS_P1P2_Merg.union(P1_P2_removed)\nprint('All (after unioning UAMS_P1P2_Merge and P1_P2_removed) count', All.select(\"Key\").count()) # 8950436\nAll = All.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## Change column name to UAMS format name\nAll = All.withColumnRenamed('Combined_Building', 'Building_Name').withColumnRenamed('Street_Type_1', 'Street_Type').withColumnRenamed('Street_1_New', 'Street_Name').withColumnRenamed('AREA', 'Area').withColumnRenamed('STD_CITY', 'City').withColumnRenamed('STATE', 'State').withColumnRenamed('POSTCODE', 'Postcode')\nprint(\"All DF columns:\", All.columns)\n\n## save intermediate file\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate4.orc\".format(date_key), mode='overwrite', compression='snappy')\n# OLD save: All.coalesce(1).write.csv(UAMS_PySpark_save_path+\"all_temp_4_backup3_before_4_{}.csv.gz\".format(date_key), header=True, mode='overwrite', compression='gzip')\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Bytes):')\n# print(usage)\n\n## delete files to clear up memory\ndel P1_P2_removed\ndel UAMS_P1P2_Merg\ndel All",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "All (after unioning UAMS_P1P2_Merge and P1_P2_removed) count 8919424\nAll DF columns: ['Account_No', 'Serviceable', 'OBJID', 'House_No', 'Building_Name', 'Street_Type', 'Street_Name', 'Street_Type_2', 'Street_2_New', 'Area', 'City', 'Postcode', 'State', 'Standard_Building_Name', 'HNUM_STRT_TM', 'Address_Type', 'P_Flag', 'index', 'Key', 'Address_ID']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 4 - FINAL ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_4\n# according to Fakhrul, the order for pipeline 4 is final_4_backup -> final_4\n### Put multiple ISP in one line\n\n#revision - fakhrul-3/7/22 - added dtype here \n# schema = StructType().add(\"Account_No\",StringType(),True).add(\"OBJID\",StringType(),True)\n# All = spark.read.csv(UAMS_PySpark_save_path+\"all_temp_4_backup3_before_4_{}.csv.gz\".format(date_key), header=True, schema = schema)\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate4.orc\".format(date_key))\nAll = All.select('Key','Address_ID', 'Account_No', 'OBJID', 'House_No','Building_Name',\n          'Standard_Building_Name', 'Street_Type','Street_Name', 'Area', 'City',\n          'Postcode', 'State', 'Address_Type', 'Serviceable', 'P_Flag')\n\n#revision - fakhrul - 2/7/22 - commenting this out because we use usecols instead above\n#All= All[[ 'Key','Address_ID', 'Account_No', 'OBJID', 'House_No','Building_Name',\n          #'Standard_Building_Name', 'Street_Type','Street_Name', 'Area', 'City',\n          #'Postcode', 'State', 'Address_Type', 'Serviceable', 'P_Flag']]\n          \nprint('checking account no :', All.select('Account_No').distinct().show())\n\n## make columns upper case\nAll = All.withColumn('Building_Name', f.upper(f.col('Building_Name')) )\nAll = All.withColumn('Street_Type', f.upper(f.col('Street_Type')) )\nAll = All.withColumn('Street_Name', f.upper(f.col('Street_Name')) )\nAll = All.withColumn('Area', f.upper(f.col('Area')) )\nAll = All.withColumn('City', f.upper(f.col('City')) )\nAll = All.withColumn('State', f.upper(f.col('State')) )\n\n## ensure postcode is 5 digit only\nAll = All.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nAll = All.withColumn(\"Postcode\", f.substring(f.col('Postcode'), 1, 5) )\nAll = All.withColumn('Postcode', f.lpad(f.col('Postcode').cast('string'), 5, '0') )\n\n#All['Account_No'].str.len().unique()\n\n## fix weird serviceable values\nAll = All.withColumn('Serviceable', when(f.col('Serviceable') == '\\\\|', '').otherwise(f.col('Serviceable')) ) \n\nprint(All.select('Serviceable').distinct().show(30))\n\n## 11/11/2022 Amzar: save intermediate file\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate5.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ----------------------------------------------------------------------------------------------------------------------------------------\n\n## delete files to clear up memory\ndel All\n\n## read intermediate file back in\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate5.orc\".format(date_key))\n\n## Fix HouseNo that are converted to date. Pyspark code taken from P2 MDU Mapping Test Qubole Zepp notebooks\nAll = All.withColumn('HouseNo', f.upper(f.trim(f.col('House_No').cast('string'))))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JAN-\",\"01-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JAN\",\"-01\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"FEB-\",\"02-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-FEB\",'-02'))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"MAR-\",'03-'))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-MAR\",\"-03\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"APR-\",\"04-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-APR\",\"-04\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"MAY-\",\"05-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-MAY\",\"-05\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JUN-\",\"06-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JUN\",\"-06\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JUL-\",\"07-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JUL\",\"-07\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"AUG-\",'08-'))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-AUG\",\"-08\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"SEP-\",\"09-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-SEP\",\"-09\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"OCT-\",\"10-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-OCT\",\"-10\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"NOV-\",\"11-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-NOV\",\"-11\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"DEC-\",\"12-\"))\nAll = All.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-DEC\",\"-12\"))\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate6.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ----------------------------------------------------------------------------------------------------------------------------------------\n\ndel All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate6.orc\".format(date_key))\nprint('Total count of All before splitting to date_house & not_date_house:', All.select('HouseNo').count()) # 8950436\n\n## Fix HouseNo that are converted to date (DD/MM/YYYY format). Pyspark code taken from P2 MDU Mapping Test Qubole Zepp notebooks\n# Filter date HouseNo\ndate_house = All.filter(f.regexp_extract('HouseNo', '^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$', 0) != '' ) \n# Spliting the HouseNo\ndate_house = date_house.withColumn('block_date',  f.substring(date_house.HouseNo, 1, 2))\ndate_house = date_house.withColumn('floor',  f.substring(date_house.HouseNo, 4, 2))\ndate_house = date_house.withColumn('unit',  f.substring(date_house.HouseNo, 9, 2))\n# Combine the split HouseNo with dashes: '-'\ndate_house = date_house.withColumn('HOUSE_NO_ASTRO', f.concat_ws('-', date_house.block_date, date_house.floor, date_house.unit))\n# Remove additional column created to combine HouseNo\ndate_house = date_house.drop(*['block_date','floor','unit'])\nprint('date_house count:', date_house.select('HouseNo').count()) # 6\n    \n# Filter not date HouseNo\nnot_date_house = All.filter( f.regexp_extract('HouseNo', '^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$', 0) == '' )\nnot_date_house = not_date_house.withColumn('HOUSE_NO_ASTRO', f.col('HouseNo').cast('string') )\n# print(not_date_house.select('ACCOUNT_NO').count(), not_date_house.select(f.countDistinct('ACCOUNT_NO')).show()) #  rows,  unique acc_no\nprint('not_date_house count:', not_date_house.select('HouseNo').count()) # 8950430\n\n# Append the 2 dfs (date_house, not_date_house) --> originally this was in 'final_3' but I've moved it here to consolidate all the parts of this HouseNo date cleaning step in 1 phase\nAll = date_house.union(not_date_house)\nprint('Total count of All after re-appending date_house & not_date_house:', All.select('HouseNo').count()) # 8950436\n# create a sequential index as Zohreh did a pandas reset_index at this step again. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nAll = All.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes) before concat frame below :')\n# print(usage)\n\n## save intermediate table\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate7.orc\".format(date_key), mode='overwrite', compression='snappy')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+\n|Account_No|\n+----------+\n|  80000127|\n|  80000165|\n|  80000231|\n|  80000322|\n|  80000461|\n|  80000750|\n|  80000882|\n|  80000904|\n|  80000931|\n|  80001358|\n|  80001717|\n|  80001758|\n|  80001771|\n|  80001901|\n|  80001922|\n|  80001973|\n|  80002376|\n|  80002403|\n|  80002529|\n|  80002794|\n+----------+\nonly showing top 20 rows\n\nchecking account no : None\n+--------------------+\n|         Serviceable|\n+--------------------+\n|TM|VDSL,maxis|FTT...|\n|  Maxis|FTTH,TM|FTTH|\n|          Maxis|FTTH|\n|          maxis|VDSL|\n|          ALLO|FTTH,|\n|TM|VDSL,ALLO|FTTH...|\n|  TM|VDSL,maxis|FTTH|\n|     TM|VDSL,TM|FTTH|\n|  maxis|FTTH,TM|FTTH|\n|          maxis|FTTH|\n|  TM|VDSL,maxis|VDSL|\n|                    |\n|  maxis|VDSL,TM|FTTH|\n|           CTS|FTTH,|\n|TM|1-G-8 JALAN PU...|\n|TM|VDSL,maxis|FTT...|\n|            ,TM|FTTH|\n|    CTS|FTTH,TM|FTTH|\n|          Maxis|VDSL|\n|            CTS|FTTH|\n|            TM|VDSL,|\n|TM|LOT 411 JALAN ...|\n|maxis|FTTH,ALLO|FTTH|\n|           ALLO|FTTH|\n|   TM|VDSL,ALLO|FTTH|\n|    TM|VDSL,CTS|FTTH|\n|             TM|VDSL|\n|   ALLO|FTTH,TM|FTTH|\n|         Maxis|FTTH,|\n|             TM|FTTH|\n+--------------------+\nonly showing top 30 rows\n\nNone\nTotal count of All before splitting to date_house & not_date_house: 8919424\ndate_house count: 6\nnot_date_house count: 8919418\nTotal count of All after re-appending date_house & not_date_house: 8919424\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# del All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate7.orc\".format(date_key))\n\n## Fix Street_Name that got converted to date format then pad with spaces\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"JAN-\",\"1/\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"-JAN\",\"/1\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), 'FEB-','2/'))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), '-FEB','/2'))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"MAR-\",'3/'))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"-MAR\",\"/3\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"APR-\",\"4/\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"-APR\",\"/4\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"MAY-\",\"5/\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"-MAY\",\"/5\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"JUN-\",\"6/\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"-JUN\",\"/6\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"JUL-\",\"7/\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"-JUL\",\"/7\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"AUG-\",'8/'))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"-AUG\",\"/8\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"SEP-\",\"9/\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"-SEP\",\"/9\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"OCT-\",\"10/\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"-OCT\",\"/10\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"NOV-\",\"11/\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"-NOV\",\"/11\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"DEC-\",\"12/\"))\nAll = All.withColumn(\"Street_Name\", f.regexp_replace(f.col('Street_Name'), \"-DEC\",\"/12\"))\n\nAll = All.withColumn('Street_Name', f.lpad(f.col('Street_Name').cast('string'), 10, ' ') )\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate8.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate8.orc\".format(date_key)) ## read in ORC version\n\nAll = All.withColumn('House_No', f.lpad(f.col('HOUSE_NO_ASTRO').cast('string'), 10, ' ') )\nFinal = All.drop(*['HOUSE_NO_ASTRO', 'HouseNo'])\n\n## House_No\nFinal = Final.withColumn(\"HouseNo\", f.regexp_replace(\"House_No\", \"#|,|'\",'')) ## this seems to cover multiple cases & runs faster than having multiple lines for each symbol to regexp_replace\n\n## Building_Name\n# _list = ['#', ',', '/', '-', '!', 'No Name', '\\.', '\\*', '=', ':','\\)', '\\(', '`', '_', '\\^'] # copied from Converting to PySpark Part2\n_list = ['#', ',', '/', '-', '!', 'No Name'] \nFinal = Final.withColumn(\"Building_Name\", f.regexp_replace(\"Building_Name\", '|'.join(_list), '')) ## this seems to cover multiple cases & runs faster than having multiple lines for each symbol to regexp_replace\n\n# Final[\"Building_Name\"]= np.where(Final[\"Building_Name\"]=='0', '',Final[\"Building_Name\"] )\n\n## assigning SDU, MDU to Address_Type\nFinal = Final.withColumn('Address_Type', when( ((f.col('Building_Name').isNull()) | (f.col('Building_Name') == '')), 'SDU').otherwise('MDU') )\n\n## Street_Type\n_list = ['#', ',', '/', '-', 'No Name']\nFinal = Final.withColumn(\"Street_Type\", f.regexp_replace(\"Street_Type\", '|'.join(_list), ''))\n# Final = Final.withColumn(\"Street_Type\", f.regexp_replace(\"Street_Type\", 'JLN','JALAN')) # copied from Converting to PySpark Part2\n# Final = Final.withColumn(\"Street_Type\", f.regexp_replace(\"Street_Type\", 'LRG','LORONG')) # copied from Converting to PySpark Part2\n\n## Street_Name\n_list = ['#', ',', 'No Name']\nFinal = Final.withColumn(\"Street_Name\", f.regexp_replace(\"Street_Name\", '|'.join(_list), ''))\n\n## Area\n_list = ['#',',', '/','-', 'No Name']\nFinal = Final.withColumn(\"Area\", f.regexp_replace(\"Area\", '|'.join(_list), ''))\n\n## City\n# _list = ['#',',', '/','-', '=', ':','\\)', '\\(', 'No Name','\\[','\\]'] # copied from Converting to PySpark Part2\n_list = ['#',',', '/','-', 'No Name']\nFinal = Final.withColumn(\"City\", f.regexp_replace(\"City\", '|'.join(_list), ''))\n\n## State\n_list = ['#',',', '/','-', 'No Name']\nFinal = Final.withColumn(\"State\", f.regexp_replace(\"State\", '|'.join(_list), ''))\n\n## Postcode\n_list = ['#',',', '/','-', 'No Name']\nFinal = Final.withColumn(\"Postcode\", f.regexp_replace(\"Postcode\", '|'.join(_list), ''))\n\n## ensuring only 5 digit postcode\nAll = All.withColumn('Postcode', f.lpad(f.col('Postcode').cast('string'), 5, '0') )\nAll = All.withColumn(\"Postcode\", f.substring(f.col('Postcode'), 1, 5) )\n\n# Final['Postcode']  = Final['Postcode'] .replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n# from string import printable \n# st = set(printable) \n# Final['Postcode'] = Final['Postcode'].apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x])) ## not sure how to do this part yet\n\n## this below part seems redundant...\nFinal = Final.withColumn('Address_Type', when(f.col('Building_Name').isNull(), 'SDU').when(f.col('Building_Name') == '', 'SDU').otherwise('MDU'))\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nAll.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate9.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel All # -- if required\nAll = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate9.orc\".format(date_key)) ## read in ORC version\n\n## remove nulls\nprint('count before removing nulls:', Final.select('Postcode').count()) # 8950436\nFinal = Final.filter(f.col('Postcode').isNotNull())\nprint('count after removing null Postcode:',Final.select('Postcode').count()) # 8950436\nFinal = Final.filter(f.col('City').isNotNull())\nprint('count after removing null City:',Final.select('Postcode').count()) # 8950436\nFinal = Final.filter(f.col('Street_Type').isNotNull())\nprint('count after removing null Street_Type:',Final.select('Postcode').count()) # 8950436\nFinal = Final.filter(f.col('Street_Name').isNotNull())\nprint('count after removing null Street_Name:',Final.select('Postcode').count()) # 8950436\n\n## check OBJID which is not length 8, and make blank any which aren't 8\nprint(Final.select(f.length(f.col('OBJID'))).distinct().show()) # variety of lengths\nFinal = Final.withColumn('OBJID', when(f.length(f.col('OBJID')) == 8, f.col('OBJID')).otherwise(''))\nFinal = Final.withColumn(\"OBJID\", f.substring(f.col('OBJID'), 1, 8) )\nprint(Final.select(f.length(f.col('OBJID'))).distinct().show()) # 8 or 0\n\n## check Account_No which is not length 8, and make blank any which aren't 8\nFinal = Final.withColumn('Account_No', when(f.length(f.col('Account_No')) == 8, f.col('Account_No')).otherwise(''))\nprint(Final.select(f.length(f.col('Account_No'))).distinct().show()) # 8 or 0\n\n## clean the Serviceable column\nFinal = Final.withColumn(\"Serviceable\", f.regexp_replace(f.col('Serviceable').cast('string'), 'MAXIS', 'Maxis') )\nFinal = Final.withColumn(\"Serviceable\", f.regexp_replace(f.col('Serviceable').cast('string'), ',Maxis\\\\|Z', '') )\nFinal = Final.withColumn(\"Serviceable\", f.regexp_replace(f.col('Serviceable').cast('string'), 'Maxis\\\\|Z', '') )\n# Final = Final.withColumn(\"Serviceable\", f.regexp_replace(f.col('Serviceable').cast('string'), 'Maxis|Z', '') )\n\nFinal = Final.withColumn(\"Serviceable\", f.regexp_replace(f.col('Serviceable').cast('string'), ',TM\\\\|Z', '') )\nFinal = Final.withColumn(\"Serviceable\", f.regexp_replace(f.col('Serviceable').cast('string'), 'TM\\\\|Z', '') )\n# Final = Final.withColumn(\"Serviceable\", f.regexp_replace(f.col('Serviceable').cast('string'), 'TM|Z', '') )\n\n\n## save intermediate table --> have to break it up coz it seems this code causes pyspark to take forever (more than 3 hours) to finish running an Action cell\nFinal.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate10.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel Final # -- if required\nFinal = spark.read.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-intermediate10.orc\".format(date_key)) ## read in ORC version\n\n## further clean Serviceable column: #code below is added with na parameter to false\nFinal_1 = Final.filter(~f.col(\"Serviceable\").cast('string').contains(\"Maxis\\\\|Z\")) # .str.contains(\"Maxis\\\\|Z\", na = False)]\nFinal_1 = Final_1.filter(~f.col(\"Serviceable\").cast('string').contains(\"TM\\\\|Z\")) # .str.contains(\"TM\\\\|Z\", na = False)]\n\n## to replace all nan values with ''\nFinal_1= Final_1.fillna('')\n\n## ensure columns are string\nFinal_1 = Final_1.withColumn('Account_No', f.regexp_replace(f.upper(f.trim(f.col('Account_No').cast('string'))), 'NAN', '') )\nFinal_1 = Final_1.withColumn('OBJID', f.regexp_replace(f.upper(f.trim(f.col('OBJID').cast('string'))), 'NAN', '') )\n\n## filter out nulls\nprint('Before filtering out nulls', Final_1.select(\"Account_No\").count()) # 8214216\nFinal_1 = Final_1.filter((f.col('Postcode').isNotNull()) & (f.col('Postcode') != ''))\nFinal_1 = Final_1.filter((f.col('City').isNotNull()) & (f.col('City') != ''))\nFinal_1 = Final_1.filter((f.col('Street_Type').isNotNull()) & (f.col('Street_Type') != ''))\nFinal_1 = Final_1.filter((f.col('Street_Name').isNotNull()) & (f.col('Street_Name') != ''))\nFinal_1 = Final_1.filter((f.col('State').isNotNull()) & (f.col('State') != ''))\n\nprint('After filtering out nulls', Final_1.select(\"Account_No\").count()) # 8214216\nprint('Columns of Final_1', Final_1.columns)\nprint('Unique Account_No after filtering out nulls', Final_1.select(f.countDistinct(\"Account_No\")).show())\nprint('final_1 account no check :', Final_1.select('Account_No').head(100))\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):') \n# print(usage)\n\nprint('Checking unique postcode length', Final_1.select(f.length(f.col('Postcode'))).distinct().show()) # only 5\n\n## Save to ORC & CSV\nFinal_1.write.orc(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-final.orc\".format(date_key), mode='overwrite', compression='snappy')\nFinal_1.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-final.csv.gz\".format(date_key), header=True, mode='overwrite', compression='gzip')\n# wr.s3.to_csv(df = Final_1, path = final_1_temp_path + 'final_1_temp.csv')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "count before removing nulls: 8919424\ncount after removing null Postcode: 8919424\ncount after removing null City: 8919424\ncount after removing null Street_Type: 8919424\ncount after removing null Street_Name: 8919424\n+-------------+\n|length(OBJID)|\n+-------------+\n|            1|\n|            6|\n|            4|\n|            7|\n|            0|\n|            8|\n|            5|\n+-------------+\n\nNone\n+-------------+\n|length(OBJID)|\n+-------------+\n|            0|\n|            8|\n+-------------+\n\nNone\n+------------------+\n|length(Account_No)|\n+------------------+\n|                 0|\n|                 8|\n+------------------+\n\nNone\nBefore filtering out nulls 8919424\nAfter filtering out nulls 8919424\nColumns of Final_1 ['Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'State', 'Address_Type', 'Serviceable', 'P_Flag', 'index', 'HouseNo']\n+--------------------------+\n|count(DISTINCT Account_No)|\n+--------------------------+\n|                   3502364|\n+--------------------------+\n\nUnique Account_No after filtering out nulls None\nfinal_1 account no check : [Row(Account_No='90863366'), Row(Account_No='96622431'), Row(Account_No='91425697'), Row(Account_No='95019035'), Row(Account_No='84796055'), Row(Account_No='94596753'), Row(Account_No='80000125'), Row(Account_No='80000964'), Row(Account_No='80001019'), Row(Account_No='80001107'), Row(Account_No='80001656'), Row(Account_No='80001725'), Row(Account_No='80001785'), Row(Account_No='80001810'), Row(Account_No='80001939'), Row(Account_No='80002075'), Row(Account_No='80002136'), Row(Account_No='80002618'), Row(Account_No='80002717'), Row(Account_No='80002798'), Row(Account_No='80002824'), Row(Account_No='80003101'), Row(Account_No='80003111'), Row(Account_No='80003158'), Row(Account_No='80003243'), Row(Account_No='80003342'), Row(Account_No='80003478'), Row(Account_No='80003574'), Row(Account_No='80003712'), Row(Account_No='80003744'), Row(Account_No='80004279'), Row(Account_No='80005051'), Row(Account_No='80005146'), Row(Account_No='80005369'), Row(Account_No='80005764'), Row(Account_No='80005959'), Row(Account_No='80006095'), Row(Account_No='80006122'), Row(Account_No='80006392'), Row(Account_No='80006456'), Row(Account_No='80006559'), Row(Account_No='80006649'), Row(Account_No='80006712'), Row(Account_No='80006768'), Row(Account_No='80007000'), Row(Account_No='80007567'), Row(Account_No='80007673'), Row(Account_No='80007748'), Row(Account_No='80007769'), Row(Account_No='80007862'), Row(Account_No='80008069'), Row(Account_No='80008168'), Row(Account_No='80008208'), Row(Account_No='80008315'), Row(Account_No='80008359'), Row(Account_No='80008516'), Row(Account_No='80008561'), Row(Account_No='80008656'), Row(Account_No='80008665'), Row(Account_No='80008674'), Row(Account_No='80008789'), Row(Account_No='80008988'), Row(Account_No='80008989'), Row(Account_No='80009228'), Row(Account_No='80009600'), Row(Account_No='80009639'), Row(Account_No='80009648'), Row(Account_No='80009824'), Row(Account_No='80009914'), Row(Account_No='80010063'), Row(Account_No='80010323'), Row(Account_No='80010358'), Row(Account_No='80010438'), Row(Account_No='80010691'), Row(Account_No='80010957'), Row(Account_No='80011209'), Row(Account_No='80011338'), Row(Account_No='80011373'), Row(Account_No='80011477'), Row(Account_No='80012105'), Row(Account_No='80012200'), Row(Account_No='80012346'), Row(Account_No='80012446'), Row(Account_No='80012514'), Row(Account_No='80013052'), Row(Account_No='80013074'), Row(Account_No='80013183'), Row(Account_No='80013216'), Row(Account_No='80013320'), Row(Account_No='80013394'), Row(Account_No='80013405'), Row(Account_No='80013567'), Row(Account_No='80013611'), Row(Account_No='80013632'), Row(Account_No='80013692'), Row(Account_No='80013705'), Row(Account_No='80013724'), Row(Account_No='80013772'), Row(Account_No='80014218'), Row(Account_No='80014221')]\n+----------------+\n|length(Postcode)|\n+----------------+\n|               5|\n+----------------+\n\nChecking unique postcode length None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}