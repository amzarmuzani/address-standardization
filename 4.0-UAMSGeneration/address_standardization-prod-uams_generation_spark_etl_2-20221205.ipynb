{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session                                                                                                  |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X                                                                            |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0)                                |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |   Changes the session type to Glue ETL.                                                                                                                   |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer                       |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# I set my own magics (config for the job)\n%number_of_workers 20\n\n## default imports by Glue Spark\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "### my import libraries\nfrom io import StringIO\nimport numpy as np\nimport pandas as pd\n# import awswrangler as wr\nimport glob\nimport re\n#import boto3\nregex_schema = \"/*.csv\"\nfrom string import printable\nst = set(printable)\nfrom datetime import datetime\n\nimport pyspark.sql.functions as f\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.functions import when\n\nfrom pyspark.sql.window import Window\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import StringType, IntegerType\n\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\n## Setting run date & save path\nimport datetime\na = datetime.datetime.now()\n# date_key = a.strftime('%Y%m%d')\ndate_key = '20221120' # temporary\nprint(date_key)\n\n# UAMS_PySpark_save_path = 's3://astro-datalake-prod-sandbox/amzar/BB/AddrStd/testing/20221028_UAMS_Step4_PySpark/' # old Qubole Zepp path\nUAMS_PySpark_save_path = 's3://astro-groupdata-prod-pipeline/address_standardization/spark_uams_generation/' ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "20221120\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## set the paths to be read in this cell\nfinal_1_temp_path = UAMS_PySpark_save_path+\"phase_2/{}/All_pipeline4-final.orc\".format(date_key)\npostcode_ref_path = 's3://astro-datalake-prod-ulm-pii/edw_address/postcode_lookup/year=2022/month=08/LKT_POSTCODE_DL_202208.csv.gz'\n# old_fullfeed_path = \"s3://astro-datalake-prod-sandbox/amzar/BB/other_adhoc/uploaded/address_checking/NEW_FULLFEED_UAMS_WITH_KEY_20220806.csv\"\nold_fullfeed_path = \"s3://astro-groupdata-prod-source/old_fullfeed_uams_with_key/NEW_FULLFEED_UAMS_WITH_KEY_after_zohreh_code.csv\" # This is the only FULLFEED UAMS that I could find which is close enough in size to the 20220806 one I used in Qubole. Also, this version was last modified on 16/8/22 which is close enough to 6/8/2022\nshamani_data_read_path = \"s3://amsdatabucket/Sales Team Data/dump_2022_09_15.csv\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# PHASE 3\n- Get the postcode lookup table from EDW and replace the EDW state for all the state in the data based on the postcode\n- Remove records with street name less than one record, city-state mismatch and, postcode less than 4 digits.\n\n----\n========================================= THIS IS THE START OF PHASE 3 ============================================\n- taken from Glue Job: address_standardization-prod-uams_generation_final_5 (Pipeline 5)...\n- originally converted to PySpark in Zepp Qubole notebook: https://us.qubole.com/notebooks#recent?id=141821&type=my-notebooks&view=home\nCombine all of P1P2 Mapped Addresses from each ISP\n\n## Pipeline 5",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## read in the table from end of Pipeline 4\n# Final_1 = wr.s3.read_csv(path = final_1_temp_path, dtype = {'Account_No':object, 'OBJID':object})\nFinal_1 = spark.read.orc(final_1_temp_path)\n\n#revision - fakhrul - 20/6/22 - otherwise postcode of 08000 will be read as 8000\nFinal_1 = Final_1.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nFinal_1 = Final_1.withColumn(\"Postcode\", f.substring(f.col('Postcode'), 1, 5) )\nFinal_1 = Final_1.withColumn('Postcode', f.lpad(f.col('Postcode').cast('string'), 5, '0') )\nprint('final 1 is: ', Final_1.select('Postcode').count()) # 8214216\n\n## Read postcode lookup table from EDW bucket\npostcode_lookup = spark.read.csv(postcode_ref_path, header=True) ## requires changing based on relevant month\n# s3://astro-datalake-prod-ulm-pii/edw_address/postcode_lookup/year=2022/month=09/LKT_POSTCODE_DL_202209.csv.gz\n## # path = '/Users/zmmzohreh/OneDrive - MEASAT Broadcast Network Systems Sdn. Bhd/Shared Documents/Input Data/'\n## postcode_lookup = pd.read_csv('POSTCODE_REF_TABLE.csv') \n# postcode_lookup = wr.s3.read_csv(path = postcode_ref_path, compression = 'gzip', encoding = 'unicode_escape')\nprint('postcode lookup is : ', postcode_lookup.select('state').count()) # 108772\nprint(postcode_lookup.columns)\nprint(postcode_lookup.head(2))\n\npostcode_lkt = postcode_lookup.select('state','postcode')\npostcode_lkt = postcode_lkt.withColumn(\"postcode\", f.regexp_replace(f.col('postcode').cast('string'), '\\.0', '') )\npostcode_lkt = postcode_lkt.withColumn(\"postcode\", f.lpad(f.substring(f.col('postcode'), 1, 5), 5, '0') )\npostcode_lkt = postcode_lkt.withColumnRenamed('postcode', 'lkt_postcode').withColumnRenamed('state', 'lkt_state')\npostcode_lkt = postcode_lkt.withColumn('lkt_state', f.upper(f.trim(f.col('lkt_state'))) )\npostcode_lkt = postcode_lkt.dropDuplicates() # order does not matter in this de-dupe\nprint('postcode lookup after cleaning & dedupe is : ', postcode_lkt.select('lkt_state').count()) # 3216\n\n#REVISION - fakhrul zohreh - 12/7/22 - because somehow we lost wilayah persekutuan\npostcode_lkt = postcode_lkt.withColumn('lkt_state', when(f.col('lkt_state') == 'WILAYAH PERSEKUTUAN', 'WILAYAH PERSEKUTUAN KUALA LUMPUR').otherwise(f.col('lkt_state')) )\n\n#this code below is added to prevent int + object merge error\nFinal_1 = Final_1.withColumn('Postcode', f.col('Postcode').cast('string'))\n\n## Get the state from joining to lookup table\nFinal_2 = Final_1.join(postcode_lkt, Final_1.Postcode == postcode_lkt.lkt_postcode, how='left')\nprint('final 2 after join to lookup table is: ', Final_2.select('Postcode').count()) # 8644370\nFinal_2 = Final_2.dropDuplicates() ## order shouldn't matter\nprint('final 2 after dedupe is: ', Final_2.select('Postcode').count()) # 8644370\nprint('final 2 account check :', Final_2.select('Account_No').head(10))\n\n## create a new column that checks if lkt_state is null. If yes, return UAMS STATE. Otherwise, return lkt_state\nFinal_2 = Final_2.withColumn('FINAL_STATE', when(f.col('lkt_state').isNull(), f.col('STATE')).otherwise(f.col('lkt_state')) )\n\n# display updated DataFrame\nprint(Final_2.head(5))\n\n## create list of valid state values & filter for those. Then reset index\nvalid_state = ['SELANGOR', 'WILAYAH PERSEKUTUAN KUALA LUMPUR', 'JOHOR', 'SARAWAK',\n       'NEGERI SEMBILAN', 'MELAKA', 'KEDAH', 'PULAU PINANG', 'SABAH',\n       'PAHANG', 'TERENGGANU', 'PERLIS', 'KELANTAN', 'PERAK', 'LABUAN',\n       'PUTRAJAYA']\nFinal_3 = Final_2.filter(f.col('FINAL_STATE').isin(valid_state))\nprint('Final_3 count (after filter for valid states)', Final_3.select('Postcode').count()) # 8644370\n# create a sequential index as Zohreh did a pandas reset_index at this step again. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nFinal_3 = Final_3.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\nprint('final 3 account check :', Final_3.select('Account_No').head(10))\n\n## save intermediate table --> have to break it up coz it seems like pyspark took a longgg time (more than 3 hours) trying to run an Action cell\nFinal_3.write.orc(UAMS_PySpark_save_path+\"phase_3/{}/intermediate1.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel Final_1\ndel Final_2\ndel Final_3\ndel postcode_lkt # -- if required ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "final 1 is:  8919424\npostcode lookup is :  108772\n['postcode_id', 'postcode', 'region', 'state_code', 'state', 'city', 'area', 'location', 'tat_hours', 'remarks', 'record_insert_datetime', 'record_update_datetime', 'record_last_update_user', 'etl_job_date', 'etl_job_run_id']\n[Row(postcode_id='536873419', postcode='06750', region='NORTHERN REGION 1', state_code='KED', state='KEDAH', city='PENDANG', area='KAMPUNG KAYU TIGA', location='RURAL', tat_hours='48', remarks=None, record_insert_datetime='2022.09.15 02:52:28', record_update_datetime=None, record_last_update_user='SYSTEM', etl_job_date='2022.09.15', etl_job_run_id='220915025228'), Row(postcode_id='536873421', postcode='06750', region='NORTHERN REGION 1', state_code='KED', state='KEDAH', city='PENDANG', area='KAMPUNG PADANG DURIAN', location='RURAL', tat_hours='48', remarks=None, record_insert_datetime='2022.09.15 02:52:28', record_update_datetime=None, record_last_update_user='SYSTEM', etl_job_date='2022.09.15', etl_job_run_id='220915025228')]\npostcode lookup after cleaning & dedupe is :  3216\nfinal 2 after join to lookup table is:  9384958\nfinal 2 after dedupe is:  9384958\nfinal 2 account check : [Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No='')]\n[Row(Key='3167,,JALAN,TOKUNGKU,,,SEREMBAN,,70100,NEGERISEMBILAN', Address_ID=4577300.0, Account_No='', OBJID='', House_No='      3167', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name=' TOK UNGKU', Area='', City='SEREMBAN', Postcode='70100', State='NEGERI SEMBILAN', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='', index=5463276, HouseNo='      3167', lkt_state='NEGERI SEMBILAN', lkt_postcode='70100', FINAL_STATE='NEGERI SEMBILAN'), Row(Key='3180,,JALAN,KUBONG,JALAN,KUBONG,LIMBANG,,98700,SARAWAK', Address_ID=4577376.0, Account_No='', OBJID='', House_No='      3180', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='    KUBONG', Area='', City='LIMBANG', Postcode='98700', State='SARAWAK', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='', index=5463352, HouseNo='      3180', lkt_state='SARAWAK', lkt_postcode='98700', FINAL_STATE='SARAWAK'), Row(Key='32,,JALAN,DESABESTA,,,SEPANG,DESASALAKPERMAI,43900,SELANGOR', Address_ID=4577573.0, Account_No='', OBJID='', House_No='        32', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='DESA BESTA', Area='DESA SALAK PERMAI', City='SEPANG', Postcode='43900', State='SELANGOR', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='', index=5463549, HouseNo='        32', lkt_state='SELANGOR', lkt_postcode='43900', FINAL_STATE='SELANGOR'), Row(Key='32,,JALAN,KEMUNING9,,,BANTING,TAMANKEMUNING,42700,SELANGOR', Address_ID=4577627.0, Account_No='', OBJID='', House_No='        32', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='KEMUNING 9', Area='TAMAN KEMUNING', City='BANTING', Postcode='42700', State='SELANGOR', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='', index=5463603, HouseNo='        32', lkt_state='SELANGOR', lkt_postcode='42700', FINAL_STATE='SELANGOR'), Row(Key='32,,JALAN,SERIDAMAK,JALAN,SERIDAMAK,KLANG,TAMANSRIANDALAS,41000,SELANGOR', Address_ID=4577760.0, Account_No='85663792', OBJID='', House_No='        32', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='SERI DAMAK', Area='TAMAN SRI ANDALAS', City='KLANG', Postcode='41000', State='SELANGOR', Address_Type='SDU', Serviceable='', P_Flag='', index=5463736, HouseNo='        32', lkt_state='SELANGOR', lkt_postcode='41000', FINAL_STATE='SELANGOR')]\nFinal_3 count (after filter for valid states) 9384958\nfinal 3 account check : [Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No='84593137')]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "Final_3 = spark.read.orc(UAMS_PySpark_save_path+\"phase_3/{}/intermediate1.orc\".format(date_key)) ## read in ORC version\n\n## Standardising state name for Wilayah\nFinal_3 = Final_3.withColumn('FINAL_STATE', when(f.col('FINAL_STATE') == 'WP KUALA LUMPUR', 'WILAYAH PERSEKUTUAN KUALA LUMPUR')\n                                            .when(f.col('FINAL_STATE') == 'WP LABUAN', 'LABUAN')\n                                            .when(f.col('FINAL_STATE') == 'WP PUTRAJAYA', 'PUTRAJAYA')\n                                            .otherwise(f.col('FINAL_STATE')) )\n\n\n# ### Removing potential street name with error (street name with 1 record)\n# create a sequential index as Zohreh did a pandas reset_index at this step again. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nFinal_3 = Final_3.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## get Number of addresses for each streetname to identify which streetnames to remove (coz there are only 1 case of that street so might be a mistake with the streetname)\nstreet_count = Final_3.groupBy('Street_Name').count().withColumnRenamed('count', 'street_count')\nstreet_count = street_count.withColumn('remove_street', when(f.col('street_count')==1, 1).otherwise(0) )\nstreet_count.head(5)\nstreet_count.groupBy('remove_street').count().show()\n\n# Left join to get remove street flag then remove those streets\nFinal_3 = Final_3.join(street_count, on='Street_Name', how='left')\nprint('Final_3 count before removing cases where Street_Name only appears once:', Final_3.select('Street_Name').count()) # 8644370\nFinal_3 = Final_3.filter(f.col('remove_street') != 1)\n# Remove street with the following address - very isolated case\nFinal_3 = Final_3.filter(f.col('Street_Name') != 'WARISAN PUTERI A17 VILA SURIA BANDAR WARISAN PUTERI 70400 SEREMBAN NEGERI SEMBILAN')\nprint('Final_3 count after removing cases where Street_Name only appears once:', Final_3.select('Street_Name').count()) # 8641491\n\n\n# ### Removing potential city-state mismatch (address with City-State less than 1 records)\n# create a sequential index as Zohreh did a pandas reset_index at this step again. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nFinal_3 = Final_3.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## Create key city+FINAL_STATE\nFinal_3 = Final_3.withColumn('city_state', f.concat_ws(' ,', f.col(\"City\").cast('string'), f.col(\"FINAL_STATE\").cast('string')) )\n## get Number of addresses for each city-state combo to identify which city-state combo to remove (coz there are too few cases of that combo so might be an incorret combo)\ncity_state_count = Final_3.groupBy('city_state').count().withColumnRenamed('count', 'city_state_count')\ncity_state_count = city_state_count.withColumn('remove_city_state', when(f.col('city_state_count')<=5, 1).otherwise(0) ) # 12/11/2022: from observation, looks like 5 is enough to filter weird combos\ncity_state_count.head(5)\ncity_state_count.groupBy('remove_city_state').count().show()\n\n# Left join to get remove street flag then remove those streets\nFinal_4 = Final_3.join(city_state_count, on='city_state', how='left')\nprint('Final_4 count before removing cases where city-state appears less than 100 times:', Final_4.select('city_state').count()) # 8641491\nFinal_4 = Final_4.filter(f.col('remove_city_state') != 1)\nprint('Final_4 count after removing cases where city-state appears less than 100 times:', Final_4.select('city_state').count()) # 8640835\n\n## save intermediate table --> have to break it up coz it seems like pyspark took a longgg time (more than 3 hours) trying to run an Action cell\nFinal_4.write.orc(UAMS_PySpark_save_path+\"phase_3/{}/intermediate2.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n# ------------------------------------------------------------------------------------------------------------\n\ndel Final_4 # -- if required \n\n## read in the intermediate table\nFinal_4 = spark.read.orc(UAMS_PySpark_save_path+\"phase_3/{}/intermediate2.orc\".format(date_key)) ## read in ORC version\n\n# #### Remove records with postcode length less than 4\nFinal_4 = Final_4.withColumn('postcode_length', f.length(f.col('Postcode').cast('string')) )\nFinal_4 = Final_4.filter(f.col('postcode_length') > 3)\nprint('Final_4 count after removing records with postcode length less than 4:', Final_4.select('Postcode').count()) # 8640835\n\n# #### Getting Urban/Rural/Remote flag from lookup table\n\n## select & rename columns\nFinal_5 = Final_4.select('Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable',\n       'P_Flag', 'lkt_state', 'lkt_postcode', 'FINAL_STATE')\nprint('final 5 account check :', Final_5.select('Account_No').head(10))\nFinal_5 = Final_5.withColumnRenamed('FINAL_STATE', 'State')\n\n## clean columns\nFinal_5 = Final_5.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nFinal_5 = Final_5.withColumn(\"Postcode\", f.substring(f.col('Postcode'), 1, 5) )\nFinal_5 = Final_5.withColumn('Postcode', f.lpad(f.col('Postcode').cast('string'), 5, '0') )\n\nFinal_5 = Final_5.withColumn('State', f.trim(f.col('State')) )\nFinal_5 = Final_5.withColumn('City', f.trim(f.col('City')) )\nFinal_5 = Final_5.withColumn('Area', f.trim(f.col('Area')) )\n\nFinal_5 = Final_5.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nFinal_5 = Final_5.fillna('', subset=['Area'])\n\n## Adding ur_key for flagging urban/rural\nFinal_5 = Final_5.withColumn(\"ur_key\", f.concat_ws(\" ,\", f.col('Area'), f.col('Postcode'), f.col('City'), f.col('State')) )\nprint(Final_5.head(5))\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\n## save intermediate table --> have to break it up coz it seems like pyspark took a longgg time (more than 3 hours) trying to run an Action cell\nFinal_5.write.orc(UAMS_PySpark_save_path+\"phase_3/{}/intermediate3.orc\".format(date_key), mode='overwrite', compression='snappy')\nFinal_5.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_3/{}/intermediate3.csv.gz\".format(date_key), header=True, mode='overwrite', compression='gzip')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------+-----+\n|remove_street|count|\n+-------------+-----+\n|            1| 5715|\n|            0|66450|\n+-------------+-----+\n\nFinal_3 count before removing cases where Street_Name only appears once: 9384958\nFinal_3 count after removing cases where Street_Name only appears once: 9379243\n+-----------------+-----+\n|remove_city_state|count|\n+-----------------+-----+\n|                1|  571|\n|                0|  813|\n+-----------------+-----+\n\nFinal_4 count before removing cases where city-state appears less than 100 times: 9379243\nFinal_4 count after removing cases where city-state appears less than 100 times: 9378206\nFinal_4 count after removing records with postcode length less than 4: 9378206\nfinal 5 account check : [Row(Account_No='83022571'), Row(Account_No='91335951'), Row(Account_No='81575795'), Row(Account_No='89766478'), Row(Account_No='96826547'), Row(Account_No='94306413'), Row(Account_No='83353661'), Row(Account_No='95682165'), Row(Account_No=''), Row(Account_No='')]\n[Row(Key='10-02-02,,JALAN,18/56,,,AMPANG?,AU3,54200,WILAYAHPERSEKUTUANKUALALUMPUR', Address_ID=3276989.0, Account_No='83022571', OBJID='70333267', House_No='  10-02-02', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='     18/56', Area='AU3', City='AMPANG?', Postcode='54200', Address_Type='SDU', Serviceable='', P_Flag='', lkt_state='WILAYAH PERSEKUTUAN KUALA LUMPUR', lkt_postcode='54200', State='WILAYAH PERSEKUTUAN KUALA LUMPUR', ur_key='AU3 ,54200 ,AMPANG? ,WILAYAH PERSEKUTUAN KUALA LUMPUR'), Row(Key='09-03-04,,JALAN,18/56,,,AMPANG?,AU3,54200,WILAYAHPERSEKUTUANKUALALUMPUR', Address_ID=7185733.0, Account_No='91335951', OBJID='30096724', House_No='  09-03-04', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='     18/56', Area='AU3', City='AMPANG?', Postcode='54200', Address_Type='SDU', Serviceable='', P_Flag='', lkt_state='WILAYAH PERSEKUTUAN KUALA LUMPUR', lkt_postcode='54200', State='WILAYAH PERSEKUTUAN KUALA LUMPUR', ur_key='AU3 ,54200 ,AMPANG? ,WILAYAH PERSEKUTUAN KUALA LUMPUR'), Row(Key='37627,,JALAN,18/56,,,AMPANG?,AU3,54200,WILAYAHPERSEKUTUANKUALALUMPUR', Address_ID=6119827.0, Account_No='81575795', OBJID='', House_No='     37627', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='     18/56', Area='AU3', City='AMPANG?', Postcode='54200', Address_Type='SDU', Serviceable='', P_Flag='', lkt_state='WILAYAH PERSEKUTUAN KUALA LUMPUR', lkt_postcode='54200', State='WILAYAH PERSEKUTUAN KUALA LUMPUR', ur_key='AU3 ,54200 ,AMPANG? ,WILAYAH PERSEKUTUAN KUALA LUMPUR'), Row(Key='NO19-2-2,,JALAN,18/56,,,AMPANG?,AU3,54200,WILAYAHPERSEKUTUANKUALALUMPUR', Address_ID=6332810.0, Account_No='89766478', OBJID='16324697', House_No=' NO 19-2-2', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='     18/56', Area='AU3', City='AMPANG?', Postcode='54200', Address_Type='SDU', Serviceable='', P_Flag='', lkt_state='WILAYAH PERSEKUTUAN KUALA LUMPUR', lkt_postcode='54200', State='WILAYAH PERSEKUTUAN KUALA LUMPUR', ur_key='AU3 ,54200 ,AMPANG? ,WILAYAH PERSEKUTUAN KUALA LUMPUR'), Row(Key='B3-G-01,PANGSAPURIDAHLIACOURT,JALAN,PANDANIND,,,AMPANG?,PANDANINDAH,55100,SELANGOR', Address_ID=4773857.0, Account_No='96826547', OBJID='78216277', House_No='   B3-G-01', Building_Name='PANGSAPURI DAHLIA COURT', Standard_Building_Name='YES', Street_Type='JALAN', Street_Name='PANDAN IND', Area='PANDAN INDAH', City='AMPANG?', Postcode='55100', Address_Type='MDU', Serviceable='', P_Flag='', lkt_state='WILAYAH PERSEKUTUAN KUALA LUMPUR', lkt_postcode='55100', State='WILAYAH PERSEKUTUAN KUALA LUMPUR', ur_key='PANDAN INDAH ,55100 ,AMPANG? ,WILAYAH PERSEKUTUAN KUALA LUMPUR')]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Phase 4\nAdding Cust_Location to UAMS data\n- To use area + postcode + city + state as mapping key instead of postcode\n- To prepare unique list of area + postcode + city + state that do not have Urban/Rural or tagged as both Urban/Rural\n\n----\n========================================= THIS IS THE START OF PHASE 4 ============================================\n- taken from Glue Job: address_standardization-prod-uams_generation_final_6 (Pipeline 6)\n- originally converted to PySpark in Zepp Qubole notebook: https://us.qubole.com/notebooks#recent?id=141821&type=my-notebooks&view=home\n\n## Pipeline 6",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## POSTCODE REFERENCE TABLE from EDW\n# ref_table = pd.read_csv('POSTCODE_REF_TABLE.csv') ## Read from EDW bucket / ##ref_table = pd.read_csv('LKT_POSTCODE_DL_202202.csv') ## Read from EDW bucket / ## ref_table = wr.s3.read_csv(path = postcode_ref_path, compression = 'gzip', encoding = 'unicode_escape')\n# Final_5 = wr.s3.read_csv(path = final_5_temp_path, dtype = {'OBJID':object, 'Account_No':object})\nref_table = spark.read.csv(postcode_ref_path, header=True) ## requires changing based on relevant month\nFinal_5 = spark.read.orc(UAMS_PySpark_save_path+\"phase_3/{}/intermediate3.orc\".format(date_key))\n\n#revision - fakhrul - 20/6/22 - otherwise postcode of 08000 will be read as 8000\nFinal_5 = Final_5.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nFinal_5 = Final_5.withColumn(\"Postcode\", f.substring(f.col('Postcode'), 1, 5) )\nFinal_5 = Final_5.withColumn('Postcode', f.lpad(f.col('Postcode'), 5, '0') )\n\nprint('checking ref_table numbers:', ref_table.count()) # 108772\nprint('checking ref_table columns:', ref_table.columns)\nprint('checking final_5 columns:', Final_5.columns)\nprint(Final_5.select('Account_No').head(10))\n\n## clean the ref_table's postcode & more cleaning\nref_table = ref_table.withColumn(\"Postcode\", f.regexp_replace(f.col('postcode').cast('string'), '\\.0', '') )\nref_table = ref_table.withColumn(\"Postcode\", f.substring(f.col('Postcode'), 1, 5) )\nref_table = ref_table.withColumn('Postcode', f.lpad(f.col('Postcode'), 5, '0') )\nref_table = ref_table.fillna('', subset=['area'])\n\n## Replace WILAYAH PERSEKUTUAN to WILAYAH PERSEKUTUAN KUALA LUMPUR\nref_table = ref_table.withColumn('state', when(f.col('state') == 'WILAYAH PERSEKUTUAN', 'WILAYAH PERSEKUTUAN KUALA LUMPUR').otherwise(f.col('state')) )\n\nref_table = ref_table.withColumn('state', f.upper(f.trim(f.col('state'))) )\nref_table = ref_table.withColumn('city', f.upper(f.trim(f.col('city'))) )\nref_table = ref_table.withColumn('area', f.upper(f.trim(f.col('area'))) )\nprint(ref_table.select('state').distinct().show(20))\n\n## Adding ur_key for flagging urban/rural then select relevant columns for dedupe\nref_table = ref_table.withColumn(\"ur_key\", f.concat_ws(\" ,\", f.col('area').cast('string'), f.col('postcode').cast('string'), f.col('city').cast('string'), f.col('state').cast('string')) )\nref_table2 = ref_table.select('ur_key', 'location')\nprint('ref_table2 count before dedupe on ur_key & location:', ref_table2.select('ur_key').count()) # 108772\nref_table2 = ref_table2.dropDuplicates()\nprint('ref_table2 count after dedupe on ur_key & location:', ref_table2.select('ur_key').count()) # 108621\nprint(ref_table2.head(2))\nref_table3 = ref_table2.drop_duplicates() # this step seems unnecessary\nprint('ref_table3 count:', ref_table3.select('ur_key').count()) # 108621\n\n## Not sure what below code does, but doesn't look like it's used later on so comment out for now\n# ORI CODE --> STRT_P1_dup = ref_table3.groupby('ur_key').filter(lambda x : x['ur_key'].shape[0]>1)\n# ATTEMPT to convert to PySpark: STRT_P1_dup = ref_table3.groupBy('ur_key').count().filter(f.col('count') > 1)\n# print('STRT_P1_dup count:', STRT_P1_dup.select('ur_key').count())\n# z.show(STRT_P1_dup.head(100))\n\n## Flag AREA+POSTCODE+CITY+STATE that tagged as both urban and rural\nlocation_pivot = ref_table.groupBy('ur_key').agg(f.concat_ws('/', f.collect_set('location')).alias('Cust_Location'))\nlocation_pivot = location_pivot.withColumn('Cust_Location', when(f.col('Cust_Location')=='RURAL/URBAN', 'URBAN/RURAL').otherwise(f.col('Cust_Location')) )\n# z.show(location_pivot.head(100))\n# location_pivot.select('Cust_Location').distinct().show()\n\n## merge Final_5 to location_pivot\nFinal_6 = Final_5.join(location_pivot,on='ur_key',how='left')\nprint('Final_6 count:', Final_6.select('ur_key').count()) # 8640835\n\n\n# #### Split UAMS data into 2 groups:\n#     1. Addresses with only Urban, Rural, Remote -- good to go for UAMS ingestion\n#     2. Addresses without Urban/Rural flag & Address with both Urban and Rural -- need to share with IFS\n\n## Addresses with only Urban or Rural -- good to go for UAMS ingestion\nvalid_loc = Final_6.filter(f.col('Cust_Location').isNotNull())\nvalid_loc = valid_loc.withColumn('Location', f.col('Cust_Location')).filter(f.col('Location').isin('URBAN','RURAL','REMOTE'))\nprint('valid_loc count:', valid_loc.select('Location').count()) # 8145739\n\n## Addresses without Urban/Rural flag & Address with both Urban and Rural -- need to share with IFS\ninvalid_loc = Final_6.filter( (f.col('Cust_Location').isNull()) | (~f.col('Cust_Location').isin('URBAN','RURAL', 'REMOTE')) )\ninvalid_loc = invalid_loc.fillna('', subset=['Area'])\nprint('Count of invalid_loc', invalid_loc.select('ur_key').count()) # 495096\nprint('Count of Unique UR Key WITHOUT valid location', invalid_loc.select(f.countDistinct('ur_key')).show()) # 8257\nprint('-----------------------')\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "checking ref_table numbers: 108772\nchecking ref_table columns: ['postcode_id', 'postcode', 'region', 'state_code', 'state', 'city', 'area', 'location', 'tat_hours', 'remarks', 'record_insert_datetime', 'record_update_datetime', 'record_last_update_user', 'etl_job_date', 'etl_job_run_id']\nchecking final_5 columns: ['Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'lkt_state', 'lkt_postcode', 'State', 'ur_key']\n[Row(Account_No='83022571'), Row(Account_No='91335951'), Row(Account_No='81575795'), Row(Account_No='89766478'), Row(Account_No='96826547'), Row(Account_No='94306413'), Row(Account_No='83353661'), Row(Account_No='95682165'), Row(Account_No=''), Row(Account_No='')]\n+--------------------+\n|               state|\n+--------------------+\n|          TERENGGANU|\n|           PUTRAJAYA|\n|WILAYAH PERSEKUTU...|\n|               PERAK|\n|            KELANTAN|\n|               JOHOR|\n|     NEGERI SEMBILAN|\n|               KEDAH|\n|             SARAWAK|\n|               SABAH|\n|        PULAU PINANG|\n|            SELANGOR|\n|              MELAKA|\n|              LABUAN|\n|              PAHANG|\n|              PERLIS|\n+--------------------+\n\nNone\nref_table2 count before dedupe on ur_key & location: 108772\nref_table2 count after dedupe on ur_key & location: 108621\n[Row(ur_key='KAMPUNG DULANG KECHIL ,06900 ,YAN ,KEDAH', location='URBAN'), Row(ur_key='TINGKAT EMAS ,13000 ,BUTTERWORTH ,PULAU PINANG', location='URBAN')]\nref_table3 count: 108621\nFinal_6 count: 9378206\nvalid_loc count: 8842843\nCount of invalid_loc 535363\n+----------------------+\n|count(DISTINCT ur_key)|\n+----------------------+\n|                  9349|\n+----------------------+\n\nCount of Unique UR Key WITHOUT valid location None\n-----------------------\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### In early Nov, Fakhrul said this below code don't need to be run anymore\n# #Edit - fakhrul 15/6/22 same comment as below\n# invalid_loc_list = invalid_loc.select(['Area','Postcode','City','State']).drop_duplicates()\n# #Edit - fakhrul - 15/6/22 invalid loc we dont use it it seems. will change back if we do\n# invalid_loc_list.coalesce(1).write.csv(UAMS_PySpark_save_path+'invalid_loc_list_{}.csv'.format(date_key), header=True, mode='overwrite') ## save it somewhere to be shared to IFS\n\n# ### Update UAMS with updated lookup table from IFS starts here -- only do this if EDW lookup table not updated by IFS\n\n# # Updated reference table from IFS team for address missing urban/rural\n# ifs_updated = spark.read.csv(UAMS_PySpark_save_path+'uploaded/Updated_Invalid_UrbanRural_from_IFS.csv', header=True) ## Read from source bucket??\n\n# ## clean columns then create ur_key\n# ifs_updated = ifs_updated.withColumn(\"Area\", when(f.col('Area') == '14-Jan', '14JAN').otherwise(f.col('Area')) )\n# ifs_updated = ifs_updated.fillna('', subset=['Area'])\n\n# ifs_updated = ifs_updated.withColumn(\"Postcode\", f.regexp_replace(f.col('postcode').cast('string'), '\\.0', '') )\n# ifs_updated = ifs_updated.withColumn(\"Postcode\", f.substring(f.col('Postcode'), 1, 5) )\n# ifs_updated = ifs_updated.withColumn('Postcode', f.lpad(f.col('Postcode'), 5, '0') )\n\n# print('IFS_Updated count:', ifs_updated.select('Area').count())\n\n# ifs_updated = ifs_updated.withColumn(\"ur_key\", f.concat_ws(\" ,\", f.col('Area').cast('string'), f.col('Postcode').cast('string'), f.col('City').cast('string'), f.col('Area').cast('State')) )\n# print(ifs_updated.head(2))\n\n# ifs_updated = ifs_updated.select(['ur_key','Location']).withColumn('Location', f.upper(f.col('Location').cast('string')) )\n# print(ifs_updated.column)\n\n# updated_invalid = invalid_loc.join(ifs_updated, on='ur_key', how='left')\n# print('updated_invalid count:', updated_invalid.select('ur_key').count())\n# updated_invalid.head(2)\n\n# final_UR = invalid_loc.join(ifs_updated,on='ur_key',how='left')\n# print('final_UR count:', final_UR.select('ur_key').count())\n# final_UR.head(2)\n# final_UR.select('Location').distinct().show()\n\n## Appending all dataframe with fixed U/R tagging for UAMS\n# final_UR = pd.concat([valid_loc,updated_invalid]) ## this step is to be used if IFS has re-standardized\nfinal_UR = valid_loc\nprint('final_UR:', final_UR.count()) # 8145739\nprint('checking final_ur here :', final_UR.select('Account_No').head(10))\n\n# keep UAMS data with cust location URBAN/RURAL/REMOTE only\nfinal_UR_removed_null = final_UR.filter(f.col('Location').isin('URBAN', 'RURAL', 'REMOTE'))\n\n## clean postcode column (make into string, remove float \\.0, select only first 5 digits, lpad with '0')\n# Final['Postcode']  = Final['Postcode'] .replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\nfinal_UR_removed_null = final_UR_removed_null.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nfinal_UR_removed_null = final_UR_removed_null.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\n\n## Handling all nan and decimals \nfinal_UR_removed_null = final_UR_removed_null.fillna('')\nprint('checking final_ur_removed_null here :', final_UR_removed_null.filter(f.col('Account_No') != '').select('Account_No').head(10))\n\n## ensure these cols are string, remove float \\.0, & replace nulls with blanks\nfinal_UR_removed_null = final_UR_removed_null.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nfinal_UR_removed_null = final_UR_removed_null.withColumn(\"OBJID\", f.regexp_replace(f.col('OBJID').cast('string'), '\\.0', '') )\nfinal_UR_removed_null = final_UR_removed_null.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No'), 'nan|NAN|null', '') )\nfinal_UR_removed_null = final_UR_removed_null.withColumn(\"OBJID\", f.regexp_replace(f.col('OBJID'), 'nan|NAN|null', '') )\n\n## select & rename columns\nfinal_UR_removed_null = final_UR_removed_null.select(['Key','Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City',\n       'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State','Location'])\nfinal_UR_removed_null = final_UR_removed_null.withColumnRenamed('Location', 'UR_Flag')  \n\n# print(final_UR_removed_null.filter(f.col('Account_No').isNotNull()).count()) # 0\nprint('No of non-blank account_no in final_UR_removed_null', final_UR_removed_null.filter(f.col('Account_No') != '').count()) # 4063540 (less than half of the size\nprint('checking final_UR_removed_null account no: ', final_UR_removed_null.filter(f.col('Account_No') != '').select('Account_No').head(10)) # this should not be null right?\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\n## save intermediate table --> have to break it up coz it seems like pyspark takes a longgg time trying to run an Action cell with so many transformations\nfinal_UR_removed_null.write.orc(UAMS_PySpark_save_path+\"phase_4/{}/final_UR_removed_null.orc\".format(date_key), mode='overwrite', compression='snappy')\nfinal_UR_removed_null.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_4/{}/final_UR_removed_null.csv.gz\".format(date_key), header=True, mode='overwrite', compression='gzip')\n\n# final_UR_removed_null.to_csv('NEW_FULLFEED_UAMS_WITH_KEY_mar.csv',index=False) ## Save in pipeline bucket\n# wr.s3.to_csv(df = final_UR_removed_null, path = final_removed_null_save_path + 'final_UR_removed_null_temp.csv')\n# Final_3.to_csv('TEMP_NEW_FULLFEED_UAMS_WITH_KEY.csv',index=False)\n# Final_3.head()\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "final_UR: 8842843\nchecking final_ur here : [Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No='')]\nchecking final_ur_removed_null here : [Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No='')]\nNo of non-blank account_no in final_UR_removed_null 4063540\nchecking final_UR_removed_null account no:  [Row(Account_No='86578100'), Row(Account_No='91750064'), Row(Account_No='89825660'), Row(Account_No='84399575'), Row(Account_No='98072161'), Row(Account_No='84320754'), Row(Account_No='98351724'), Row(Account_No='97235235'), Row(Account_No='83140280'), Row(Account_No='97248767')]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Phase 5\n- Removing duplicate address with different address ID\n\n----\n========================================= THIS IS THE START OF PHASE 5 ============================================\n- taken from Glue Job: address_standardization-prod-uams_generation_final_7_backup (Pipeline 7)\n- originally converted to PySpark in Zepp Qubole notebook: https://us.qubole.com/notebooks#recent?id=141821&type=my-notebooks&view=home\n\n## Pipeline 7",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# new_uams = Final_4\nfinal_UR_removed_null = spark.read.orc(UAMS_PySpark_save_path+\"phase_4/{}/final_UR_removed_null.orc\".format(date_key))\nfinal_UR_removed_null = final_UR_removed_null.select(['Key', 'Address_ID', 'Account_No', 'OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag'])\nprint('checking objid value after reading :', final_UR_removed_null.select('OBJID').head(5))\n\n#revision - fakhrul - 20/6/22 - otherwise postcode of 08000 will be read as 8000\n## clean postcode column (make into string, remove float \\.0, select only first 5 digits, lpad with '0')\nfinal_UR_removed_null = final_UR_removed_null.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nfinal_UR_removed_null = final_UR_removed_null.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\n\nnew_uams = final_UR_removed_null\ndel final_UR_removed_null\nprint('new uams without duplicate removal is :', new_uams.count()) # 8145739\n\n## read in OLD UAMS\nold_uams = spark.read.csv(old_fullfeed_path, header=True) # read in a version which has \"Key\"\n#old_uams = pd.read_csv('Old_FULLFEED_UAMS_WITH_KEY.csv') # from source bucket\n# old_uams = spark.read.csv(UAMS_PySpark_save_path+\"uploaded/Old_Fullfeed_UAMS-20220818(1).csv.gz\", header=True) # this looks like the latest version available on AWS as of 13/11/2022 # not sure why this version of Old UAMS Fullfeed DOESN'T have \"Key\" column in it... But I may have to recreate the Key column...\nold_uams = old_uams.select('Key', 'Address_ID', 'Account_No', 'OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag') \n# old_uams = old_uams.withColumn(\"Key\", f.concat_ws(\" ,\", \"House_No\", \"Building_Name\", \"Street_Type_1\", \"Street_1_New\", \"Street_Type_2\", \"Street_2_New\", \"STD_CITY\", \"AREA\", \"POSTCODE\", \"STATE\") )\n# old_uams = old_uams.withColumn(\"Key\", f.regexp_replace(f.upper(f.col(\"Key\")), \" \", \"\") )\n\n## ensure these cols are string, remove float \\.0\nold_uams = old_uams.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nold_uams = old_uams.withColumn(\"OBJID\", f.regexp_replace(f.col('OBJID').cast('string'), '\\.0', '') )\nprint('checking objid value after reading in old uams :', old_uams.select('OBJID').head(10))\nprint('old_uams serviceable checking :', old_uams.groupBy('Serviceable').count().orderBy('count').show())\nprint('old_uams without duplicate removal is :', old_uams.select('Serviceable').count()) # 9381682\nprint('checking old uams info ',old_uams.columns)\n\n## don't know how to pyspark these lines below:\n# print('new_uams sum of nulls:', new_uams.isna().sum())\n# print('old_uams sum of nulls:', old_uams.isna().sum())\n\n## fillna & deal with nulls\nold_uams = old_uams.fillna('')\nold_uams = old_uams.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No'), 'nan|NAN|null', '') )\n\n## clean postcode column (make into string, remove float \\.0, select only first 5 digits, lpad with '0')\nold_uams = old_uams.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nold_uams = old_uams.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\n## Create key to check for duplicates address\nold_uams = old_uams.withColumn(\"duplicate_key\", f.concat_ws(\" ,\", f.col('Account_No').cast('string'),  f.col('House_No').cast('string'),  f.col('Building_Name').cast('string'),  f.col('Street_Type').cast('string'),  f.col('Street_Name').cast('string'),  f.col('Area').cast('string'),  f.col('City').cast('string'),  f.col('Postcode').cast('string'),  f.col('State').cast('string')) )\nold_uams = old_uams.withColumn(\"duplicate_key\", f.regexp_replace(f.upper(f.trim(f.col('duplicate_key').cast('string'))), \" \", \"\") )\n\n# create a sequential index to prepare for the dedupe step next. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nold_uams = old_uams.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n## de-dupe on Key, & keep first based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['duplicate_key']).orderBy(f.col(\"index\").asc())\nold_uams_removed_dup = old_uams.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\nprint('old_uams with duplicate removal is :', old_uams_removed_dup.select('duplicate_key').count()) # 9381639\nprint('old_uams_removed_dup account id checking: ', old_uams_removed_dup.head(2))\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\n## fillna & deal with nulls\nnew_uams = new_uams.fillna('')\nnew_uams = new_uams.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No'), 'nan|NAN|null', '') )\nnew_uams = new_uams.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\n\n## clean postcode column (make into string, remove float \\.0, select only first 5 digits, lpad with '0')\nnew_uams = new_uams.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nnew_uams = new_uams.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\n## Create key to check for duplicates address\nnew_uams = new_uams.withColumn(\"duplicate_key\", f.concat_ws(\" ,\", f.col('Account_No').cast('string'),  f.col('House_No').cast('string'),  f.col('Building_Name').cast('string'),  f.col('Street_Type').cast('string'),  f.col('Street_Name').cast('string'),  f.col('Area').cast('string'),  f.col('City').cast('string'),  f.col('Postcode').cast('string'),  f.col('State').cast('string')) )\nnew_uams = new_uams.withColumn(\"duplicate_key\", f.regexp_replace(f.upper(f.trim(f.col('duplicate_key').cast('string'))), \" \", \"\") )\n\n# create a sequential index to prepare for the dedupe step next. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nnew_uams = new_uams.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n## de-dupe on Key, & keep first based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['duplicate_key']).orderBy(f.col(\"index\").asc())\nnew_uams_removed_dup = new_uams.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\nprint('new uams with duplicate removal is :', new_uams_removed_dup.select('duplicate_key').count()) # 8095903\nprint('new_uams account id checking: ', new_uams_removed_dup.head(2))\n\n# ### Adding AddressID for new UAMS \nold_uams = old_uams_removed_dup\nnew_uams = new_uams_removed_dup\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\n## save intermediate tables --> have to break it up coz it seems like pyspark takes a longgg time trying to run an Action cell with so many transformations\nnew_uams.write.orc(UAMS_PySpark_save_path+\"phase_5/{}/new_uams_intermediate1.orc\".format(date_key), mode='overwrite', compression='snappy')\n# new_uams.coalesce(1).write.csv(UAMS_PySpark_save_path+\"new_uams_temp_7_backup_{}.csv.gz\".format(date_key), header=True, mode='overwrite', compression='gzip')\nold_uams.write.orc(UAMS_PySpark_save_path+\"phase_5/{}/old_uams_intermediate1.orc\".format(date_key), mode='overwrite', compression='snappy')\n# old_uams.coalesce(1).write.csv(UAMS_PySpark_save_path+\"old_uams_temp_7_backup_{}.csv.gz\".format(date_key), header=True, mode='overwrite', compression='gzip')\n\n#Revision - fakhrul - 27/6/22 the merging is too big, it killed the job. have to build another pipeline\n# wr.s3.to_csv(df = new_uams, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_uams_temp_7_backup.csv')\n# wr.s3.to_csv(df = old_uams, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_uams_temp_7_backup.csv')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "checking objid value after reading : [Row(OBJID=''), Row(OBJID=''), Row(OBJID=''), Row(OBJID=''), Row(OBJID='')]\nnew uams without duplicate removal is : 8842843\nchecking objid value after reading in old uams : [Row(OBJID=None), Row(OBJID=None), Row(OBJID=None), Row(OBJID=None), Row(OBJID=None), Row(OBJID='53445914'), Row(OBJID=None), Row(OBJID=None), Row(OBJID='16848414'), Row(OBJID=None)]\n+--------------+-----+\n|   Serviceable|count|\n+--------------+-----+\n| \"   \"\"5-14-9\"|    1|\n| \"    \"\"2-3-1\"|    1|\n| \"     \"\"5-05\"|    1|\n| \" 6220\"\" 42B\"|    1|\n| \"    \"\"01-04\"|    1|\n| \"    \"\"9-3-1\"|    1|\n|\"\"\"1062-12-22\"|    1|\n| \" 6220\"\" 42C\"|    1|\n| \"  \"\"10-5-02\"|    1|\n| \"    \"\"5-2-3\"|    1|\n| \"   \"\"2220-1\"|    1|\n| \"  \"\"123-7-4\"|    1|\n| \"   \"\"125-37\"|    1|\n| \"    \"\"7-2-9\"|    1|\n| \"    \"\"18-19\"|    1|\n| \"  \"\"23 2-17\"|    1|\n| \"  \"\"4-10-11\"|    1|\n| \"   \"\"1A-3-1\"|    1|\n| \"    \"\"1-12B\"|    1|\n| \"  \"\"2C-16-7\"|    1|\n+--------------+-----+\nonly showing top 20 rows\n\nold_uams serviceable checking : None\nold_uams without duplicate removal is : 9361290\nchecking old uams info  ['Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag']\nold_uams with duplicate removal is : 9361246\nold_uams_removed_dup account id checking:  [Row(Key='(153),,JALAN,PERUSAHAAN1,,,SEMENYIH,KAWASANPERINDUSTRIANBERANANG,43700,SELANGOR', Address_ID='1692532.0', Account_No='', OBJID='', House_No='    (153 )', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='PERUSAHAAN 1', Area='KAWASAN PERINDUSTRIAN BERANANG', City='SEMENYIH', Postcode='43700', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='', State='SELANGOR', UR_Flag='URBAN', duplicate_key=',(153),,JALAN,PERUSAHAAN1,KAWASANPERINDUSTRIANBERANANG,SEMENYIH,43700,SELANGOR', index=2376550), Row(Key='(16-18-11),GRANDVIEWHEIGHTS,LORONG,BUKITKUKUS,,,GEORGETOWN,PAYATERUBONG,11060,PULAUPINANG', Address_ID='1692533.0', Account_No='', OBJID='', House_No='(16-18-11)', Building_Name='GRAND VIEW HEIGHTS', Standard_Building_Name='YES', Street_Type='LORONG', Street_Name='BUKIT KUKUS', Area='PAYA TERUBONG', City='GEORGE TOWN', Postcode='11060', Address_Type='MDU', Serviceable='', P_Flag='', State='PULAU PINANG', UR_Flag='URBAN', duplicate_key=',(16-18-11),GRANDVIEWHEIGHTS,LORONG,BUKITKUKUS,PAYATERUBONG,GEORGETOWN,11060,PULAUPINANG', index=2376551)]\nnew uams with duplicate removal is : 8662409\nnew_uams account id checking:  [Row(Key='\\t31-2,,JALAN,3/93,,,KUALALUMPUR,TAMANMIHARJA,55200,WILAYAHPERSEKUTUANKUALALUMPUR', Address_ID=3266880.0, Account_No='', OBJID='', House_No='     \\t31-2', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='      3/93', Area='TAMAN MIHARJA', City='KUALA LUMPUR', Postcode='55200', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='', State='WILAYAH PERSEKUTUAN KUALA LUMPUR', UR_Flag='URBAN', duplicate_key=',\\t31-2,,JALAN,3/93,TAMANMIHARJA,KUALALUMPUR,55200,WILAYAHPERSEKUTUANKUALALUMPUR', index=2690718), Row(Key='\\t31-2DM1,,JALAN,3/93,,,KUALALUMPUR,TAMANMIHARJA,55200,WILAYAHPERSEKUTUANKUALALUMPUR', Address_ID=2986459.0, Account_No='', OBJID='', House_No='  \\t31-2DM1', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='      3/93', Area='TAMAN MIHARJA', City='KUALA LUMPUR', Postcode='55200', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='', State='WILAYAH PERSEKUTUAN KUALA LUMPUR', UR_Flag='URBAN', duplicate_key=',\\t31-2DM1,,JALAN,3/93,TAMANMIHARJA,KUALALUMPUR,55200,WILAYAHPERSEKUTUANKUALALUMPUR', index=2690852)]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Phase 6\n- Adding AddressID for new UAMS\n- Compare New AMS and Old AMS\n- Adding Shamani's data to the delta delete\n\n----\n========================================= THIS IS THE START OF PHASE 6 ============================================\n- taken from Glue Job: address_standardization-prod-uams_generation_final_7 (Pipeline 7)\n- originally converted to PySpark in Zepp Qubole notebook: https://us.qubole.com/notebooks#recent?id=141821&type=my-notebooks&view=home\n\n## Pipeline 7",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 7 - FINAL ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_7\n# ### Adding AddressID for new UAMS \n## ====================================\n\nnew_uams = spark.read.orc(UAMS_PySpark_save_path+\"phase_5/{}/new_uams_intermediate1.orc\".format(date_key))\n# new_uams = wr.s3.read_csv('s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_uams_temp_7_backup.csv')\nprint('this is new uams cols', new_uams.columns)\n\nold_uams = spark.read.orc(UAMS_PySpark_save_path+\"phase_5/{}/old_uams_intermediate1.orc\".format(date_key))\n# old_uams = wr.s3.read_csv('s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_uams_temp_7_backup.csv')\nprint('this is old uams cols', old_uams.columns)\n\n## rename column (as spark does not add suffixes to duplicate columns after joining)\nold_uams = old_uams.withColumnRenamed('Address_ID', 'Address_ID_old')\nMerge_new_old_key = new_uams.join(old_uams.select('Key', 'Address_ID_old'), on ='Key', how = 'left')\nprint('Merge_new_old_key count', Merge_new_old_key.select('Key').count()) # 21008589\nprint('checking merge new old account no unique values :', Merge_new_old_key.groupBy('Account_No').count())\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\n## save intermediate tables --> have to break it up coz it seems like pyspark takes a longgg time trying to run an Action cell with so many transformations\nMerge_new_old_key.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/merge_new_old_key_intermediate1.orc\".format(date_key), mode='overwrite', compression='snappy')\nMerge_new_old_key.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_6/{}/merge_new_old_key_intermediate1.csv.gz\".format(date_key), header=True, mode='overwrite', compression='gzip')\n\n#Revision - fakhrul - 27/6/22 the merging is too big, it killed the job. have to build another pipeline\n# wr.s3.to_csv(df = Merge_new_old_key, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/merge_new_old_key_temp_7.csv')\n#wr.s3.to_csv(df = old_uams, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_uams_temp_7.csv')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "this is new uams cols ['Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag', 'duplicate_key', 'index']\nthis is old uams cols ['Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag', 'duplicate_key', 'index']\nMerge_new_old_key count 21574347\nchecking merge new old account no unique values : DataFrame[Account_No: string, count: bigint]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Pipeline 8",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 8 - BACKUP 1 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_8_backup\n# according to Fakhrul, the order for pipeline 8 is final_8_backup -> final_8_backup_2 -> final_8_backup_3 -> final_8\n# ----------------------------------\n### Split new and old UAMS\n\n## 13/11/2022: Amzar's pyspark method does not run the for loop that Fakhrul used\nMerge_new_old_key = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/merge_new_old_key_intermediate1.orc\".format(date_key)) # count = 21008589\nNew_address = Merge_new_old_key.filter(f.col('Address_ID_old').isNull()) \nOld_address = Merge_new_old_key.filter(f.col('Address_ID_old').isNotNull()) \nprint('New Address count:', New_address.select('Address_ID_old').count()) # 3959358\nprint('Old Address count:', Old_address.select('Address_ID_old').count()) # 17049231\n\n## save the files\nNew_address.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/new_address_intermediate1.orc\".format(date_key), mode='overwrite', compression='snappy')\nOld_address.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/old_address_intermediate1.orc\".format(date_key), mode='overwrite', compression='snappy')\n\n#Revision - fakhrul - 27/6/22 - dropped duplicate key and unnamed column and also address_ID_x to save on memory\n# chunksize = 100000\n# count = 1\n# for Merge_new_old_key in wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/merge_new_old_key_temp_7.csv', usecols = ['Key','Account_No','OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag','Address_ID_y'], chunksize=chunksize):\n#     New_address = Merge_new_old_key[Merge_new_old_key['Address_ID_y'].isnull()]\n#     Old_address = Merge_new_old_key[Merge_new_old_key['Address_ID_y'].notnull()]\n#     wr.s3.to_csv(df = New_address, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_backup_before_8_folder' + '/' + timestr + '/new_address_backup_before_8_' + str(count) + '.csv')\n#     wr.s3.to_csv(df = Old_address, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_address_backup_before_8_folder' + '/' + timestr + '/old_address_backup_before_8_' + str(count) + '.csv')\n#     count += 1\n\n# print('memory after reading : ')\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "New Address count: 4207663\nOld Address count: 17366684\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 8 - BACKUP 2 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_8_backup_2\n# according to Fakhrul, the order for pipeline 8 is final_8_backup -> final_8_backup_2 -> final_8_backup_3 -> final_8\n# new_address_path = args['new_address_path'] = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_backup_before_8_folder/new_address_backup_before_8_*.csv\n## ================================================================================================================================\n\n#Revision - fakhrul - 27/6/22 - dropped duplicate key and unnamed column and also address_ID_x to save on memory\n#Merge_new_old_key = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/merge_new_old_key_temp_7.csv',\n#usecols = ['Key','Account_No','OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag','Address_ID_y'],\n#dtype = {'Postcode':object, 'Address_ID_y': 'float32', 'Account_No': 'float32', 'OBJID': 'float32'})\n\n## read in old_uams to get latest Address_ID\nold_uams = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/old_address_intermediate1.orc\".format(date_key))\n# old_uams = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_uams_temp_7_backup.csv', usecols = ['Address_ID'], dtype = {'Address_ID': 'int32'})\n\n#print('this is merge info :', Merge_new_old_key.info())\n#print('this is coolumns of merge: ', Merge_new_old_key.columns)\n#print('this is old uams info :', old_uams.info())\n#print('this is coolumns of old_uams: ', old_uams.columns)\n\n#print('memory after reading : ')\n#usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n#print('[debug] memory usage is (Megabytes):')\n#print(usage)\n\n### Add Address_ID to New_AMS\n\n## first extract the last Address_ID currently available (max)\nLast_Address_ID = old_uams.select(f.max(f.col('Address_ID').cast('integer'))).first()[0]\nprint('Last_Address_ID:', Last_Address_ID, 'Last_Address_ID type:', type(Last_Address_ID)) # 12841512 \n\n## read in New_address\nNew_address = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/new_address_intermediate1.orc\".format(date_key))\n\n# New_address = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_backup_before_8_folder/' + timestr + '/' + 'new_address_backup_before_8_*.csv')\n#print('this is last address id max:', Last_Address_ID)\n#\n#New_address = Merge_new_old_key[Merge_new_old_key['Address_ID_y'].isnull()]\n#Old_address = Merge_new_old_key[Merge_new_old_key['Address_ID_y'].notnull()]\n#\n#print('memory after not null and null : ')\n#usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n#print('[debug] memory usage is (Megabytes):')\n#print(usage)\n#\n#print('acc no of new: ', New_address[New_address.Account_No.isnull()])\n#print('p flag of new: ', New_address[New_address.P_Flag.isnull()])\n#print('acc no of old: ', Old_address[Old_address.Account_No.isnull()])\n#print('p flag of old: ', Old_address[Old_address.P_Flag.isnull()])\n\n\n#revision - fakhrul - 3/7/22 - regex to see if it makes sense or not\n#New_address= New_address[New_address[\"House_No\"]!= \"-\"]\n#New_address= New_address[New_address[\"House_No\"]!= \"\\*\"]\n#New_address= New_address[New_address[\"House_No\"]!= \"\\&\"]\n#New_address= New_address[New_address[\"House_No\"]!= \"\\+\"]\n#New_address= New_address[New_address[\"House_No\"]!= \"\\*/\"]\n#New_address= New_address[New_address[\"House_No\"]!= \"\\-..-\"]\n\n#New_address['House_No'] = New_address['House_No'].astype(str)\n#New_address = New_address[~New_address['House_No'].str.contains('-', regex = True)]\n#New_address = New_address[~New_address['House_No'].str.contains('\\*', regex = True)]\n#New_address = New_address[~New_address['House_No'].str.contains('\\&', regex = True)]\n#New_address = New_address[~New_address['House_No'].str.contains('\\+', regex = True)]\n#New_address = New_address[~New_address['House_No'].str.contains('\\*/', regex = True)]\n#New_address = New_address[~New_address['House_No'].str.contains('\\-..-', regex = True)]\n#New_address = New_address[~New_address['House_No'].str.contains('\\:', regex = True)]\n#New_address = New_address[~New_address['House_No'].str.contains('VALUE', regex = True)]\n#New_address = New_address[~New_address['House_No'].str.contains('nan', regex = True)]\n#New_address = New_address[New_address['Building_Name'] != New_address['State']]\n#New_address = New_address[New_address['Building_Name'] != New_address['City']]\n#New_address = New_address[New_address['Building_Name'] != 'KUALA TERENGGANU']\n\n#revision - fakhrul 8/7/22 - testing this out \n#New_address['Key'] = New_address['Key'].astype(str)\n#New_address = New_address[~New_address['Key'].str.contains(\"[`^_]\", regex = True)]\n\nprint('checking new address here: ', New_address.count(), New_address.columns) # 3959358 \n#print('checking old address here: ', Old_address.shape)\n\n#wr.s3.to_csv(df = New_address, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_test_check.csv')\n\n#print('checking memory usage here to see if its ridiculous or not : ')\n#usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n#print('[debug] memory usage is (Megabytes):')\n#print(usage)\n\n#print('checking new address shape', New_address.shape)\n#print('checking old address shape', Old_address.shape)\n\n# create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nNew_address = New_address.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n## de-dupe on Key, & keep LAST based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Key']).orderBy(f.col(\"index\").desc()) ## descending because we want to keep 'last'\nNew_address = New_address.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\nprint('New_address count AFTER de-dupe on Key:', New_address.select('Key').count()) # 3459577\n\n## Generate new ID for new address\ni = Last_Address_ID\nprint(i) # 12841512\nj= i+2 # 12841514\n# New_address = New_address.reset_index()\n\nNew_address = New_address.withColumn('Address_ID_old', f.col('index') + j)\n# New_address['Address_ID_y'] = New_address.index + j\nprint('this is new address account no checking', New_address.select('Account_No').head(10))\n\n#Old_address = Old_address.drop_duplicates(subset = ['Account_No', 'Address_ID_y'])\n#print('this is old address account no checking', Old_address.Account_No.head(100))\n\n## save the files\nNew_address.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/new_address_intermediate2.orc\".format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_csv(df = New_address, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_backup_before_8_2.csv')\n#wr.s3.to_csv(df = Old_address, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_address_backup_before_8.csv')\n\ndel old_uams\n\n# print('checking memory usage here to see if its ridiculous or not : ')\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "Last_Address_ID: 8018089 Last_Address_ID type: <class 'int'>\nchecking new address here:  4207663 ['Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag', 'duplicate_key', 'index', 'Address_ID_old']\nNew_address count AFTER de-dupe on Key: 3713748\n8018089\nthis is new address account no checking [Row(Account_No='97561971'), Row(Account_No=''), Row(Account_No=''), Row(Account_No='97725472'), Row(Account_No=''), Row(Account_No='97438645'), Row(Account_No='98624835'), Row(Account_No='97515688'), Row(Account_No='97532218'), Row(Account_No='97522870')]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 8 - BACKUP 3 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_8_backup_3\n# according to Fakhrul, the order for pipeline 8 is final_8_backup -> final_8_backup_2 -> final_8_backup_3 -> final_8\n# old_address_path = args['old_address_path'] = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_address_backup_before_8_folder/old_address_backup_before_8*.csv\n## ================================================================================================================================\n\n## read in Old_address\nOld_address = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/old_address_intermediate1.orc\".format(date_key))\n# Old_address = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_address_backup_before_8_folder/' + timestr + '/' + 'old_address_backup_before_8*.csv')\n\nprint('Old_address count BEFORE de-dupe on Account_No & Address_ID_old:', Old_address.select('Key').count()) # 17049231\n\n# create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nOld_address = Old_address.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n## de-dupe on 'Account_No', 'Address_ID_old', & keep FIRST based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Account_No', 'Address_ID_old']).orderBy(f.col(\"index\").asc()) ## ascending because we want to keep 'first'\nOld_address = Old_address.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\nprint('Old_address count AFTER de-dupe on Account_No & Address_ID_old:', Old_address.select('Key').count()) # 4122207\nprint('this is old address account no checking', Old_address.filter(f.col('Account_No') != '').select('Account_No').head(10))\n\n## save the files\nOld_address.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/old_address_intermediate2.orc\".format(date_key), mode='overwrite', compression='snappy')\n#wr.s3.to_csv(df = New_address, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_backup_before_8_3.csv')\n# wr.s3.to_csv(df = Old_address, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_address_backup_before_8_3.csv')\n\n# print('checking memory usage here to see if its ridiculous or not : ')\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Megabytes):')\n# print(usage)\n\ndel Old_address",
			"metadata": {
				"trusted": true
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "Old_address count BEFORE de-dupe on Account_No & Address_ID_old: 17366684\nOld_address count AFTER de-dupe on Account_No & Address_ID_old: 4439602\nthis is old address account no checking [Row(Account_No='80000008'), Row(Account_No='80000011'), Row(Account_No='80000013'), Row(Account_No='80000015'), Row(Account_No='80000032'), Row(Account_No='80000033'), Row(Account_No='80000050'), Row(Account_No='80000053'), Row(Account_No='80000056'), Row(Account_No='80000060')]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 8 - FINAL ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_8\n# according to Fakhrul, the order for pipeline 8 is final_8_backup -> final_8_backup_2 -> final_8_backup_3 -> final_8\n# new_fullfeed_path = s3://astro-groupdata-prod-source/new_fullfeed_uams_with_key/\n# old_uams_temp_save_path = s3://astro-groupdata-prod-pip eline/address_standardization/uams_temp_final/\n# new_address_temp_save_path = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/\n## ================================================================================================================================\n\n## read in the new_address & old_address\nNew_address = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/new_address_intermediate2.orc\".format(date_key))\nOld_address = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/old_address_intermediate2.orc\".format(date_key))\n# New_address = wr.s3.read_csv('s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_backup_before_8_2.csv')\n# #Old_address = wr.s3.read_csv('s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_address_backup_before_8.csv')\n# Old_address = wr.s3.read_csv('s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_address_backup_before_8_3.csv')\n\n#revision - fakhrul - 13/8/22 - remove this below as we dont use it at all\n#old_uams = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_uams_temp_7.csv', usecols = ['Address_ID'], dtype = {'Address_ID': 'int'})\n\nprint('New_address shape', New_address.select('Key').count()) #  3713748\nprint('Old_address shape', Old_address.select('Key').count()) # 4439602\n\n## Concat/union Old_address with New_address\nprint(Old_address.columns, New_address.columns)\nNew_AMS = Old_address.union(New_address)\n# Frame = [Old_address,New_address]\n# New_AMS = pd.concat(Frame)\nprint('checking new AMS shape', New_AMS.select('Key').count()) #  8153350\n\n## select & rename columns\nNew_AMS = New_AMS.select(['Key', 'Address_ID_old', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City',\n       'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State','UR_Flag']) \nNew_AMS = New_AMS.withColumnRenamed('Address_ID_old','Address_ID')\n\nprint('checking head of new uams fullfeed : ', New_AMS.head(10))\nprint('checking head of account no new uams fullfeed : ', New_AMS.select('Account_No').head(10))\nprint('checking number of rows in acc no of new uams fullfeed : ', New_AMS.select('Account_No').count())\nprint('checking nan account no in new uams :', New_AMS.filter(f.col('Account_No').isNull()).count()) # 7581784\nprint('checking empty account no in new uams :', New_AMS.filter(f.col('Account_No') == '').count()) # 0\nprint('checking OBJID of new fullfeed: ', New_AMS.select('OBJID').distinct().show()) # 4776402\n\n## clean postcode column (make into string, remove float \\.0, select only first 5 digits, lpad with '0')\nNew_AMS = New_AMS.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nNew_AMS = New_AMS.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\nprint('heyyyyyyyyyyyyy', New_AMS.select(f.length(f.col('Postcode').cast('string'))).distinct().show())\n\n## clean some other columns\nNew_AMS = New_AMS.withColumn(\"Address_ID\", f.regexp_replace(f.col('Address_ID').cast('string'), '\\.0', '') )\nNew_AMS = New_AMS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\n\n#revision - fakhrul - 5/8/22 - remove the weird double space entry\nNew_AMS = New_AMS.withColumn('Address_Type', when(f.col('Building_Name') == '  ', 'SDU').otherwise(f.col('Address_Type')) )\nNew_AMS = New_AMS.withColumn(\"Building_Name\", f.regexp_replace(f.col('Building_Name').cast('string'), '  ', '') )\n\nprint('this is P1 numbers :', New_AMS.filter(f.col('P_Flag') == 'P1').select('Account_No').count() )\nprint('this is P2 numbers :', New_AMS.filter(f.col('P_Flag') == 'P2').select('Account_No').count() )\n\nNew_AMS = New_AMS.filter(~f.col(\"Serviceable\").cast('string').contains('ERROR'))\nprint('Count of New_AMS',New_AMS.select(\"Serviceable\").count())\nprint('Unique serviceable values in New_AMS', New_AMS.select('Serviceable').distinct().show())\n\n## save files\nNew_AMS.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/NEW_FULLFEED_UAMS_WITH_KEY_{}.orc\".format(date_key, date_key), mode='overwrite', compression='snappy')\nNew_AMS.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_6/{}/NEW_FULLFEED_UAMS_WITH_KEY_{}.csv.gz\".format(date_key, date_key), mode='overwrite', header=True, compression='gzip')\nNew_address.write.orc(UAMS_PySpark_save_path+\"new_address_temp_8_{}.orc\".format(date_key), mode='overwrite', compression='snappy') # this file doesn't seem to be used later on anywher\n\n#wr.s3.to_csv(df = New_AMS, path = old_uams_temp_save_path + 'NEW_FULLFEED_UAMS_WITH_KEY_postcode_check_with_padding.csv')\n# wr.s3.to_csv(df = New_AMS, path = new_fullfeed_path + 'NEW_FULLFEED_UAMS_WITH_KEY.csv')\n#wr.s3.to_csv(df = New_AMS, path = new_address_temp_save_path + 'NEW_FULLFEED_UAMS_WITH_KEY.parquet')\n#New_AMS.to_csv('NEW_FULLFEED_UAMS_WITH_KEY.csv') ## Save in pipeline bucket\n#wr.s3.to_csv(df = old_uams, path = old_uams_temp_save_path + 'old_uams_temp_8.csv')\n# wr.s3.to_csv(df = New_address, path = new_address_temp_save_path + 'new_address_temp_8.csv')\n",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "New_address shape 3713748\nOld_address shape 4439602\n['Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag', 'duplicate_key', 'index', 'Address_ID_old'] ['Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag', 'duplicate_key', 'index', 'Address_ID_old']\nchecking new AMS shape 8153350\nchecking head of new uams fullfeed :  [Row(Key='B2-18-03,KENWINGSTONSQUAREGARDENBLOKB2,PERSIARAN,BESTARI,,,CYBERJAYA,CYBER9,63000,SELANGOR', Address_ID='10000001.0', Account_No='', OBJID='', House_No='  B2-18-03', Building_Name='KENWINGSTON SQUARE GARDEN BLOK B2', Standard_Building_Name='NO', Street_Type='PERSIARAN', Street_Name='   BESTARI', Area='CYBER 9', City='CYBERJAYA', Postcode='63000', Address_Type='MDU', Serviceable='TM|FTTH', P_Flag='', State='SELANGOR', UR_Flag='URBAN'), Row(Key='B2-18-06,KENWINGSTONSQUAREGARDENBLOKB2,PERSIARAN,BESTARI,,,CYBERJAYA,CYBER9,63000,SELANGOR', Address_ID='10000004.0', Account_No='', OBJID='', House_No='  B2-18-06', Building_Name='KENWINGSTON SQUARE GARDEN BLOK B2', Standard_Building_Name='NO', Street_Type='PERSIARAN', Street_Name='   BESTARI', Area='CYBER 9', City='CYBERJAYA', Postcode='63000', Address_Type='MDU', Serviceable='TM|FTTH', P_Flag='', State='SELANGOR', UR_Flag='URBAN'), Row(Key='B2-18-07,KENWINGSTONSQUAREGARDENBLOKB2,PERSIARAN,BESTARI,,,CYBERJAYA,CYBER9,63000,SELANGOR', Address_ID='10000007.0', Account_No='', OBJID='', House_No='  B2-18-07', Building_Name='KENWINGSTON SQUARE GARDEN BLOK B2', Standard_Building_Name='NO', Street_Type='PERSIARAN', Street_Name='   BESTARI', Area='CYBER 9', City='CYBERJAYA', Postcode='63000', Address_Type='MDU', Serviceable='TM|FTTH', P_Flag='', State='SELANGOR', UR_Flag='URBAN'), Row(Key='B2-18-08,KENWINGSTONSQUAREGARDENBLOKB2,PERSIARAN,BESTARI,,,CYBERJAYA,CYBER9,63000,SELANGOR', Address_ID='10000009.0', Account_No='', OBJID='', House_No='  B2-18-08', Building_Name='KENWINGSTON SQUARE GARDEN BLOK B2', Standard_Building_Name='NO', Street_Type='PERSIARAN', Street_Name='   BESTARI', Area='CYBER 9', City='CYBERJAYA', Postcode='63000', Address_Type='MDU', Serviceable='TM|FTTH', P_Flag='', State='SELANGOR', UR_Flag='URBAN'), Row(Key='B2-18-09,KENWINGSTONSQUAREGARDENBLOKB2,PERSIARAN,BESTARI,,,CYBERJAYA,CYBER9,63000,SELANGOR', Address_ID='10000012.0', Account_No='', OBJID='', House_No='  B2-18-09', Building_Name='KENWINGSTON SQUARE GARDEN BLOK B2', Standard_Building_Name='NO', Street_Type='PERSIARAN', Street_Name='   BESTARI', Area='CYBER 9', City='CYBERJAYA', Postcode='63000', Address_Type='MDU', Serviceable='TM|FTTH', P_Flag='', State='SELANGOR', UR_Flag='URBAN'), Row(Key='B2-18-09,LAKEPOINTRESIDENCEB2,PERSIARAN,SEPANG,,,CYBERJAYA,TAMANSAINSSELANGOR2,63000,SELANGOR', Address_ID='10000013.0', Account_No='', OBJID='', House_No='  B2-18-09', Building_Name='LAKE POINT RESIDENCE B2', Standard_Building_Name='NO', Street_Type='PERSIARAN', Street_Name='    SEPANG', Area='TAMAN SAINS SELANGOR 2', City='CYBERJAYA', Postcode='63000', Address_Type='MDU', Serviceable='TM|FTTH', P_Flag='', State='SELANGOR', UR_Flag='URBAN'), Row(Key='B2-19,,JALAN,1/18,,,SEMENYIH,BANDARTEKNOLOGIKAJANG,43500,SELANGOR', Address_ID='10000018.0', Account_No='', OBJID='', House_No='     B2-19', Building_Name='', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='      1/18', Area='BANDAR TEKNOLOGI KAJANG', City='SEMENYIH', Postcode='43500', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='', State='SELANGOR', UR_Flag='URBAN'), Row(Key='B2-19,,PERSIARAN,JELAPANG6,,,IPOH,TAMANSRIWAN,30100,PERAK', Address_ID='10000019.0', Account_No='', OBJID='', House_No='     B2-19', Building_Name='', Standard_Building_Name='NO', Street_Type='PERSIARAN', Street_Name='JELAPANG 6', Area='TAMAN SRI WAN', City='IPOH', Postcode='30100', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='', State='PERAK', UR_Flag='URBAN'), Row(Key='B2-19,BLOKB,JALAN,BEREMBANG,,,PELABUHANKLANG,PELABUHANSELATAN,42000,SELANGOR', Address_ID='10000020.0', Account_No='', OBJID='', House_No='     B2-19', Building_Name='BLOK B', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name=' BEREMBANG', Area='PELABUHAN SELATAN', City='PELABUHAN KLANG', Postcode='42000', Address_Type='MDU', Serviceable='TM|FTTH', P_Flag='', State='SELANGOR', UR_Flag='URBAN'), Row(Key='B2-19,BLOKBFLATTAMANMAWAR,JALAN,TK5/26,,,PUCHONG,TAMANKINRARA,47190,SELANGOR', Address_ID='10000024.0', Account_No='', OBJID='', House_No='     B2-19', Building_Name='BLOK B FLAT TAMAN MAWAR', Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='   TK 5/26', Area='TAMAN KINRARA', City='PUCHONG', Postcode='47190', Address_Type='MDU', Serviceable='TM|FTTH', P_Flag='', State='SELANGOR', UR_Flag='URBAN')]\nchecking head of account no new uams fullfeed :  [Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No='')]\nchecking number of rows in acc no of new uams fullfeed :  8153350\nchecking nan account no in new uams : 0\nchecking empty account no in new uams : 4741628\n+--------+\n|   OBJID|\n+--------+\n|84755396|\n|82145658|\n|68036770|\n|55606748|\n|86999077|\n|88550324|\n|28707765|\n|80070160|\n|10550457|\n|87678162|\n|83275335|\n|34581047|\n|20712563|\n|70939216|\n|87217121|\n|20992038|\n|86202616|\n|82564577|\n|16844393|\n|81155925|\n+--------+\nonly showing top 20 rows\n\nchecking OBJID of new fullfeed:  None\n+--------------------------------+\n|length(CAST(Postcode AS STRING))|\n+--------------------------------+\n|                               5|\n+--------------------------------+\n\nheyyyyyyyyyyyyy None\nthis is P1 numbers : 1414929\nthis is P2 numbers : 1304368\nCount of New_AMS 8153350\n+--------------------+\n|         Serviceable|\n+--------------------+\n|  Maxis|FTTH,TM|FTTH|\n|TM|VDSL,maxis|FTT...|\n|          Maxis|FTTH|\n|          maxis|VDSL|\n|          ALLO|FTTH,|\n|TM|VDSL,ALLO|FTTH...|\n|  TM|VDSL,maxis|FTTH|\n|     TM|VDSL,TM|FTTH|\n|  maxis|FTTH,TM|FTTH|\n|          maxis|FTTH|\n|                    |\n|  TM|VDSL,maxis|VDSL|\n|  maxis|VDSL,TM|FTTH|\n|           CTS|FTTH,|\n|            ,TM|FTTH|\n|TM|VDSL,maxis|FTT...|\n|TM|1-G-8 JALAN PU...|\n|    CTS|FTTH,TM|FTTH|\n|          Maxis|VDSL|\n|            CTS|FTTH|\n+--------------------+\nonly showing top 20 rows\n\nUnique serviceable values in New_AMS None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Pipeline 9",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 9 - Backup 1 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_9_backup\n# according to Fakhrul, the order for pipeline 9 is final_9_backup -> final_9_backup_2 -> final_9_backup_3 -> final_9_backup_4 -> final_9_backup_5 -> final_9_backup_6 -> final_9\n## ================================================================================================================================\n\n## read in old_uams\n# old_uams = spark.read.csv(UAMS_PySpark_save_path+\"uploaded/Old_Fullfeed_UAMS-20220818(1).csv.gz\", header=True) # from Qubole Zep (but it's not the same file as the old_uams read in earlier...)\nold_uams = spark.read.csv(old_fullfeed_path, header=True)\nprint(old_uams.columns)\nold_uams = old_uams.select(['Address_ID','Account_No','OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag'])\n#old_uams = wr.s3.read_csv(path = old_uams_temp_read_path, usecols = ['Key','Address_ID','Account_No','OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag','duplicate_key'], dtype = {'Postcode': object})\n# old_uams = wr.s3.read_csv(path = 's3://astro-groupdata-prod-source/old_fullfeed_uams_with_key/Old_FUllfeed_UAMS_18_Aug (1).csv', usecols = ['Address_ID','Account_No','OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag'], dtype = {'Postcode': object})\n\n#revision - fakhrul - 20/6/22 - otherwise postcode of 08000 will be read as 8000\n## clean postcode column (make into string, remove float \\.0, select only first 5 digits, lpad with '0')\nold_uams = old_uams.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nold_uams = old_uams.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\n\n## read in New_AMS\nNew_AMS = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/NEW_FULLFEED_UAMS_WITH_KEY_{}.orc\".format(date_key, date_key))\nNew_AMS = New_AMS.select('Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name','Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City','Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State','UR_Flag')\n# New_AMS = wr.s3.read_csv(path = new_fullfeed_with_key_read_path, usecols = ['Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name','Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City','Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State','UR_Flag'], dtype = {'Postcode':object})\n\n#revision - fakhrul - 20/6/22 - otherwise postcode of 08000 will be read as 8000\n## clean postcode column (make into string, remove float \\.0, select only first 5 digits, lpad with '0')\nNew_AMS = New_AMS.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nNew_AMS = New_AMS.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\n\n#revision - fakhrul - 27/6/22 removed address_id_x and duplicate_key columns here as prev we removed it. we shall see if it works\n#New_address = wr.s3.read_csv(path = new_address_temp_read_path, usecols = ['index','Key','Account_No','OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag','Address_ID_y'], dtype = {'Postcode':object})\n#print('shape of new address is: ', New_address.shape)\n\n# ###  Compare New AMS and Old AMS\n#revision - fakhrul - 20/6/22 - otherwise postcode of 08000 will be read as 8000\n#New_address['Postcode']= New_address['Postcode'].astype(str).apply(lambda x:x[0:5])\n#New_address['Postcode'] = New_address['Postcode'].str.pad(width=5, side='left', fillchar='0')  \n\nprint(old_uams.head(5))\n#print(New_address.head())\n\n### ---- Creating combined key (looks more like I'm deleting based on Account_No)\nOld_AMS = old_uams\n\n## first do some cleaning in both Old_AMS & New_AMS\nOld_AMS = Old_AMS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), 'nan', '') )\nOld_AMS = Old_AMS.fillna('', subset='Account_No')\nOld_AMS = Old_AMS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nOld_AMS = Old_AMS.withColumn(\"Address_ID\", f.regexp_replace(f.col('Address_ID').cast('string'), '\\.0', '') )\n\nNew_AMS = New_AMS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), 'nan', '') )\nNew_AMS = New_AMS.fillna('', subset='Account_No')\nNew_AMS = New_AMS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nNew_AMS = New_AMS.withColumn(\"Address_ID\", f.regexp_replace(f.col('Address_ID').cast('string'), '\\.0', '') )\n\n## rename columns before the join (coz spark doesn't add suffixes to duplicate columns after joins)\nOld_AMS = Old_AMS.withColumnRenamed(\"Account_No\", \"Account_No_old\")\nMerge_Old_New_ONID = Old_AMS.select(['Address_ID','Account_No_old']).join(New_AMS, on ='Address_ID', how = 'inner')\nprint('Merge_Old_New_ONID count post JOIN', Merge_Old_New_ONID.select('Account_No').count()) # 4089257\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage after the merging isssss (Bytes):')\n# print(usage)\n\nUpdated_accounts = Merge_Old_New_ONID.filter(f.col('Account_No_old') != f.col('Account_No'))\nprint('Updated_accounts count post filter on Account_No_old != Account_No', Updated_accounts.select('Account_No').count()) # 644557\n\n# create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nUpdated_accounts = Updated_accounts.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n## de-dupe on 'Account_No', 'Address_ID_old', & keep FIRST based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Address_ID', 'Account_No']).orderBy(f.col(\"index\").asc()) ## ascending because we want to keep 'first'\nUpdated_accounts = Updated_accounts.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\nprint('Updated_accounts count AFTER de-dupe on Address_ID & Account_No:', Updated_accounts.select('Account_No').count()) # 644557\n\n## select relevant columns & rename some\nUpdated_accounts = Updated_accounts.select(['Address_ID', 'Account_No_old', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City',\n       'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State','UR_Flag'])\nUpdated_accounts = Updated_accounts.withColumnRenamed('Account_No_old','Account_No')\n\nprint('final updated accounts count:', Updated_accounts.count()) # 644557\nprint('checking Serviceable values in updated_accounts:', Updated_accounts.groupBy('Serviceable').count().orderBy('count', ascending=False).show())\n\n## save files\nUpdated_accounts.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_Updated_accounts.orc\".format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_csv(df = Updated_accounts, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/Updated_accounts_9_backup.csv')\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage after the merging isssss (Bytes):')\n# print(usage)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "['_c0', 'Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag']\n[Row(Address_ID='2556933.0', Account_No='80000006.0', OBJID=None, House_No='        15', Building_Name=None, Standard_Building_Name='NO', Street_Type='LORONG', Street_Name='BAGAN JERMAL', Area=None, City='GEORGE TOWN', Postcode='10250', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='P1', State='PULAU PINANG', UR_Flag='URBAN'), Row(Address_ID='1043943.0', Account_No='80000008.0', OBJID=None, House_No='        46', Building_Name=None, Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='  SS 14/5B', Area='SS 14', City='SUBANG JAYA', Postcode='47500', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='P1', State='SELANGOR', UR_Flag='URBAN'), Row(Address_ID='1232373.0', Account_No='80000011.0', OBJID=None, House_No='        63', Building_Name=None, Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='  PENCHALA', Area='PJS 4', City='PETALING JAYA', Postcode='46050', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='P1', State='SELANGOR', UR_Flag='URBAN'), Row(Address_ID='4842594.0', Account_No='80000013.0', OBJID=None, House_No='       595', Building_Name=None, Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='   ASEAN 3', Area='TAMAN ASEAN', City='MELAKA', Postcode='75250', Address_Type='SDU', Serviceable='TM|FTTH,ALLO|FTTH', P_Flag='P1', State='MELAKA', UR_Flag='URBAN'), Row(Address_ID='1226336.0', Account_No='80000033.0', OBJID=None, House_No='        62', Building_Name=None, Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='  USJ 2/5E', Area='USJ 2', City='SUBANG JAYA', Postcode='47600', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='P1', State='SELANGOR', UR_Flag='URBAN')]\nMerge_Old_New_ONID count post JOIN 20931320\nUpdated_accounts count post filter on Account_No_old != Account_No 15186338\nUpdated_accounts count AFTER de-dupe on Address_ID & Account_No: 2586838\nfinal updated accounts count: 2586838\n+------------------+-------+\n|       Serviceable|  count|\n+------------------+-------+\n|           TM|FTTH|1771932|\n|                  | 639743|\n|           TM|VDSL|  47773|\n| ALLO|FTTH,TM|FTTH|  36360|\n|          ,TM|FTTH|  35018|\n|  CTS|FTTH,TM|FTTH|  20053|\n|maxis|FTTH,TM|FTTH|  16202|\n|         ALLO|FTTH|   6361|\n|        maxis|FTTH|   5127|\n|          CTS|FTTH|   3003|\n|TM|VDSL,maxis|FTTH|   2832|\n|        Maxis|FTTH|    938|\n|TM|VDSL,maxis|VDSL|    284|\n|maxis|VDSL,TM|FTTH|    276|\n| TM|VDSL,ALLO|FTTH|    245|\n|          TM|VDSL,|    213|\n|        ALLO|FTTH,|    205|\n|   TM|VDSL,TM|FTTH|    126|\n|        maxis|VDSL|     48|\n|Maxis|FTTH,TM|FTTH|     32|\n+------------------+-------+\nonly showing top 20 rows\n\nchecking Serviceable values in updated_accounts: None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 9 - Backup 2 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_9_backup_2\n# according to Fakhrul, the order for pipeline 9 is final_9_backup -> final_9_backup_2 -> final_9_backup_3 -> final_9_backup_4 -> final_9_backup_5 -> final_9_backup_6 -> final_9\n# args = getResolvedOptions(sys.argv,\n                        #   ['old_uams_temp_read_path', = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_uams_temp_7.csv\n                        #   'new_fullfeed_with_key_read_path', = s3://astro-groupdata-prod-source/new_fullfeed_uams_with_key/NEW_FULLFEED_UAMS_WITH_KEY.csv\n                        #   'new_address_temp_read_path']) = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_temp_8.csv\n## ================================================================================================================================\n\n# ### ---- Creating combined key\nOld_AMS = Old_AMS.withColumnRenamed('Account_No_old', 'Account_No')\nOld_AMS = Old_AMS.withColumn('combined_Key', f.concat_ws(\",\", f.col('Account_No').cast('string'), f.col('Address_ID').cast('string')) )\nNew_AMS = New_AMS.withColumn('combined_Key', f.concat_ws(\",\", f.col('Account_No').cast('string'), f.col('Address_ID').cast('string')) )\n\n## rename columns to smoothen the join\nNew_AMS = New_AMS.withColumnRenamed('Address_ID', 'Address_ID_new')\nMerge_Old_New_AMS = Old_AMS.join(New_AMS.select('Address_ID_new', 'combined_Key'), on ='combined_Key', how = 'left')\nprint('Old_AMS count:', Old_AMS.count(), 'New_AMS count:', New_AMS.count(), 'Merge_Old_New_AMS (after joining) count:', Merge_Old_New_AMS.count()) \n# Old_AMS count: 9435243 New_AMS count: 7581784 Merge_Old_New_AMS (after joining) count: 9435243\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage after the merging isssss (Bytes):')\n# print(usage)\n\n## filtering & dedupe\nRemoved_accounts = Merge_Old_New_AMS.filter(f.col('Address_ID_new').isNull())\nprint('removed_accounts (null Address ID new) count:', Removed_accounts.select(\"Address_ID\").count()) # 5990543\nRemoved_accounts = Removed_accounts.filter(f.col('Account_No') != '')\nprint('removed_accounts after filtering out blank Account_No count:', Removed_accounts.select(\"Address_ID\").count()) # 1493494\n\n# create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nRemoved_accounts = Removed_accounts.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n## de-dupe on 'Account_No' & keep FIRST based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['Account_No']).orderBy(f.col(\"index\").asc()) ## ascending because we want to keep 'first'\nRemoved_accounts = Removed_accounts.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\nprint('removed_accounts after dedupe on Account_No, keep first count:', Removed_accounts.select(\"Address_ID\").count())  # 1486736\n\n## select & rename columns\nRemoved_accounts = Removed_accounts.select([ 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City',\n       'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State','UR_Flag'])\n\nprint('checking final removed accs count : ', Removed_accounts.count()) # 1486736\n\n## save files\nRemoved_accounts.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_Removed_accounts.orc\".format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_csv(df = Removed_accounts, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/Removed_accounts_9_backup_2.csv')\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage after the merging isssss (Bytes):')\n# print(usage)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "Old_AMS count: 9361290 New_AMS count: 8153350 Merge_Old_New_AMS (after joining) count: 10095118\nremoved_accounts (null Address ID new) count: 4350136\nremoved_accounts after filtering out blank Account_No count: 1355526\nremoved_accounts after dedupe on Account_No, keep first count: 1355140\nchecking final removed accs count :  1355140\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 9 - Backup 3 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_9_backup_3\n# according to Fakhrul, the order for pipeline 9 is final_9_backup -> final_9_backup_2 -> final_9_backup_3 -> final_9_backup_4 -> final_9_backup_5 -> final_9_backup_6 -> final_9\n# args = getResolvedOptions(sys.argv,\n                        #   ['old_uams_temp_read_path', = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_uams_temp_7.csv\n                        #   'new_fullfeed_with_key_read_path', = s3://astro-groupdata-prod-source/new_fullfeed_uams_with_key/NEW_FULLFEED_UAMS_WITH_KEY.csv\n                        #   'new_address_temp_read_path']) = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_temp_8.csv\n## ================================================================================================================================\n\n## rename columns for a smoother join\nOld_AMS = Old_AMS.withColumnRenamed('Serviceable', 'Serviceable_old')\nInner_Old_New_AMS = Old_AMS.select(['combined_Key','Serviceable_old']).join(New_AMS, on ='combined_Key', how = 'inner')\nprint('Inner_Old_New_AMS (after join) count:', Inner_Old_New_AMS.select('Serviceable').count()) # 3444700\n\n## filtering & dedupe\nUpdated_serviceability = Inner_Old_New_AMS.filter(f.col('Serviceable') != f.col('Serviceable_old'))\nprint('Updated_serviceability after filter for records where Serviceable != Serviceable_old count:', Updated_serviceability.select(\"Serviceable\").count())  # 299059\n\n# create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nUpdated_serviceability = Updated_serviceability.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n## de-dupe on combined_Key & keep FIRST based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['combined_Key']).orderBy(f.col(\"index\").asc()) ## ascending because we want to keep 'first'\nUpdated_serviceability = Updated_serviceability.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\nprint('Updated_serviceability after dedupe on combined_Key, keep first count:', Updated_serviceability.select(\"Serviceable\").count())  # 299059\n\n## rename & select columns\nUpdated_serviceability = Updated_serviceability.withColumnRenamed('Address_ID_new', 'Address_ID')\nUpdated_serviceability = Updated_serviceability.select('Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City',\n       'Postcode', 'Address_Type', 'Serviceable_old', 'P_Flag', 'State','UR_Flag')\nUpdated_serviceability = Updated_serviceability.withColumnRenamed('Serviceable_old', 'Serviceable')\nprint('updated serviceability checking :', Updated_serviceability.groupBy('Serviceable').count().orderBy('count', ascending=False).show())\n\n## save files\nUpdated_serviceability.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_Updated_serviceability.orc\".format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_csv(df = Updated_serviceability, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/Updated_serviceability_9_backup_3.csv')\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage after the merging isssss (Bytes):')\n# print(usage)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "Inner_Old_New_AMS (after join) count: 5744982\nUpdated_serviceability after filter for records where Serviceable != Serviceable_old count: 564921\nUpdated_serviceability after dedupe on combined_Key, keep first count: 564173\n+--------------------+------+\n|         Serviceable| count|\n+--------------------+------+\n|             TM|FTTH|233866|\n|             TM|VDSL|203343|\n|          Maxis|FTTH| 27204|\n|           ALLO|FTTH| 26141|\n|   TM|FTTH,ALLO|FTTH| 23749|\n|    TM|FTTH,CTS|FTTH| 20946|\n|  TM|FTTH,Maxis|FTTH| 16737|\n|            CTS|FTTH|  6982|\n|  TM|VDSL,Maxis|FTTH|  3733|\n|  TM|FTTH,Maxis|VDSL|   575|\n|  TM|VDSL,Maxis|VDSL|   434|\n|          Maxis|VDSL|   428|\n|   TM|VDSL,ALLO|FTTH|    18|\n|TM|FTTH,Maxis|FTT...|     8|\n|TM|VDSL,Maxis|FTT...|     6|\n|TM|VDSL,Maxis|FTT...|     1|\n|TM|VDSL,Maxis|VDS...|     1|\n|    TM|VDSL,CTS|FTTH|     1|\n+--------------------+------+\n\nupdated serviceability checking : None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 9 - Backup 4 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_9_backup_4\n# according to Fakhrul, the order for pipeline 9 is final_9_backup -> final_9_backup_2 -> final_9_backup_3 -> final_9_backup_4 -> final_9_backup_5 -> final_9_backup_6 -> final_9\n# args = getResolvedOptions(sys.argv,\n                        #   ['old_uams_temp_read_path', = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_uams_temp_7.csv\n                        #   'new_fullfeed_with_key_read_path', = s3://astro-groupdata-prod-source/new_fullfeed_uams_with_key/NEW_FULLFEED_UAMS_WITH_KEY.csv\n                        #   'new_address_temp_read_path']) = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_temp_8.csv\n## ================================================================================================================================\n\nprint('unique serviceability of new : ', New_AMS.select('Serviceable').distinct().show())\nprint('unique serviceability of old : ', old_uams.select('Serviceable').distinct().show())\n\n#revision - 22/8/22 - only for this run (should I replicate if it was only for the 22/8/22 run?)\nNew_AMS = New_AMS.withColumnRenamed('Address_ID_new','Address_ID')\n## do a left antijoin (want to remove Address_ID that are in Old_AMS from New_AMS\nNew_address = New_AMS.join(Old_AMS, on='Address_ID', how='leftanti')\nprint('this is new address shape', New_address.select('Address_ID').count()) # 3492527\n\n# ###  Compare New AMS and Old AMS\n# revision - fakhrul - 20/6/22 - otherwise postcode of 08000 will be read as 8000\nNew_address = New_address.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nNew_address = New_address.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\n\nOld_AMS = Old_AMS.withColumn('combined_Key', f.concat_ws(\",\", f.col('Account_No').cast('string'), f.col('Address_ID').cast('string')) )\nNew_AMS = New_AMS.withColumn('combined_Key', f.concat_ws(\",\", f.col('Account_No').cast('string'), f.col('Address_ID').cast('string')) )\n\nNew_address = New_address.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nNew_address = New_address.withColumn(\"Address_ID\", f.regexp_replace(f.col('Address_ID').cast('string'), '\\.0', '') )\n\nNew_address = New_address.select([ 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City',\n       'Postcode','State', 'Address_Type', 'Serviceable', 'P_Flag', 'UR_Flag'])\n# New_address = New_address.withColumnRenamed('Address_ID_old','Address_ID')\nprint('new address serviceability checking :', New_address.groupBy('Serviceable').count().orderBy('count', ascending=False).show())\n\n## save files\nNew_address.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_New_address.orc\".format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_csv(df = New_address, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/New_address_9_backup_4.csv')\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage after the merging isssss (Bytes):')\n# print(usage)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+\n|         Serviceable|\n+--------------------+\n|TM|VDSL,ALLO|FTTH...|\n|            ,TM|FTTH|\n|    CTS|FTTH,TM|FTTH|\n|            TM|VDSL,|\n|maxis|FTTH,ALLO|FTTH|\n|   ALLO|FTTH,TM|FTTH|\n|             TM|VDSL|\n|maxis|FTTH,ALLO|F...|\n|TM|VDSL,maxis|FTT...|\n|          maxis|VDSL|\n|  maxis|VDSL,TM|FTTH|\n|TM|VDSL,maxis|FTT...|\n|TM|LOT 411 JALAN ...|\n|           ALLO|FTTH|\n|   TM|VDSL,ALLO|FTTH|\n|    TM|VDSL,CTS|FTTH|\n|             TM|FTTH|\n|  Maxis|FTTH,TM|FTTH|\n|          Maxis|FTTH|\n|  TM|VDSL,maxis|FTTH|\n+--------------------+\nonly showing top 20 rows\n\nunique serviceability of new :  None\n+-------------+\n|  Serviceable|\n+-------------+\n|\"   \"\"3-15-2\"|\n|\"     \"\"3-04\"|\n|\"     \"\"5-1A\"|\n|\"  \"\"A-04-04\"|\n|\" \"\"A1-02-02\"|\n|       KLUANG|\n|\"   \"\"26-4-5\"|\n|\"  \"\"5-09-30\"|\n|\"  \"\"7540-23\"|\n|\" \"\"60-05-02\"|\n|\"    \"\"2-1-9\"|\n|\"    \"\"1-322\"|\n|\"   \"\"40-2-3\"|\n|\"     \"\"17-7\"|\n|\"   \"\"7-1-27\"|\n|\"   \"\"7545-4\"|\n|\"     \"\"1-39\"|\n|\"     \"\"3-20\"|\n|\"    \"\"17-07\"|\n|\"    \"\"06-07\"|\n+-------------+\nonly showing top 20 rows\n\nunique serviceability of old :  None\nthis is new address shape 155399\n+--------------------+------+\n|         Serviceable| count|\n+--------------------+------+\n|             TM|FTTH|124379|\n|                    | 15453|\n|             TM|VDSL|  8962|\n|            ,TM|FTTH|  1606|\n|           ALLO|FTTH|  1462|\n|   ALLO|FTTH,TM|FTTH|  1372|\n|            CTS|FTTH|   856|\n|  maxis|FTTH,TM|FTTH|   393|\n|          Maxis|FTTH|   380|\n|    CTS|FTTH,TM|FTTH|   305|\n|          maxis|FTTH|    83|\n|  TM|VDSL,maxis|FTTH|    61|\n|     TM|VDSL,TM|FTTH|    51|\n|  Maxis|FTTH,TM|FTTH|    13|\n|          ALLO|FTTH,|     9|\n|            TM|VDSL,|     8|\n|   TM|VDSL,ALLO|FTTH|     3|\n|  maxis|VDSL,TM|FTTH|     1|\n|maxis|FTTH,ALLO|F...|     1|\n|  TM|VDSL,maxis|VDSL|     1|\n+--------------------+------+\n\nnew address serviceability checking : None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 9 - Backup 5 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_9_backup_5\n# according to Fakhrul, the order for pipeline 9 is final_9_backup -> final_9_backup_2 -> final_9_backup_3 -> final_9_backup_4 -> final_9_backup_5 -> final_9_backup_6 -> final_9\n# args = getResolvedOptions(sys.argv,\n                        #   ['old_uams_temp_read_path', = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_uams_temp_7.csv\n                        #   'new_fullfeed_with_key_read_path', = s3://astro-groupdata-prod-source/new_fullfeed_uams_with_key/NEW_FULLFEED_UAMS_WITH_KEY.csv\n                        #   'new_address_temp_read_path']) = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_temp_8.csv\n## ================================================================================================================================\n\n\n## read in old_uams\n# old_uams = spark.read.csv(UAMS_PySpark_save_path+\"uploaded/Old_Fullfeed_UAMS-20220818(1).csv.gz\", header=True) # from Qubole Zep (but it's not the same file as the old_uams read in earlier...)\nold_uams = spark.read.csv(old_fullfeed_path, header=True)\n\nprint(old_uams.columns)\nold_uams = old_uams.select(['Address_ID','Account_No','OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag'])\n# old_uams = wr.s3.read_csv(path = 's3://astro-groupdata-prod-source/old_fullfeed_uams_with_key/Old_FUllfeed_UAMS_18_Aug (1).csv', usecols = ['Address_ID','Account_No','OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag'], dtype = {'Postcode': object})\n#old_uams = wr.s3.read_csv(path = old_uams_temp_read_path, usecols = ['Key','Address_ID','Account_No','OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag','duplicate_key'], dtype = {'Postcode': object})\n\n#revision - fakhrul - 20/6/22 - otherwise postcode of 08000 will be read as 8000\n## clean postcode column (make into string, remove float \\.0, select only first 5 digits, lpad with '0')\nold_uams = old_uams.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nold_uams = old_uams.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\n\n## read in New_AMS\n# New_AMS = spark.read.orc(UAMS_PySpark_save_path+\"NEW_FULLFEED_UAMS_WITH_KEY_{}.orc\".format(date_key))\nNew_AMS = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/NEW_FULLFEED_UAMS_WITH_KEY_{}.orc\".format(date_key, date_key))\nNew_AMS = New_AMS.select('Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name','Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City','Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State','UR_Flag')\n# New_AMS = wr.s3.read_csv(path = new_fullfeed_with_key_read_path, usecols = ['Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name','Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City','Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State','UR_Flag'], dtype = {'Postcode':object})\n\n#revision - fakhrul - 20/6/22 - otherwise postcode of 08000 will be read as 8000\n## clean postcode column (make into string, remove float \\.0, select only first 5 digits, lpad with '0')\nNew_AMS = New_AMS.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nNew_AMS = New_AMS.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\n\n#revision - fakhrul - 27/6/22 removed address_id_x and duplicate_key columns here as prev we removed it. we shall see if it works\n#New_address = wr.s3.read_csv(path = new_address_temp_read_path, usecols = ['index','Key','Account_No','OBJID','House_No','Building_Name','Standard_Building_Name','Street_Type','Street_Name','Area','City','Postcode','Address_Type','Serviceable','P_Flag','State','UR_Flag','Address_ID_y'], dtype = {'Postcode':object})\n#print('shape of new address is: ', New_address.shape)\n\n# ###  Compare New AMS and Old AMS\n#revision - fakhrul - 20/6/22 - otherwise postcode of 08000 will be read as 8000\n#New_address['Postcode']= New_address['Postcode'].astype(str).apply(lambda x:x[0:5])\n#New_address['Postcode'] = New_address['Postcode'].str.pad(width=5, side='left', fillchar='0')  \n\nprint(old_uams.head(5))\n#print(New_address.head())\n\n### ---- Creating combined key\nOld_AMS = old_uams\n\n## first do some cleaning in both Old_AMS & New_AMS\nOld_AMS = Old_AMS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), 'nan', '') )\nOld_AMS = Old_AMS.fillna('', subset='Account_No')\nOld_AMS = Old_AMS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nOld_AMS = Old_AMS.withColumn(\"Address_ID\", f.regexp_replace(f.col('Address_ID').cast('string'), '\\.0', '') )\n\nNew_AMS = New_AMS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), 'nan', '') )\nNew_AMS = New_AMS.fillna('', subset='Account_No')\nNew_AMS = New_AMS.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nNew_AMS = New_AMS.withColumn(\"Address_ID\", f.regexp_replace(f.col('Address_ID').cast('string'), '\\.0', '') )\n\nOld_AMS = Old_AMS.withColumn('combined_Key', f.concat_ws(\",\", f.col('Account_No').cast('string'), f.col('Address_ID').cast('string')) )\nNew_AMS = New_AMS.withColumn('combined_Key', f.concat_ws(\",\", f.col('Account_No').cast('string'), f.col('Address_ID').cast('string')) )\n\n## Revision 18 Aug 2022 --> when inner joining Old and New AMS on the combined Address_ID+Account_No --> if the UR_Flag value is different on the result of merging, it means UR_Flag is updated for these records\n# rename columns for smoother join\nOld_AMS = Old_AMS.withColumnRenamed('UR_Flag', 'UR_Flag_old')\nInner_Old_New_AMS = Old_AMS.select(['combined_Key','UR_Flag_old']).join(New_AMS, on ='combined_Key', how = 'inner')\nprint('Inner_Old_New_AMS (after join):', Inner_Old_New_AMS.select('UR_Flag').count()) # 3444700\n\nUpdated_Location = Inner_Old_New_AMS.filter(f.col('UR_Flag_old')!= f.col('UR_Flag'))\nprint('Updated_Location after filtering UR_Flag_old != UR_Flag:', Updated_Location.select('UR_Flag').count()) # 627367\n\n## Revision 18 Aug 2022\n# create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nUpdated_Location = Updated_Location.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n## de-dupe on combined_Key & keep FIRST based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['combined_Key']).orderBy(f.col(\"index\").asc()) ## ascending because we want to keep 'first'\nUpdated_Location = Updated_Location.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\nprint('Updated_Location after dedupe on combined_Key, keep first count:', Updated_Location.select(\"Serviceable\").count())  # 627367\n\n## Revision 18 Aug 2022\n## select columns (for the UR_Flag, use the ones from New AMS)\nUpdated_Location = Updated_Location.select([ 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City',\n       'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State','UR_Flag'])\n\nprint('Updated_Location checking :', Updated_Location.groupBy('Serviceable').count().orderBy('count', ascending=False).show())\n\n## save files\nUpdated_Location.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_Updated_location.orc\".format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_csv(df = Updated_Location, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/Updated_location_9_5.csv')\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage after the merging isssss (Bytes):')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "['_c0', 'Key', 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag']\n[Row(Address_ID='2556933.0', Account_No='80000006.0', OBJID=None, House_No='        15', Building_Name=None, Standard_Building_Name='NO', Street_Type='LORONG', Street_Name='BAGAN JERMAL', Area=None, City='GEORGE TOWN', Postcode='10250', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='P1', State='PULAU PINANG', UR_Flag='URBAN'), Row(Address_ID='1043943.0', Account_No='80000008.0', OBJID=None, House_No='        46', Building_Name=None, Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='  SS 14/5B', Area='SS 14', City='SUBANG JAYA', Postcode='47500', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='P1', State='SELANGOR', UR_Flag='URBAN'), Row(Address_ID='1232373.0', Account_No='80000011.0', OBJID=None, House_No='        63', Building_Name=None, Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='  PENCHALA', Area='PJS 4', City='PETALING JAYA', Postcode='46050', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='P1', State='SELANGOR', UR_Flag='URBAN'), Row(Address_ID='4842594.0', Account_No='80000013.0', OBJID=None, House_No='       595', Building_Name=None, Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='   ASEAN 3', Area='TAMAN ASEAN', City='MELAKA', Postcode='75250', Address_Type='SDU', Serviceable='TM|FTTH,ALLO|FTTH', P_Flag='P1', State='MELAKA', UR_Flag='URBAN'), Row(Address_ID='1226336.0', Account_No='80000033.0', OBJID=None, House_No='        62', Building_Name=None, Standard_Building_Name='NO', Street_Type='JALAN', Street_Name='  USJ 2/5E', Area='USJ 2', City='SUBANG JAYA', Postcode='47600', Address_Type='SDU', Serviceable='TM|FTTH', P_Flag='P1', State='SELANGOR', UR_Flag='URBAN')]\nInner_Old_New_AMS (after join): 5744982\nUpdated_Location after filtering UR_Flag_old != UR_Flag: 1144125\nUpdated_Location after dedupe on combined_Key, keep first count: 1059205\n+------------------+------+\n|       Serviceable| count|\n+------------------+------+\n|           TM|FTTH|958439|\n|           TM|VDSL| 36263|\n|         ALLO|FTTH| 18001|\n| ALLO|FTTH,TM|FTTH| 14202|\n|          ,TM|FTTH|  9481|\n|          CTS|FTTH|  9302|\n|  CTS|FTTH,TM|FTTH|  5547|\n|        Maxis|FTTH|  5523|\n|        maxis|FTTH|  1587|\n|   TM|VDSL,TM|FTTH|   296|\n|                  |   223|\n|maxis|FTTH,TM|FTTH|   163|\n|        ALLO|FTTH,|    84|\n|Maxis|FTTH,TM|FTTH|    34|\n|          TM|VDSL,|    23|\n|       Maxis|FTTH,|    19|\n|         CTS|FTTH,|    13|\n| TM|VDSL,ALLO|FTTH|     3|\n|TM|VDSL,maxis|FTTH|     2|\n+------------------+------+\n\nUpdated_Location checking : None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 9 - Backup 6 ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_9_backup_6\n# according to Fakhrul, the order for pipeline 9 is final_9_backup -> final_9_backup_2 -> final_9_backup_3 -> final_9_backup_4 -> final_9_backup_5 -> final_9_backup_6 -> final_9\n# args = getResolvedOptions(sys.argv,\n                        #   ['old_uams_temp_read_path', = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/old_uams_temp_7.csv\n                        #   'new_fullfeed_with_key_read_path', = s3://astro-groupdata-prod-source/new_fullfeed_uams_with_key/NEW_FULLFEED_UAMS_WITH_KEY.csv\n                        #   'new_address_temp_read_path']) = s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/new_address_temp_8.csv\n## ================================================================================================================================\n\n## Revision 18 Aug 2022 --> when inner joining Old and New AMS on the combined Address_ID+Account_No --> if the P_Flag value is different on the result of mergging, it means P_Flag is updated for these records\nOld_AMS_notnull = Old_AMS.filter(f.col('P_Flag').isNotNull())\nprint('checking old ams notnull count', Old_AMS.select(\"P_Flag\").count()) # 9435243\n# rename columns for smoother join\nOld_AMS = Old_AMS.withColumnRenamed(\"P_Flag\", \"P_Flag_old\")\nInner_Old_New_AMS = Old_AMS.select('combined_Key','P_Flag_old').join(New_AMS, on ='combined_Key', how = 'inner')\nprint('Inner_Old_New_AMS (after join):', Inner_Old_New_AMS.select('P_Flag').count()) # 3444700\nUpdated_P_Flag = Inner_Old_New_AMS.filter(f.col('P_Flag_old') != f.col('P_Flag'))\nprint('Updated_P_Flag after filtering P_Flag_old != P_Flag:', Updated_P_Flag.select('P_Flag').count()) # 95880\n\n## Revision 18 Aug 2022\n# create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nUpdated_P_Flag = Updated_P_Flag.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n## de-dupe on combined_Key & keep FIRST based on index. To do in Spark: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\nwindow = Window.partitionBy(['combined_Key']).orderBy(f.col(\"index\").asc()) ## ascending because we want to keep 'first'\nUpdated_P_Flag = Updated_P_Flag.withColumn('row', f.row_number().over(window)).filter(col('row') == 1).drop('row')\nprint('Updated_P_Flag after dedupe on combined_Key, keep first count:', Updated_P_Flag.select(\"Serviceable\").count())  # 95880\n\n## Revision 18 Aug 2022\n## select columns (for the P_Flag, use the ones from New AMS)\nUpdated_P_Flag = Updated_P_Flag.select([ 'Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City',\n       'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State','UR_Flag'])\n\nprint('updated p flag count is : ', Updated_P_Flag.select('Serviceable').count()) # 95880\n\n## save files\nUpdated_P_Flag.write.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_Updated_P_Flag.orc\".format(date_key), mode='overwrite', compression='snappy')\n# wr.s3.to_csv(df = Updated_P_Flag, path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/Updated_P_Flag_9_6.csv')\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage after the merging isssss (Bytes):') ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "checking old ams notnull count 9361290\nInner_Old_New_AMS (after join): 5744982\nUpdated_P_Flag after filtering P_Flag_old != P_Flag: 17508\nUpdated_P_Flag after dedupe on combined_Key, keep first count: 17508\nupdated p flag count is :  17508\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Pipeline 9 - FINAL",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## ============================================================ THIS IS THE START OF Pipeline 9 - FINAL ============================================================\n# taken from Glue Job: address_standardization-prod-uams_generation_final_9\n# according to Fakhrul, the order for pipeline 9 is final_9_backup -> final_9_backup_2 -> final_9_backup_3 -> final_9_backup_4 -> final_9_backup_5 -> final_9_backup_6 -> final_9\n# args = getResolvedOptions(sys.argv,\n                        #   'uams_bucket_path', = s3://amsdatabucket/\n                        #   'date', = 22_August_2022\n                        #   'shamani_data_read_path', = s3://amsdatabucket/Sales Team Data/full_dump-2022-07-21.csv\n                        #   'target_bucket_path']) = s3://astro-groupdata-prod-target/address_standardization/\n## ================================================================================================================================\n\n### Read all the files created in Pipeline 9 back in\nUpdated_serviceability = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_Updated_serviceability.orc\".format(date_key))\n# Updated_serviceability = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/Updated_serviceability_9_backup_3.csv')\nprint('updated servc count', Updated_serviceability.select('Serviceable').count()) # updated servc count 299059\n\nUpdated_accounts = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_Updated_accounts.orc\".format(date_key))\n# Updated_accounts = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/Updated_accounts_9_backup.csv')\nprint('updated accs count', Updated_accounts.select('Serviceable').count()) # updated accs count 644557\n\nNew_address = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_New_address.orc\".format(date_key))\n# New_address = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/New_address_9_backup_4.csv')\nprint('new address shape', New_address.select('Serviceable').count()) # new address shape 3492527\n\nRemoved_accounts = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_Removed_accounts.orc\".format(date_key))\n# Removed_accounts = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/Removed_accounts_9_backup_2.csv')\nprint('removed accs shape', Removed_accounts.select('Serviceable').count()) # removed accs shape 1486736\n\nUpdated_P_Flag = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_Updated_P_Flag.orc\".format(date_key))\n# Updated_P_Flag = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/Updated_P_Flag_9_6.csv')\nprint('updated p flag shape', Updated_P_Flag.select('Serviceable').count()) # updated p flag shape 95880\nprint('updated p flag columns', Updated_P_Flag.columns)\nprint('updated p flag unique', Updated_P_Flag.select('P_Flag').distinct().show())\n\nUpdated_Location = spark.read.orc(UAMS_PySpark_save_path+\"phase_6/{}/pipeline9_Updated_location.orc\".format(date_key))\n# Updated_Location = wr.s3.read_csv(path = 's3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/Updated_location_9_5.csv')\nprint('updated location shape', Updated_Location.select('Serviceable').count()) # 627367\n\n## rearrange the columns for all DFs to ensure Union step is clean\nUpdated_P_Flag = Updated_P_Flag.select('Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag')\nUpdated_Location = Updated_Location.select('Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag')\nUpdated_serviceability = Updated_serviceability.select('Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag')\nUpdated_accounts = Updated_accounts.select('Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag')\nNew_address = New_address.select('Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag')\n    \n## concat all the above DFs except removed_accounts\nDelta = Updated_P_Flag.union(Updated_Location).union(Updated_serviceability).union(Updated_accounts).union(New_address)\n# old code: Frame = [Updated_P_Flag, Updated_Location, Updated_serviceability, Updated_accounts, New_address]\n# old code: Delta = pd.concat(Frame)\nDelta = Delta.drop_duplicates()\nprint('this is checking updated serviceability, updated accounts and new address: ', Updated_serviceability.count(), Updated_accounts.count(), New_address.count()) # 299059 644557 3492527\n\nDelta = Delta.fillna(\"\")\nRemoved_accounts = Removed_accounts.fillna(\"\")\n\nDelta = Delta.select(['Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City',\n'Postcode', 'State','Address_Type', 'Serviceable', 'P_Flag', 'UR_Flag'])\nRemoved_accounts = Removed_accounts.select(['Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City',\n'Postcode', 'State','Address_Type', 'Serviceable', 'P_Flag', 'UR_Flag'])\n\nprint(Delta.select(f.length(f.col('Account_No').cast('string'))).distinct().show())\nprint(Delta.select(f.length(f.col('Postcode').cast('string'))).distinct().show())\n\nDelta = Delta.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nDelta = Delta.withColumn(\"OBJID\", f.regexp_replace(f.col('OBJID').cast('string'), '\\.0', '') )\nDelta = Delta.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nDelta = Delta.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\n\nRemoved_accounts = Removed_accounts.withColumn(\"Account_No\", f.regexp_replace(f.col('Account_No').cast('string'), '\\.0', '') )\nRemoved_accounts = Removed_accounts.withColumn(\"OBJID\", f.regexp_replace(f.col('OBJID').cast('string'), '\\.0', '') )\nRemoved_accounts = Removed_accounts.withColumn(\"Postcode\", f.regexp_replace(f.col('Postcode').cast('string'), '\\.0', '') )\nRemoved_accounts = Removed_accounts.withColumn(\"Postcode\", f.lpad(f.substring(f.col('Postcode'), 1, 5), 5, '0') )\n\nDelta = Delta.withColumn('Address_Type', when(f.col('Building_Name')=='', 'SDU').otherwise('MDU') )\n# create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nDelta = Delta.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## extra cleaning after finding some weird values in Serviceable column\nDelta = Delta.withColumn('Serviceable', f.upper(f.col(\"Serviceable\").cast('string')) )\nDelta = Delta.withColumn( 'Serviceable', when((f.col('Serviceable').contains('|FTTH') | f.col('Serviceable').contains('|VDSL')), f.col('Serviceable')).otherwise(f.lit('TM|FTTH')) )  ## fix cases where there are no FTTH or VDSL in the name - they are all TM & I checked they are FTTH. In the future, TM is only laying FTTH, so this line of code can continue to be used\n\n## add astrofibre FTTH into Serviceable column whenever it's TM|FTTH \nDelta = Delta.withColumn('Serviceable', when( f.col(\"Serviceable\").contains('TM|FTTH'), f.concat(f.lit('AstroFibre|FTTH,'), f.col('Serviceable')) ).otherwise(f.col('Serviceable')) )\n# Delta = Delta.withColumn('Serviceable', when( f.col(\"Serviceable\").contains('TM|FTTH'), f.lit('AstroFibre|FTTH,'+ f.col('Serviceable')) ).otherwise(f.col('Serviceable')) )\n# ORI_CODE --> add_astrofiber_delta= Delta[Delta['Serviceable'].str.contains('[Tt][Mm]\\|FTTH')].index  # add_astrofiber_delta = list(add_astrofiber_delta)  # Delta.loc[add_astrofiber_delta,'Serviceable'] = 'AstroFibre|FTTH,' + Delta['Serviceable'].astype(str).str.upper()\n\n## ## extra cleaning after finding some weird values in Serviceable column\nDelta = Delta.withColumn('Serviceable', f.regexp_replace(f.col('Serviceable'), ',,', ',')) ## fix cases with double commas\nDelta = Delta.withColumn( 'Serviceable', when( f.col('Serviceable').endswith(','), f.expr(\"substring(Serviceable, 1, length(Serviceable) - 1)\") ).otherwise(f.col('Serviceable')) )  ## fix cases where the value ends in a comma ## a bit slow coz it uses f.expr\n\nprint('checking delta numbers here : ', Delta.select('Account_No').count())\nprint('checking account no of delta :', Delta.select('Account_No').head(10))\n# print('checking number of null account number in delta: ', Delta.filter(f.col('Account_No').isNull()).count()) # 0\nprint('checking number of null account number in delta: ', Delta.select('Account_No').filter(f.col('Account_No')=='').count()) # 3056868\n\n# create a sequential index as Zohreh did a pandas reset_index at this step. To do it in Spark: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\nRemoved_accounts = Removed_accounts.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())) )\n\n## extra cleaning after finding some weird values in Serviceable column\nRemoved_accounts = Removed_accounts.withColumn('Serviceable', f.upper(f.col(\"Serviceable\").cast('string')) )\nRemoved_accounts = Removed_accounts.withColumn( 'Serviceable', when((f.col('Serviceable').contains('|FTTH') | f.col('Serviceable').contains('|VDSL')), f.col('Serviceable')).otherwise(f.lit('TM|FTTH')) )  ## fix cases where there are no FTTH or VDSL in the name - they are all TM & I checked they are FTTH. In the future, TM is only laying FTTH, so this line of code can continue to be used\n\n## add astrofibre FTTH into Serviceable column whenever it's TM|FTTH \nRemoved_accounts = Removed_accounts.withColumn('Serviceable', when( f.col(\"Serviceable\").contains('TM|FTTH'), f.concat(f.lit('AstroFibre|FTTH,'), f.col('Serviceable')) ).otherwise(f.col('Serviceable')) )\n# Removed_accounts = Removed_accounts.withColumn('Serviceable', when( f.col(\"Serviceable\").contains('TM|FTTH'), f.lit('AstroFibre|FTTH'+ f.col('Serviceable')) ).otherwise(f.col('Serviceable')) )\n# ORI_CODE --> add_astrofiber_remove= Removed_accounts[Removed_accounts['Serviceable'].str.contains('TM\\|FTTH')].index\n# add_astrofiber_remove = list(add_astrofiber_remove)\n# Removed_accounts.loc[add_astrofiber_remove,'Serviceable'] = 'AstroFibre|FTTH,' + Removed_accounts['Serviceable'].astype(str)\n\n## extra cleaning after finding some weird values in Serviceable column\nRemoved_accounts = Removed_accounts.withColumn('Serviceable', f.regexp_replace(f.col('Serviceable'), ',,', ',')) ## fix cases with double commas\nRemoved_accounts = Removed_accounts.withColumn( 'Serviceable', when( f.col('Serviceable').endswith(','), f.expr(\"substring(Serviceable, 1, length(Serviceable) - 1)\") ).otherwise(f.col('Serviceable')) )  ## fix cases where the value ends in a comma ## a bit slow coz it uses f.expr\n\nprint('checking removed_accounts count here: ', Removed_accounts.select('Account_No').count()) # 1486736\n\n#Delta.to_csv('DELTA_ADD_UPDATE_UAMS_16Mar2022.csv') ## Save in UAMS bucket\n#Removed_accounts.to_csv('DELTA_DELETE_UAMS_16Mar2022.csv')\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Bytes):')\n# print(usage)\n\nprint('this is delta serviceable unique values : ', Delta.groupBy('Serviceable').count().orderBy('count', ascending=False).show(30, False))\nprint('this is Removed_accounts serviceable unique values : ', Removed_accounts.groupBy('Serviceable').count().orderBy('count', ascending=False).show(30, False))\n\n#Delta.to_csv('DELTA_ADD_UPDATE_UAMS_16Mar2022.csv') ## Save in UAMS bucket\n#wr.s3.to_csv(df = Delta, path = uams_bucket_path + 'DELTA_ADD_UPDATE_UAMS_' + str(date) + '.csv')\n\n#Removed_accounts.to_csv('DELTA_DELETE_UAMS_16Mar2022.csv')\n#wr.s3.to_csv(df = Removed_accounts, path = uams_bucket_path + 'DELTA_DELETE_UAMS_' + str(date) + '.csv') ",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "updated servc count 564173\nupdated accs count 2586838\nnew address shape 155399\nremoved accs shape 1355140\nupdated p flag shape 17508\nupdated p flag columns ['Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'Address_Type', 'Serviceable', 'P_Flag', 'State', 'UR_Flag']\n+--------+\n|  P_Flag|\n+--------+\n|CTS_file|\n|        |\n|      P1|\n|      P2|\n+--------+\n\nupdated p flag unique None\nupdated location shape 1059205\nthis is checking updated serviceability, updated accounts and new address:  564173 2586838 155399\n+----------------------------------+\n|length(CAST(Account_No AS STRING))|\n+----------------------------------+\n|                                 0|\n|                                 8|\n|                                 5|\n+----------------------------------+\n\nNone\n+--------------------------------+\n|length(CAST(Postcode AS STRING))|\n+--------------------------------+\n|                               5|\n+--------------------------------+\n\nNone\nchecking delta numbers here :  4272134\nchecking account no of delta : [Row(Account_No=''), Row(Account_No=''), Row(Account_No='96113671'), Row(Account_No='81761524'), Row(Account_No=''), Row(Account_No='97314110'), Row(Account_No=''), Row(Account_No=''), Row(Account_No=''), Row(Account_No='')]\nchecking number of null account number in delta:  3064143\nchecking removed_accounts count here:  1355140\n+--------------------------------------------+-------+\n|Serviceable                                 |count  |\n+--------------------------------------------+-------+\n|AstroFibre|FTTH,TM|FTTH                     |3707132|\n|TM|VDSL                                     |294827 |\n|ALLO|FTTH                                   |51909  |\n|AstroFibre|FTTH,ALLO|FTTH,TM|FTTH           |50991  |\n|MAXIS|FTTH                                  |40510  |\n|AstroFibre|FTTH,TM|FTTH,ALLO|FTTH           |23749  |\n|CTS|FTTH                                    |21128  |\n|AstroFibre|FTTH,TM|FTTH,CTS|FTTH            |20946  |\n|AstroFibre|FTTH,CTS|FTTH,TM|FTTH            |20928  |\n|AstroFibre|FTTH,TM|FTTH,MAXIS|FTTH          |16737  |\n|AstroFibre|FTTH,MAXIS|FTTH,TM|FTTH          |14766  |\n|TM|VDSL,MAXIS|FTTH                          |5942   |\n|AstroFibre|FTTH,TM|FTTH,MAXIS|VDSL          |575    |\n|TM|VDSL,MAXIS|VDSL                          |569    |\n|MAXIS|VDSL                                  |489    |\n|AstroFibre|FTTH,TM|VDSL,TM|FTTH             |479    |\n|AstroFibre|FTTH,MAXIS|VDSL,TM|FTTH          |219    |\n|TM|VDSL,ALLO|FTTH                           |211    |\n|TM|VDSL,MAXIS|FTTH,ALLO|FTTH                |9      |\n|AstroFibre|FTTH,TM|FTTH,MAXIS|FTTH,ALLO|FTTH|8      |\n|AstroFibre|FTTH,MAXIS|FTTH,ALLO|FTTH,TM|FTTH|4      |\n|TM|VDSL,CTS|FTTH                            |3      |\n|TM|VDSL,MAXIS|FTTH,CTS|FTTH                 |1      |\n|MAXIS|FTTH,ALLO|FTTH                        |1      |\n|TM|VDSL,MAXIS|VDSL,ALLO|FTTH                |1      |\n+--------------------------------------------+-------+\n\nthis is delta serviceable unique values :  None\n+--------------------------------------------+-------+\n|Serviceable                                 |count  |\n+--------------------------------------------+-------+\n|AstroFibre|FTTH,TM|FTTH                     |1272663|\n|TM|VDSL                                     |24440  |\n|AstroFibre|FTTH,TM|FTTH,ALLO|FTTH           |24233  |\n|AstroFibre|FTTH,TM|FTTH,MAXIS|FTTH          |10887  |\n|AstroFibre|FTTH,TM|FTTH,CTS|FTTH            |10109  |\n|MAXIS|FTTH                                  |6491   |\n|CTS|FTTH                                    |3448   |\n|TM|VDSL,MAXIS|FTTH                          |1674   |\n|ALLO|FTTH                                   |1001   |\n|AstroFibre|FTTH,TM|FTTH,MAXIS|VDSL          |70     |\n|TM|VDSL,ALLO|FTTH                           |56     |\n|TM|VDSL,MAXIS|VDSL                          |39     |\n|AstroFibre|FTTH,TM|FTTH,MAXIS|FTTH,ALLO|FTTH|15     |\n|MAXIS|VDSL                                  |9      |\n|TM|VDSL,MAXIS|FTTH,ALLO|FTTH                |3      |\n|TM|VDSL,CTS|FTTH                            |2      |\n+--------------------------------------------+-------+\n\nthis is Removed_accounts serviceable unique values :  None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## Revision 17MAy2022\nprint('checking removed accounts info here----------------',Removed_accounts.select('Account_No').count(), Removed_accounts.columns) # 1486736\nprint('testing reading shamani data here')\n#Shamani_data = pd.read_csv('prodDump.csv')\n# Shamani = wr.s3.read_csv('s3://astro-datalake-prod-sandbox/amzar/BB/other_adhoc/uploaded/address_checking/Shamani_data/dump_20220915.csv', dtype=str) # Qubole read path\n# Shamani = wr.s3.read_csv(path = shamani_data_read_path)\n\n# # read in shamani data # added on 24/11/2022\n\n# shamani_data = glueContext.create_dynamic_frame_from_options(\n\n#     connection_type = 's3',\n#     connection_options = {'paths' : [shamani_data_read_path]},\n#     format = 'csv',\n#     format_options = {'withHeader':True}\n\n# ) # read in dynamic \n\n# Shamani = shamani_data.toDF() # convert DynamicFrame to Spark DF first\nShamani = spark.read.csv(shamani_data_read_path, header=True) # read in the data as pyspark DF\nShamani = Shamani.toPandas() # convert Spark DF to Pandas DF to allow for pandas code\n\n#revision - zohreh - 2/8/22 - splitting objid-----------------------------------------------------------------------------------------------------------------------------------\n\ns1 = Shamani.acc_nos.str.split(',', expand=True).stack().str.strip()\ns2 = Shamani.obj_ids.str.split(',', expand=True).stack().str.strip()\nShamani_V1 = pd.concat([s1,s2], axis=1, keys=['acc_nos','obj_ids'])\nprint('Shamani_V1 shape:', Shamani_V1.shape)\nShamani_V1 = Shamani_V1.reset_index()\nShamani_V1 = Shamani_V1[['acc_nos','obj_ids']]\n\nShamani_V2= Shamani.set_index(Shamani.columns.drop('acc_nos',1).tolist()).acc_nos.str.split(',', expand=True).stack().reset_index().rename(columns={0:'acc_nos'}).loc[:, Shamani.columns]\nShamani_V2 = Shamani_V2.drop(['obj_ids','acc_nos'], axis=1)\n\nS1 = Shamani_V2['Address_ID']\n\nShamani_V3 = pd.concat([S1,Shamani_V1], axis=1)\n\nShamani_Final = pd.merge(Shamani_V3, Shamani_V2, on=\"Address_ID\", how=\"left\")\nprint('Shamani_Final shape:', Shamani_Final.shape)\n\nShamani_Final = Shamani_Final[Shamani_Final['Address_ID'].notnull()]\n\nShamani_Final['Address_ID'] = Shamani_Final['Address_ID'].astype(int)\n\nShamani_Final = Shamani_Final.drop_duplicates()\n\nShamani_data = Shamani_Final\n#------------------------------------------------------------------------------------------\n\nShamani_data['acc_nos'] = Shamani_data['acc_nos'].astype(str)\nShamani_data['acc_nos'] = Shamani_data['acc_nos'].str.replace('\\.0','', case = False)\n\nShamani_data['acc_nos'] = Shamani_data['acc_nos'].apply(lambda x : \"\" if len(x)!= 8 else x)\n\n\nShamani_data['Standard_Building_Name'] = 'NO'\nShamani_data.loc[Shamani_data[\"Building_Name\"].notnull(),\"Standard_Building_Name\"] = 'YES'\n\n\nShamani_data = Shamani_data.fillna(\"\")\n\nShamani_data = Shamani_data.rename(columns={'acc_nos':'Account_No','state':'State'\n                                            ,'obj_ids':'OBJID'})\nShamani_data['OBJID'] = Shamani_data['OBJID'].astype(str)\nShamani_data['OBJID']  = Shamani_data['OBJID'].str.replace('\\.0','', case = False)\n\nShamani_data['Postcode'] = Shamani_data['Postcode'].map(str).apply(lambda x: x.zfill(5))\n\nShamani_data['Postcode'] = Shamani_data['Postcode'].str.pad(width=5)\n\nShamani_data = Shamani_data[['Address_ID', 'Account_No', 'OBJID',\n                             'House_No', 'Building_Name',\n                             'Standard_Building_Name', 'Street_Type',\n                             'Street_Name', 'Area', 'City','Postcode',\n                             'State','Address_Type', 'Serviceable',\n                             'P_Flag','UR_Flag']]\n\n\nprint('done reading shamani data here')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "checking removed accounts info here---------------- 1355140 ['Address_ID', 'Account_No', 'OBJID', 'House_No', 'Building_Name', 'Standard_Building_Name', 'Street_Type', 'Street_Name', 'Area', 'City', 'Postcode', 'State', 'Address_Type', 'Serviceable', 'P_Flag', 'UR_Flag', 'index']\ntesting reading shamani data here\nShamani_V1 shape: (2264, 2)\nShamani_Final shape: (2280, 16)\ndone reading shamani data here\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Phase 7\n- at least I think the below cell is Phase 7... (actually maybe not after looking at the original local notebook from Zohreh...)\n- At the end of the notebook we need to replace the new UAMS into the old UAMS for the next cycle. It means what we newly generated will be the old UAMS in the next cycle.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## 15/11/2022: added line to convert Shamani's data to a PySpark DF\nShamani_data_spark = spark.createDataFrame(Shamani_data)\n\n## Revision 17MAy2022\nRemoved_accounts = Removed_accounts.drop(*['index']).union(Shamani_data_spark) ## remove the Old_Inactive_GAPI for next cycle\n\nprint('this is to see unique serviceable values :', Removed_accounts.groupBy('Serviceable').count().orderBy('count', ascending=False).show())\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage is (Bytes):')\n# print(usage)\n\n#revision - fakhrul - 4/8/22 - fillna because priyanka complained abt na\nRemoved_accounts = Removed_accounts.fillna('')\n#Delta['Account_No'] = Delta['Account_No'].str.replace('\\.0','', case = False)\nDelta = Delta.fillna('')\nDelta = Delta.withColumn('Account_No', when(f.col('Account_No') == 'nan', '').otherwise(f.col('Account_No')) )\nDelta = Delta.withColumn('Address_Type', when(f.trim(f.col('Building_Name')) == '', 'SDU').otherwise(f.col('Building_Name')) )\nDelta = Delta.withColumn('Building_Name', f.trim(f.upper(f.col('Building_Name'))) )\n\n#Delta['Account_No'] = Delta['Account_No'].replace('NAN', '')\n#Delta['Account_No'] = Delta['Account_No'].replace('NaN', '')\n# Removed_accounts = Removed_accounts.replace(np.nan, '', regex=True)\n\n#Delta['Account_No'] = pd.to_numeric(Delta['Account_No'])\n#Delta = Delta.fillna('')\n\n## Saving files\n\n#Delta.to_csv('DELTA_ADD_UPDATE_UAMS_16Mar2022.csv') ## Save in UAMS bucket\n# wr.s3.to_csv(df = Delta, path = uams_bucket_path + 'DELTA_ADD_UPDATE_UAMS_' + str(date) + '_URBAN_RURAL.csv', s3_additional_kwargs = {\"ACL\":\"bucket-owner-full-control\"})\nDelta.write.orc(UAMS_PySpark_save_path+\"phase_7/{}/DELTA_ADD_UPDATE_UAMS_URBAN_RURAL.orc\".format(date_key), mode='overwrite', compression='snappy')\nDelta.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_7/{}/DELTA_ADD_UPDATE_UAMS_URBAN_RURAL.csv\".format(date_key), mode='overwrite', header=True)\n\n#Revision - fakhrul 15/6/22 - send delta add update to target for hazim to check against edw data\n# wr.s3.to_csv(df = Delta, path = target_bucket_path + 'uams_delta_add_update_full/' + 'DELTA_ADD_UPDATE_UAMS_' + timestr + '.csv')\nDelta.write.orc(UAMS_PySpark_save_path+\"phase_7/{}/DELTA_ADD_UPDATE_UAMS_{}.orc\".format(date_key, date_key), mode='overwrite', compression='snappy')\nDelta.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_7/{}/DELTA_ADD_UPDATE_UAMS_{}.csv\".format(date_key, date_key), mode='overwrite', header=True)\n\n#Removed_accounts.to_csv('DELTA_DELETE_UAMS_16Mar2022.csv')\n# wr.s3.to_csv(df = Removed_accounts, path = uams_bucket_path + 'DELTA_DELETE_UAMS_' + str(date) + '_URBAN_RURAL.csv', s3_additional_kwargs = {\"ACL\":\"bucket-owner-full-control\"})\nRemoved_accounts.write.orc(UAMS_PySpark_save_path+\"phase_7/{}/DELTA_DELETE_UAMS_URBAN_RURAL.orc\".format(date_key), mode='overwrite', compression='snappy')\nRemoved_accounts.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_7/{}/DELTA_DELETE_UAMS_URBAN_RURAL.csv\".format(date_key), mode='overwrite', header=True)\n\n#Revision - 27/5/22 - fakhrul Send to target bucket as requirement for izham\n# Acc_no_df = Removed_accounts[['Account_No']]\n# wr.s3.to_csv(df = Acc_no_df, path = target_bucket_path + 'uams_delta_delete/' + 'DELTA_DELETE_UAMS_' + timestr + '.csv')\nAcc_no_df = Removed_accounts.select('Account_No')\nAcc_no_df.write.orc(UAMS_PySpark_save_path+\"phase_7/{}/DELTA_DELETE_UAMS_acc.orc\".format(date_key), mode='overwrite', compression='snappy')\nAcc_no_df.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_7/{}/DELTA_DELETE_UAMS_acc.csv\".format(date_key), mode='overwrite', header=True)\n\n#Revision - 9/6/22 fakhrul - to save full for reading to update c360 pqp2 tm\n# wr.s3.to_csv(df = Removed_accounts, path = 's3://astro-groupdata-prod-target/address_standardization/uams_delta_delete_full/' + 'DELTA_DELETE_UAMS_' + timestr + '.csv')\nRemoved_accounts.write.orc(UAMS_PySpark_save_path+\"phase_7/{}/DELTA_DELETE_UAMS.orc\".format(date_key), mode='overwrite', compression='snappy')\nRemoved_accounts.coalesce(1).write.csv(UAMS_PySpark_save_path+\"phase_7/{}/DELTA_DELETE_UAMS.csv\".format(date_key), mode='overwrite', header=True)\n\n#revision - 27/6/22 - fakhrul - testing to see final output\n# wr.s3.to_csv(df = Delta, path = 's3://astro-groupdata-prod-target/address_standardization/delta_add_update_uams_test.csv')\n\n# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n# print('[debug] memory usage after the merging isssss (Bytes):')\n# print(usage)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+-------+\n|         Serviceable|  count|\n+--------------------+-------+\n|AstroFibre|FTTH,T...|1272898|\n|             TM|VDSL|  24449|\n|AstroFibre|FTTH,T...|  24233|\n|AstroFibre|FTTH,T...|  10887|\n|AstroFibre|FTTH,T...|  10109|\n|          MAXIS|FTTH|   6491|\n|            CTS|FTTH|   3452|\n|     AstroFibre|FTTH|   1835|\n|  TM|VDSL,MAXIS|FTTH|   1674|\n|           ALLO|FTTH|   1016|\n|                    |    155|\n|AstroFibre|FTTH,T...|     70|\n|   TM|VDSL,ALLO|FTTH|     56|\n|  TM|VDSL,MAXIS|VDSL|     39|\n|AstroFibre|FTTH,T...|     15|\n|          MAXIS|VDSL|      9|\n|          Maxis|FTTH|      7|\n|TM|VDSL,MAXIS|FTT...|      3|\n|AstroFibre|FTTH,M...|      2|\n|    TM|VDSL,CTS|FTTH|      2|\n+--------------------+-------+\nonly showing top 20 rows\n\nthis is to see unique serviceable values : None\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}