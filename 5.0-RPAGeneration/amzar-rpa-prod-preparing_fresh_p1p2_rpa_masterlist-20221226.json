{
  "jobConfig": {
    "name": "amzar-rpa-prod-preparing_fresh_p1p2_rpa_masterlist",
    "description": "",
    "role": "arn:aws:iam::741993363917:role/AWSGlueServiceRole",
    "command": "pythonshell",
    "version": "1.0",
    "workerType": null,
    "numberOfWorkers": null,
    "maxCapacity": 1,
    "maxRetries": 0,
    "timeout": 2880,
    "maxConcurrentRuns": 1,
    "security": "none",
    "scriptName": "amzar-rpa-prod-preparing_fresh_p1p2_rpa_masterlist.py",
    "scriptLocation": "s3://aws-glue-assets-741993363917-ap-southeast-1/scripts/",
    "language": "python-3",
    "jobParameters": [
      {
        "key": "--class",
        "value": "GlueApp",
        "existing": false
      },
      {
        "key": "--prebuilt-library-option",
        "value": "prebuilt-library-enable",
        "existing": false
      }
    ],
    "tags": [
      {
        "key": "Name",
        "value": "rpa-prod-preparing_fresh_p1p2_rpa_masterlist",
        "existing": false
      },
      {
        "key": "Project",
        "value": "RPA",
        "existing": false
      },
      {
        "key": "Technical_Owner",
        "value": "Amzar",
        "existing": false
      }
    ],
    "jobMode": "DEVELOPER_MODE",
    "useGlueProvidedDataLakeLibs": false,
    "developerMode": true,
    "connectionsList": [],
    "temporaryDirectory": "s3://aws-glue-assets-741993363917-ap-southeast-1/temporary/",
    "glueHiveMetastore": true,
    "etlAutoTuning": false,
    "pythonPath": "s3://astro-groupdata-prod-config/addressable_ads/python_library/fsspec-2022.1.0-py3-none-any.whl,s3://astro-groupdata-prod-config/addressable_ads/python_library/awswrangler-2.13.0-py3-none-any.whl,s3://astro-groupdata-prod-config/addressable_ads/python_library/s3fs-2022.1.0-py3-none-any.whl,s3://astro-groupdata-prod-config/addressable_ads/python_library/pandas_glue_module-0.1-py3-none-any.whl, s3://astro-groupdata-prod-config/RPA/python library/openpyxl-3.0.9-py2.py3-none-any.whl, s3://astro-groupdata-prod-config/RPA/python library/XlsxWriter-3.0.3-py3-none-any.whl",
    "flexExecution": false,
    "minFlexWorkers": null
  },
  "hasBeenSaved": false,
  "script": "## Amzar 14/10/2022 --> Duplicated the job \"rpa-prod-preparing_fresh_p1p2_rpa_masterlist\" to create this job\r\n## AFTER RUNNING THIS NOTEBOOK, remember to upload the CCMPP1_date_New file to tmautobotprod\r\n\r\nimport awswrangler as wr\r\nimport sys\r\n# from awsglue.utils import getResolvedOptions\r\nimport pandas as pd\r\nfrom io import StringIO\r\nimport glob\r\nimport re\r\nimport numpy as np\r\nimport boto3\r\nimport json\r\nfrom botocore.exceptions import ClientError\r\n\r\n# curr_date = str(pd.datetime.today().strftime('%Y%m%d')) # Amzar 21/11/22: added this here\r\ncurr_date = '20221121' # temporary assignment (not automated)\r\n\r\ndef read_new_p1_file(new_p1_path):\r\n    # new_p1 = wr.s3.read_csv(new_p1_path, usecols = ['ACCOUNT_ID','CRM_OBJID','DTYPE','ASTRO_HOUSE_NO','ASTRO_CONDO_NAME','ASTRO_BLOCK','ASTRO_AREA','TM_CONDO','TM_STREET_TYPE','TM_STREET_NAME','TM_AREA','TM_CITY','TM_STATE','TM_POSTCODE','ISP_INDICATOR'])\r\n    new_p1 = wr.s3.read_csv(new_p1_path, dtype=str, compression='gzip', usecols = ['ACCOUNT_ID','CRM_OBJID','DTYPE','ASTRO_HOUSE_NO','ASTRO_CONDO_NAME','ASTRO_BLOCK','ASTRO_AREA','TM_CONDO','TM_STREET_TYPE','TM_STREET_NAME','TM_AREA','TM_CITY','TM_STATE','TM_POSTCODE','ISP_INDICATOR'])\r\n     ## Amzar 19/10/2022 --> added new line for reading in gzip \r\n     \r\n    new_p1['ACCOUNT_ID'] = new_p1['ACCOUNT_ID'].astype(str)\r\n    new_p1['ACCOUNT_ID'] = new_p1['ACCOUNT_ID'].str.replace('\\.0','', case = False)\r\n    \r\n    print('New P1:',new_p1.shape)  ## Amzar 14/10/2022 --> added more to the print statement\r\n    \r\n    new_p1_clean = new_p1[(new_p1['TM_STATE'].notnull())&(new_p1['TM_POSTCODE'].notnull())]\r\n    print('non-null TM state or postcode P1:', new_p1.shape,new_p1_clean.shape) ## Amzar 14/10/2022 --> added more to the print statement\r\n\r\n    return new_p1_clean\r\n    \r\ndef read_new_p2_file(new_p2_path):\r\n    # new_p2 = wr.s3.read_csv(new_p2_path, usecols = ['ACCOUNT_ID','CRM_OBJID','DTYPE','ASTRO_HOUSE_NO','ASTRO_CONDO_NAME','ASTRO_BLOCK','ASTRO_AREA','TM_CONDO','TM_STREET_TYPE','TM_STREET_NAME','TM_AREA','TM_CITY','TM_STATE','TM_POSTCODE','ISP_INDICATOR'])\r\n    new_p2 = wr.s3.read_csv(new_p2_path, dtype=str, compression='gzip', usecols = ['ACCOUNT_ID','CRM_OBJID','DTYPE','ASTRO_HOUSE_NO','ASTRO_CONDO_NAME','ASTRO_BLOCK','ASTRO_AREA','TM_CONDO','TM_STREET_TYPE','TM_STREET_NAME','TM_AREA','TM_CITY','TM_STATE','TM_POSTCODE','ISP_INDICATOR'])\r\n     ## Amzar 19/10/2022 --> added new line for reading in gzip \r\n\r\n    new_p2['ACCOUNT_ID'] = new_p2['ACCOUNT_ID'].astype(str)\r\n    new_p2['ACCOUNT_ID'] = new_p2['ACCOUNT_ID'].str.replace('\\.0','', case = False)\r\n    \r\n    print('New P2:',new_p2.shape)  ## Amzar 14/10/2022 --> added more to the print statement\r\n    \r\n    new_p2_clean = new_p2[(new_p2['TM_STATE'].notnull())&(new_p2['TM_POSTCODE'].notnull())]\r\n    print('non-null TM state or postcode P2:',new_p2.shape,new_p2_clean.shape)  ## Amzar 14/10/2022 --> added more to the print statement\r\n    \r\n    return new_p2_clean\r\n    \r\ndef read_old_p1_file(old_p1_path):\r\n    old_p1 = wr.s3.read_csv(old_p1_path, usecols = ['ACCOUNT_ID','CRM_OBJID','DTYPE','ASTRO_HOUSE_NO','ASTRO_CONDO_NAME','ASTRO_BLOCK','ASTRO_AREA','TM_STREET_TYPE','TM_STREET_NAME','TM_AREA','TM_CITY','TM_STATE','TM_POSTCODE','ISP_INDICATOR', 'acc_status', 'cust_sub_type'])\r\n    \r\n    old_p1['ACCOUNT_ID'] = old_p1['ACCOUNT_ID'].astype(str)\r\n    old_p1['ACCOUNT_ID'] = old_p1['ACCOUNT_ID'].str.replace('\\.0','', case = False)\r\n    \r\n    print('P1 old:',old_p1.shape)\r\n    \r\n    old_p1_clean = old_p1[(old_p1['TM_STATE'].notnull())&(old_p1['TM_POSTCODE'].notnull())]\r\n    print(old_p1.shape,old_p1_clean.shape)\r\n    \r\n    return old_p1_clean\r\n    \r\ndef read_old_p2_file(old_p2_path):\r\n    old_p2 = wr.s3.read_csv(old_p2_path, usecols = ['ACCOUNT_ID','CRM_OBJID','DTYPE','ASTRO_HOUSE_NO','ASTRO_CONDO_NAME','ASTRO_BLOCK','ASTRO_AREA','TM_STREET_TYPE','TM_STREET_NAME','TM_AREA','TM_CITY','TM_STATE','TM_POSTCODE','ISP_INDICATOR', 'acc_status', 'cust_sub_type'])\r\n    \r\n    old_p2['ACCOUNT_ID'] = old_p2['ACCOUNT_ID'].astype(str)\r\n    old_p2['ACCOUNT_ID'] = old_p2['ACCOUNT_ID'].str.replace('\\.0','', case = False)\r\n    \r\n    print('P2 old:',old_p2.shape)\r\n    \r\n    old_p2_clean = old_p2[(old_p2['TM_STATE'].notnull())&(old_p2['TM_POSTCODE'].notnull())]\r\n    print(old_p2.shape,old_p2_clean.shape)\r\n    \r\n    return old_p2_clean\r\n    \r\ndef generate_fresh_p1_file(old_p1_clean, new_p1_clean):\r\n\r\n    old_p1_clean_list = list(old_p1_clean['ACCOUNT_ID'])\r\n    p1_clean_isin_old = new_p1_clean[new_p1_clean[\"ACCOUNT_ID\"].isin(old_p1_clean_list)]\r\n\r\n    ## Getting the fresh p1_clean\r\n    p1_clean_notin_old = new_p1_clean[~new_p1_clean[\"ACCOUNT_ID\"].isin(old_p1_clean_list)]\r\n    print('New P1 that exists in old P1:', p1_clean_isin_old.shape, 'Fresh P1:', p1_clean_notin_old.shape)  ## Amzar 14/10/2022 --> added more to the print statement\r\n    \r\n    print('Fresh P1 unique acc:', p1_clean_notin_old.ACCOUNT_ID.nunique()) ## Amzar 14/10/2022 --> added more to the print statement\r\n    p1_clean_notin_old.head()\r\n    #print(type(p1_clean_notin_old))\r\n    \r\n    return p1_clean_notin_old\r\n    \r\n    ##write to S3 - the delta file - difference between existing master and the new p1 addr std base \r\n    ## another copy to tmautombot for the delta file : filter by active accounts and non njoi - “CCMPP1_{curr_date}New.xlsx” 2.2.3.2\r\n    #testing\r\n    #s3://astro-groupdata-prod-target/rpa/\r\n    #wr.s3.to_excel(p1_clean_notin_old,'s3://astro-groupdata-prod-target/rpa/Fresh_P1_TM_Format.xlsx')\r\n    \r\ndef generate_fresh_p2_file(old_p2_clean, new_p2_clean):  \r\n    \r\n    old_p2_clean_list = list(old_p2_clean['ACCOUNT_ID'])\r\n    p2_clean_isin_old = new_p2_clean[new_p2_clean[\"ACCOUNT_ID\"].isin(old_p2_clean_list)]\r\n    \r\n    ## New or updated addresses\r\n    p2_clean_notin_old = new_p2_clean[~new_p2_clean[\"ACCOUNT_ID\"].isin(old_p2_clean_list)]\r\n    print('New P2 that exists in old P2:',p2_clean_isin_old.shape, 'Fresh P2:', p2_clean_notin_old.shape) ## Amzar 14/10/2022 --> added more to the print statement\r\n    \r\n    print('Fresh P2 unique acc:',p2_clean_notin_old.ACCOUNT_ID.nunique()) ## Amzar 14/10/2022 --> added more to the print statement\r\n    p2_clean_notin_old.head()\r\n    \r\n    return p2_clean_notin_old\r\n\r\n    ##write to S3 - the delta file - difference between existing master and the new P2 addr std base \r\n    #wr.s3.to_excel(p2_clean_notin_old,'s3://astro-groupdata-prod-target/rpa/Fresh_P2_TM_Format.xlsx')\r\n    \r\ndef masterlist_tmk_1_file(tmk_1_path):\r\n    \r\n    tmk_1 = wr.s3.read_csv(tmk_1_path)\r\n    tmk_1['ACCOUNT'] = tmk_1['ACCOUNT'].astype(str)\r\n    tmk_1['ACCOUNT'] = tmk_1['ACCOUNT'].str.replace('\\.0','', case = False)\r\n    \r\n    tmk_1_clean = tmk_1[(tmk_1['ACCOUNT'].notnull())]\r\n    return tmk_1_clean\r\n    \r\ndef masterlist_tmk_2_file(tmk_2_path):\r\n    \r\n    tmk_2 = wr.s3.read_csv(tmk_2_path)\r\n    tmk_2['ACCOUNT'] = tmk_2['ACCOUNT'].astype(str)\r\n    tmk_2['ACCOUNT'] = tmk_2['ACCOUNT'].str.replace('\\.0','', case = False)\r\n    \r\n    tmk_2_clean = tmk_2[(tmk_2['ACCOUNT'].notnull())]\r\n    return tmk_2_clean\r\n\r\ndef masterlist_tmk_files(tmk_1_clean,tmk_2_clean,p1_clean_notin_old,p2_clean_notin_old):\r\n    \r\n    tmk_list = pd.concat([tmk_1_clean,tmk_2_clean])\r\n    tmk_list.rename(columns = {list(tmk_list)[0]:'ACCOUNT'}, inplace=True)\r\n    tmk_list = tmk_list.drop_duplicates()\r\n    print('Total TMK Masterlist shape:', tmk_list.shape) ## Amzar 14/10/2022 --> added more to the print statement\r\n    tmk_list.head()\r\n    \r\n    tmk_list = list(tmk_list['ACCOUNT'])\r\n\r\n    p1_clean_notin_old = p1_clean_notin_old[~p1_clean_notin_old[\"ACCOUNT_ID\"].isin(tmk_list)]\r\n    p2_clean_notin_old = p2_clean_notin_old[~p2_clean_notin_old[\"ACCOUNT_ID\"].isin(tmk_list)]\r\n    \r\ndef acc_status_base(acc_status_path, p1_clean_notin_old,p2_clean_notin_old): # 21/12/22: removed final_delta_p1, final_delta_p2 from the function's arguments\r\n\r\n#     print(\"read the edw data file from the astro base - tech team\")\r\n#     print(\"return the dataframe - new base\")\r\n\r\n    acc_status = wr.s3.read_csv(acc_status_path, compression = 'gzip')\r\n    acc_status['account_no'] = acc_status['account_no'].astype(str)\r\n    acc_status['account_no'] = acc_status['account_no'].str.replace('\\.0','', case = False)\r\n    acc_status = acc_status.rename({'account_no':'ACCOUNT_ID'},axis=1)\r\n    acc_status.head()\r\n    \r\n    print('acc_status shape:',acc_status.shape)\r\n    \r\n    p1_clean_notin_old = pd.merge(p1_clean_notin_old,acc_status,on='ACCOUNT_ID',how='left')\r\n    p2_clean_notin_old = pd.merge(p2_clean_notin_old,acc_status,on='ACCOUNT_ID',how='left')\r\n    \r\n    # print('p1_clean_notin_old:',p1_clean_notin_old.shape)\r\n    # print('p2_clean_notin_old:',p2_clean_notin_old.shape)\r\n    \r\n    ## Filter PayTV active only\r\n    final_delta_p1 = p1_clean_notin_old[(p1_clean_notin_old['cust_sub_type']=='NORMAL')&(p1_clean_notin_old['acc_status']=='ACTIVE')]\r\n    final_delta_p2 = p2_clean_notin_old[(p2_clean_notin_old['cust_sub_type']=='NORMAL')&(p2_clean_notin_old['acc_status']=='ACTIVE')]\r\n    \r\n    # wr.s3.to_excel(p1_clean_notin_old,'s3://astro-groupdata-prod-target/rpa/Fresh_P1_TM_Format_20221014.xlsx', index=False) ## Amzar 14/10/2022 --> added date_key & index=False\r\n    # wr.s3.to_excel(p2_clean_notin_old,'s3://astro-groupdata-prod-target/rpa/Fresh_P2_TM_Format_20221014.xlsx', index=False) ## Amzar 14/10/2022 --> added date_key & index=False\r\n    # wr.s3.to_excel(p1_clean_notin_old,'s3://astro-groupdata-prod-target/rpa/Fresh_P1_TM_Format_20221019.xlsx', index=False) ## Amzar 19/10/2022 --> added new date for experimenting\r\n    # wr.s3.to_excel(p2_clean_notin_old,'s3://astro-groupdata-prod-target/rpa/Fresh_P2_TM_Format_20221019.xlsx', index=False) ## Amzar 19/10/2022 --> added new date for experimenting (to see difference between Fresh & New P1P2 TM Format files. Found that this 20221019 version is more correct compared to the 20221014 version )\r\n    wr.s3.to_excel(p1_clean_notin_old,'s3://astro-groupdata-prod-target/rpa/Fresh_P1_TM_Format.xlsx', index=False) # Amzar 21/11/2022 --> automated version\r\n    wr.s3.to_excel(p1_clean_notin_old,'s3://astro-groupdata-prod-target/rpa/historical_folder/Fresh_TM_Format/Fresh_P1_TM_Format_'+str(curr_date)+'.xlsx', index=False) # Amzar 21/11/2022 --> historical version\r\n    wr.s3.to_excel(p2_clean_notin_old,'s3://astro-groupdata-prod-target/rpa/Fresh_P2_TM_Format.xlsx', index=False) # Amzar 21/11/2022 --> automated version\r\n    wr.s3.to_excel(p2_clean_notin_old,'s3://astro-groupdata-prod-target/rpa/historical_folder/Fresh_TM_Format/Fresh_P2_TM_Format_'+str(curr_date)+'.xlsx', index=False) # Amzar 21/11/2022 --> historical version\r\n\r\n    # final_delta_p1['p_base'] = 'P1'\r\n    # final_delta_p2['p_base'] = 'P2'\r\n\r\n    print('Fresh P1P2 final shapes (PayTV Active):', final_delta_p1.shape,final_delta_p2.shape) ## Amzar 14/10/2022 --> added more to the print statement\r\n    \r\n    return # 21/12/2022: commented out below lines & replaced it with this line coz in main(), we don't assign the output of this function to anything\r\n    # return final_delta_p1\r\n    # return final_delta_p2\r\n    \r\n    #'acc_status', 'cust_sub_type' -> active, non njoi \r\n    # generate delta -> filter \r\n    # generate master -> evertyhing  \r\n    \r\ndef final_delta_files(final_delta_p1,final_delta_p2):\r\n    \r\n    curr_date = str(pd.datetime.today().strftime('%d%m%Y'))\r\n    print('curr_date of run:',curr_date)  ## Amzar 14/10/2022 --> added more to the print statement\r\n    \r\n    final_delta = pd.concat([final_delta_p1,final_delta_p2]) # 22/11/22: CONSIDER not concatenating with P2 file coz it may not be accurate enough for RPA\r\n    print('Final delta (Fresh P1P2) shape before removing nulls:', final_delta.shape) ## Amzar 14/10/2022 --> added more to the print statement\r\n    \r\n    final_delta['ACCOUNT_ID'] = final_delta['ACCOUNT_ID'].astype(str)\r\n    final_delta['ACCOUNT_ID'] = final_delta['ACCOUNT_ID'].str.replace('\\.0','', case = False)\r\n\r\n    # Remove missing data\r\n    final_delta = final_delta[final_delta['ISP_INDICATOR'].notnull()]\r\n    final_delta = final_delta[final_delta['TM_POSTCODE'].notnull()]\r\n    print(final_delta.shape, final_delta.ACCOUNT_ID.nunique())\r\n\r\n    #Correcting Postcode\r\n    final_delta['TM_POSTCODE'] = final_delta['TM_POSTCODE'].fillna('')\r\n    final_delta['TM_POSTCODE']= final_delta['TM_POSTCODE'].astype(str).replace('\\.0', '', regex=True)\r\n    final_delta['TM_POSTCODE']= final_delta['TM_POSTCODE'].astype(str).apply(lambda x:x[0:5])\r\n    final_delta['TM_POSTCODE'] = final_delta['TM_POSTCODE'].str.pad(width=5, side='left', fillchar='0')\r\n    final_delta.TM_POSTCODE.unique()\r\n\r\n    # Shuffle P1/P2 position\r\n    from sklearn.utils import shuffle\r\n    final = shuffle(final_delta)\r\n\r\n    # Selecting final columns\r\n    final_rpa = final[['ACCOUNT_ID','CRM_OBJID', 'DTYPE',\r\n       'ASTRO_HOUSE_NO', 'ASTRO_CONDO_NAME', 'ASTRO_BLOCK', 'ASTRO_AREA',\r\n       'TM_STREET_TYPE', 'TM_STREET_NAME', 'TM_AREA', 'TM_CITY', 'TM_STATE',\r\n       'TM_POSTCODE', 'ISP_INDICATOR']]\r\n    print('Final RPA-ready delta (Fresh P1P2) shape (includes NJOI, PayTV, Active, Inactive):', final_rpa.shape) ## Amzar 14/10/2022 --> added more to the print statement\r\n    final_rpa.head()\r\n    \r\n    ## 21/12/22: added section below to Filter PayTV active only, for Fadhil's RPA purposes. Then select final columns\r\n    final_ptv_active = final[(final['cust_sub_type']=='NORMAL')&(final['acc_status']=='ACTIVE')]\r\n    final_rpa_ptv_active = final_ptv_active[['ACCOUNT_ID','CRM_OBJID', 'DTYPE',\r\n       'ASTRO_HOUSE_NO', 'ASTRO_CONDO_NAME', 'ASTRO_BLOCK', 'ASTRO_AREA',\r\n       'TM_STREET_TYPE', 'TM_STREET_NAME', 'TM_AREA', 'TM_CITY', 'TM_STATE',\r\n       'TM_POSTCODE', 'ISP_INDICATOR']]\r\n    print('Final RPA-ready delta (Fresh P1P2) shape (PayTV Active only):', final_rpa_ptv_active.shape) ## Amzar 14/10/2022 --> added more to the print statement\r\n    final_rpa_ptv_active.head()\r\n    \r\n    ## save the files\r\n    # wr.s3.to_excel(final_rpa,'s3://astro-groupdata-prod-target/rpa/CCMPP1_'+curr_date+'New.xlsx', index=False) ##  Amzar 14/10/2022 --> added index=False\r\n    wr.s3.to_excel(final_rpa,'s3://astro-groupdata-prod-target/rpa/New_CCMPP1/CCMPP1_'+curr_date+'New.xlsx', index=False) ##  Amzar 19/11/2022 --> added folder New_CCMP1 to organize. 21/12/22: decided to keep this line just in case we want NJOI or Inactive customers to go through RPA at any point\r\n    wr.s3.to_excel(final_rpa_ptv_active,'s3://astro-groupdata-prod-target/rpa/New_CCMPP1/CCMPP1_'+curr_date+'NewPayTVActive.xlsx', index=False) ##  Amzar 19/11/2022 --> added folder New_CCMP1 to organize. AFTER CREATING THIS FILE, remember to upload the CCMPP1_date_NewPayTVActive file to tmautobotprod. 21/12/22: added this line to save only the NewPayTVActive base\r\n    \r\n    return\r\n\r\ndef generate_latest_p1_file(new_p1_clean, old_p1_clean, acc_status_path):\r\n\r\n    ### Amzar 19/10/2022 --> added new codes to add EDW details to new_p1_clean\r\n    acc_status = wr.s3.read_csv(acc_status_path, compression = 'gzip', usecols=['account_no', 'service_postcode', 'service_city', 'acc_status', 'cust_sub_type'])\r\n    ## select relevant columns\r\n    # acc_status_small = acc_status.loc[:, ['account_no', 'service_postcode', 'service_city', 'acc_status', 'cust_sub_type']]\r\n    acc_status['account_no'] = acc_status['account_no'].astype(str)\r\n    acc_status['account_no'] = acc_status['account_no'].str.replace('\\.0','', case = False)\r\n    acc_status = acc_status.rename({'account_no':'ACCOUNT_ID'},axis=1)\r\n    \r\n    ## join the 2 tables to add the columns\r\n    new_p1_clean_1 = pd.merge(new_p1_clean,acc_status,on='ACCOUNT_ID',how='left')\r\n\r\n    latest_p1 = pd.concat([new_p1_clean_1,old_p1_clean]).drop_duplicates(subset='ACCOUNT_ID')\r\n    print('Latest P1 RPA Masterlist shape:', latest_p1.shape, latest_p1.ACCOUNT_ID.nunique()) ## Amzar 14/10/2022 --> added more to the print statement\r\n    \r\n    ### Amzar 19/10/2022 --> added new codes to clean weird instances of state/city from RPA\r\n    weird_postcodes = ['00000', '00100', '00300', '00nan', 'ERROR', '00NAN'] # found from looking at unique values in TM_POSTCODE\r\n    latest_p1.loc[latest_p1['TM_POSTCODE'].isin(weird_postcodes), 'TM_POSTCODE'] = latest_p1.loc[latest_p1['TM_POSTCODE'].isin(weird_postcodes), 'service_postcode']\r\n    weird_cities = [',SUNGAI PETANI', '*', '-', '1', '26600', '41200', '22200', '23000', '35900', '70450', '93250', 'nan'] # found from looking at unique values in TM_CITY\r\n    latest_p1.loc[latest_p1['TM_CITY'].isin(weird_postcodes), 'TM_CITY'] = latest_p1.loc[latest_p1['TM_CITY'].isin(weird_cities), 'service_city']\r\n\r\n    latest_p1['TM_POSTCODE']= latest_p1['TM_POSTCODE'].astype(str).replace('\\.0', '', regex=True)\r\n    latest_p1['TM_POSTCODE']= latest_p1['TM_POSTCODE'].astype(str).apply(lambda x:x[0:5])\r\n    latest_p1['TM_POSTCODE'] = latest_p1['TM_POSTCODE'].str.pad(width=5, side='left', fillchar='0')\r\n    # latest_p1.head()\r\n\r\n    ### Amzar 19/10/2022 --> put this step after cleaning TM_POSTCODE\r\n    latest_p1 = latest_p1[['ACCOUNT_ID','CRM_OBJID', 'DTYPE',\r\n       'ASTRO_HOUSE_NO', 'ASTRO_CONDO_NAME', 'ASTRO_BLOCK', 'ASTRO_AREA',\r\n       'TM_STREET_TYPE', 'TM_STREET_NAME', 'TM_AREA', 'TM_CITY', 'TM_STATE',\r\n       'TM_POSTCODE', 'ISP_INDICATOR', 'acc_status', 'cust_sub_type']]\r\n\r\n    ##write to S3 - either this goes in source bucket, or the source (old p1) should refer to pipeline : filter by active accounts and non njoi \r\n    # wr.s3.to_csv(latest_p1,'s3://astro-groupdata-prod-target/rpa/Master_P1_TM_Format.csv')\r\n    # wr.s3.to_csv(latest_p1,'s3://astro-groupdata-prod-target/rpa/Master_P1_TM_Format_20221014.csv', index=False)  ## Amzar 14/10/2022 --> added date_key & index=False\r\n    # wr.s3.to_csv(latest_p1,'s3://astro-groupdata-prod-target/rpa/Master_P1_TM_Format_20221019.csv', index=False) ## Amzar 19/10/2022 --> added new date for experimenting (to see difference between Fresh & New P1P2 TM Format files. Found that this 20221019 version is more correct compared to the 20221014 version )\r\n    wr.s3.to_csv(latest_p1,'s3://astro-groupdata-prod-target/rpa/Master_P1_TM_Format.csv', index=False) # Amzar 21/11/2022 --> automated version\r\n    wr.s3.to_csv(latest_p1,'s3://astro-groupdata-prod-target/rpa/historical_folder/Master_TM_Format/Master_P1_TM_Format_'+str(curr_date)+'.csv', index=False) # Amzar 21/11/2022 --> historical version\r\n    \r\ndef generate_latest_p2_file(new_p2_clean, old_p2_clean, acc_status_path):\r\n    \r\n    ### Amzar 19/10/2022 --> added new codes to add EDW details to new_p1_clean\r\n    acc_status = wr.s3.read_csv(acc_status_path, compression = 'gzip', usecols=['account_no', 'service_postcode', 'service_city', 'acc_status', 'cust_sub_type'])\r\n    ## select relevant columns\r\n    # acc_status_small = acc_status.loc[:, ['ACCOUNT_ID', 'service_postcode', 'service_city', 'acc_status', 'cust_sub_type']]\r\n    acc_status['account_no'] = acc_status['account_no'].astype(str)\r\n    acc_status['account_no'] = acc_status['account_no'].str.replace('\\.0','', case = False)\r\n    acc_status = acc_status.rename({'account_no':'ACCOUNT_ID'},axis=1)\r\n    ## join the 2 tables to add the columns\r\n    new_p2_clean_1 = pd.merge(new_p2_clean,acc_status,on='ACCOUNT_ID',how='left')\r\n\r\n    latest_p2 = pd.concat([new_p2_clean_1,old_p2_clean]).drop_duplicates(subset='ACCOUNT_ID')\r\n    print('Latest P1 RPA Masterlist shape:', latest_p2.shape, latest_p2.ACCOUNT_ID.nunique()) ## Amzar 14/10/2022 --> added more to the print statement\r\n\r\n    ### Amzar 19/10/2022 --> added new codes to clean weird instances of state/city from RPA\r\n    weird_postcodes = ['00000', '00100', '00300', '00nan', 'ERROR', '00NAN'] # found from looking at unique values in TM_POSTCODE\r\n    latest_p2.loc[latest_p2['TM_POSTCODE'].isin(weird_postcodes), 'TM_POSTCODE'] = latest_p2.loc[latest_p2['TM_POSTCODE'].isin(weird_postcodes), 'service_postcode']\r\n    weird_cities = [',SUNGAI PETANI', '*', '-', '1', '26600', '41200', '22200', '23000', '35900', '70450', '93250', 'nan'] # found from looking at unique values in TM_CITY\r\n    latest_p2.loc[latest_p2['TM_CITY'].isin(weird_postcodes), 'TM_CITY'] = latest_p2.loc[latest_p2['TM_CITY'].isin(weird_cities), 'service_city']\r\n\r\n    latest_p2['TM_POSTCODE']= latest_p2['TM_POSTCODE'].astype(str).replace('\\.0', '', regex=True)\r\n    latest_p2['TM_POSTCODE']= latest_p2['TM_POSTCODE'].astype(str).apply(lambda x:x[0:5])\r\n    latest_p2['TM_POSTCODE'] = latest_p2['TM_POSTCODE'].str.pad(width=5, side='left', fillchar='0')\r\n    # latest_p2.head()\r\n\r\n    ### Amzar 19/10/2022 --> put this step after cleaning TM_POSTCODE\r\n    latest_p2 = latest_p2[['ACCOUNT_ID','CRM_OBJID', 'DTYPE',\r\n       'ASTRO_HOUSE_NO', 'ASTRO_CONDO_NAME', 'ASTRO_BLOCK', 'ASTRO_AREA',\r\n       'TM_STREET_TYPE', 'TM_STREET_NAME', 'TM_AREA', 'TM_CITY', 'TM_STATE',\r\n       'TM_POSTCODE', 'ISP_INDICATOR', 'acc_status', 'cust_sub_type']]\r\n    \r\n    ##write to S3\r\n    # wr.s3.to_csv(latest_p2,'s3://astro-groupdata-prod-target/rpa/Master_P2_TM_Format.csv')  \r\n    # wr.s3.to_csv(latest_p2,'s3://astro-groupdata-prod-target/rpa/Master_P2_TM_Format_20221014.csv', index=False)  ## Amzar 14/10/2022 --> added date_key & index=False\r\n    # wr.s3.to_csv(latest_p2,'s3://astro-groupdata-prod-target/rpa/Master_P2_TM_Format_20221019.csv', index=False) ## Amzar 19/10/2022 --> added new date for experimenting (to see difference between Fresh & New P1P2 TM Format files. Found that this 20221019 version is more correct compared to the 20221014 version )\r\n    wr.s3.to_csv(latest_p2,'s3://astro-groupdata-prod-target/rpa/Master_P2_TM_Format.csv', index=False) # Amzar 21/11/2022 --> automated version\r\n    wr.s3.to_csv(latest_p2,'s3://astro-groupdata-prod-target/rpa/historical_folder/Master_TM_Format/Master_P2_TM_Format_'+str(curr_date)+'.csv', index=False) # Amzar 21/11/2022 --> historical version\r\n\r\ndef main():\r\n\r\n    s3_client = boto3.client('s3')\r\n    # curr_date = str(pd.datetime.today().strftime('%Y%m%d')) \r\n\r\n    # # new_p1_path = 's3://astro-groupdata-prod-source/rpa/New_P1_TM_Format.csv'\r\n    # # new_p1_path = 's3://astro-groupdata-prod-source/rpa/New_P1_TM_Format_20221014.csv' ## Amzar 14/10/2022 --> use the newly generated one\r\n    # new_p1_path = 's3://astro-groupdata-prod-source/rpa/New_P1_TM_Format_old_code_20221018.csv.gz' ## Amzar 19/10/2022 --> added new date for experimenting\r\n    new_p1_path = 's3://astro-groupdata-prod-source/rpa/New_P1_TM_Format_glue_spark.csv.gz' ## Amzar 21/11/2022 --> automated version\r\n    new_p1_clean = read_new_p1_file(new_p1_path)\r\n\r\n    # # new_p2_path = 's3://astro-groupdata-prod-source/rpa/New_P2_TM_Format.csv'\r\n    # # new_p2_path = 's3://astro-groupdata-prod-source/rpa/New_P2_TM_Format_20221014.csv' ## Amzar 14/10/2022 --> use the newly generated one\r\n    # new_p2_path = 's3://astro-groupdata-prod-source/rpa/New_P2_TM_Format_old_code_20221018.csv.gz' ## Amzar 19/10/2022 --> added new date for experimenting\r\n    new_p2_path = 's3://astro-groupdata-prod-source/rpa/New_P2_TM_Format_glue_spark.csv.gz' ## Amzar 21/11/2022 --> automated version\r\n    new_p2_clean = read_new_p2_file(new_p2_path)\r\n    # ## pipeline bucket vs source bucket in other \r\n\r\n    # # old_p1_path = 's3://astro-groupdata-prod-source/rpa/Master_P1_TM_Format.csv' \r\n    # old_p1_path = 's3://astro-groupdata-prod-target/rpa/Master_P1_TM_Format.csv'  ## Amzar 14/10/2022 --> use the one in prod-target as there is none in source. Automated version\r\n    old_p1_path = 's3://astro-groupdata-prod-target/rpa/Master_P1_TM_Format_20221019.csv' # Amzar 21/11/2022 - temporary\r\n    old_p1_clean = read_old_p1_file(old_p1_path)\r\n    # ## pipeline bucket vs source bucket in other \r\n\r\n    # # old_p2_path = 's3://astro-groupdata-prod-source/rpa/Master_P2_TM_Format.csv'\r\n    # old_p2_path = 's3://astro-groupdata-prod-target/rpa/Master_P2_TM_Format.csv'  ## Amzar 14/10/2022 --> use the one in prod-target as there is none in source. Automated version\r\n    old_p2_path = 's3://astro-groupdata-prod-target/rpa/Master_P2_TM_Format_20221019.csv' # Amzar 21/11/2022 - temporary\r\n    old_p2_clean = read_old_p2_file(old_p2_path)\r\n\r\n    tmk_1_path = 's3://astro-groupdata-prod-source/sftp/rpa/Masterlist_TMK_1.csv'\r\n    tmk_1_clean = masterlist_tmk_1_file(tmk_1_path)\r\n\r\n    tmk_2_path = 's3://astro-groupdata-prod-source/sftp/rpa/Masterlist_TMK_2.csv'\r\n    tmk_2_clean = masterlist_tmk_2_file(tmk_2_path)\r\n\r\n    # # acc_status_path = 's3://astro-groupdata-prod-source/astro/new/EDW_DL_ADDRESS_DATA_202204.csv.gz'\r\n    # # acc_status_path = 's3://astro-datalake-prod-ulm-pii/edw_address/monthly_address_details/year=2022/month=08/EDW_DL_ADDRESS_DATA_202208.csv.gz' ## Amzar 14/10/2022 --> switched to 202208 & path directly from Qubole. Turns out this path is forbidden from AWS. So need to download on Qubole, then upload to the AWS path\r\n    acc_status_path = 's3://astro-groupdata-prod-source/astro/new/EDW_DL_ADDRESS_DATA_202209.csv.gz' ## changeable based on latest EDW address dump available in Qubole S3. Amzar 14/10/2022 --> had to upload since above path is forbidden    \r\n    \r\n    p1_clean_notin_old = generate_fresh_p1_file(old_p1_clean, new_p1_clean)\r\n    p2_clean_notin_old = generate_fresh_p2_file(old_p2_clean, new_p2_clean)\r\n    masterlist_tmk_files(tmk_1_clean,tmk_2_clean,p1_clean_notin_old,p2_clean_notin_old)\r\n    acc_status_base(acc_status_path, p1_clean_notin_old,p2_clean_notin_old) # 21/12/22: removed the final_delta_p1/p2 from func args\r\n    final_delta_p1 = p1_clean_notin_old\r\n    final_delta_p2 = p2_clean_notin_old\r\n    final_delta_files(final_delta_p1,final_delta_p2)\r\n    generate_latest_p1_file(new_p1_clean, old_p1_clean, acc_status_path)\r\n    generate_latest_p2_file(new_p2_clean, old_p2_clean, acc_status_path)\r\n    \r\nmain()"
}