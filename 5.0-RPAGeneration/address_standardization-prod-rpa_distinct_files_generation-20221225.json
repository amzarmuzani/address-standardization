{
  "jobConfig": {
    "name": "address_standardization-prod-rpa_distinct_files_generation",
    "description": "",
    "role": "arn:aws:iam::741993363917:role/AWSGlueServiceRole",
    "command": "glueetl",
    "version": "2.0",
    "workerType": "G.2X",
    "numberOfWorkers": 5,
    "maxCapacity": 10,
    "maxRetries": 0,
    "timeout": 60,
    "maxConcurrentRuns": 10,
    "security": "none",
    "scriptName": "RPA_Distinct_Files_Generation_220601.py",
    "scriptLocation": "s3://astro-groupdata-prod-config/address_standardization/python_scripts/",
    "language": "python-3",
    "jobParameters": [
      {
        "key": "--ISP_Name",
        "value": "TM",
        "existing": false
      },
      {
        "key": "--additional-python-modules",
        "value": "awswrangler==2.12.1",
        "existing": false
      },
      {
        "key": "--job_name",
        "value": "address_standardization-prod-rpa_distinct_files_generation",
        "existing": false
      },
      {
        "key": "--rpa_bucket_path",
        "value": "s3://astro-groupdata-prod-source/rpa/",
        "existing": false
      },
      {
        "key": "--temp_isp_corrected_read_path",
        "value": "s3://astro-groupdata-prod-pipeline/address_standardization/uams_temp_final/temp_isp_corrected_before_rpa_distinct.csv.gz",
        "existing": false
      }
    ],
    "tags": [
      {
        "key": "Name",
        "value": "address_standardization-prod-rpa_distinct_files_generation",
        "existing": false
      },
      {
        "key": "Project",
        "value": "Address_standardization",
        "existing": false
      },
      {
        "key": "Technical_Owner",
        "value": "Amzar",
        "existing": false
      }
    ],
    "jobMode": "DEVELOPER_MODE",
    "useGlueProvidedDataLakeLibs": false,
    "developerMode": true,
    "connectionsList": [],
    "temporaryDirectory": "s3://astro-groupdata-prod-config/address_standardization/python_scripts/",
    "logging": true,
    "etlAutoTuning": false,
    "pythonPath": "s3://astro-groupdata-prod-config/address_standardization/python_library/awswrangler-2.12.1-py3-none-any.whl",
    "bookmark": "job-bookmark-disable",
    "flexExecution": false,
    "minFlexWorkers": null,
    "etlAutoScaling": false
  },
  "hasBeenSaved": false,
  "script": "import awswrangler as wr\r\nimport sys\r\nfrom awsglue.utils import getResolvedOptions\r\nimport pandas as pd\r\nimport numpy as np\r\nimport re\r\nimport resource\r\nfrom datetime import datetime\r\n\r\n## Amzar 18/11/2022 -> added a datetime key for tracking file creation\r\ncurr_date = str(pd.datetime.today().strftime('%Y%m%d'))\r\n# curr_date = '20221118' # temporary assigning\r\n\r\nargs = getResolvedOptions(sys.argv,\r\n                          ['job_name',\r\n                           'ISP_Name',\r\n                           'temp_isp_corrected_read_path',\r\n                           'rpa_bucket_path'])\r\n\r\nISP_Name = args['ISP_Name']\r\ntemp_isp_corrected_read_path = args['temp_isp_corrected_read_path'] #new std path\r\nrpa_bucket_path = args['rpa_bucket_path'] #rpa bucket\r\n\r\nisp_corrected = wr.s3.read_csv(path = temp_isp_corrected_read_path)\r\n\r\n#revision - 29/8/22 fakhrul adding cleaning code here as postcode is messed up due to decimal\r\n#print('checking isp corrected before correcting: ', isp_corrected.info())\r\n#print(isp_corrected[['Postcode']].head())\r\n\r\n#isp_corrected['Postcode'] = isp_corrected['Postcode'].astype(str)\r\n#isp_corrected['Postcode']  = isp_corrected['Postcode'].str.replace('\\.0','', case = False)\r\n\r\nisp_corrected['Postcode'] = isp_corrected.Postcode.astype(str)\r\nisp_corrected['Postcode'] = isp_corrected['Postcode'].map(str).apply(lambda x: x.zfill(5))\r\nisp_corrected['Postcode'] = isp_corrected['Postcode'].str.pad(width=5)\r\n\r\n#print('checking isp corrected after correcting: ', isp_corrected.info())\r\n#print(isp_corrected[['Postcode']].head())\r\n\r\nif ISP_Name == 'TM':\r\n\r\n    ### ----- below codes are commented out as we eventually decided to go with the old mapping key which is HNUM_STRT_TM instead of Mix_key (Zohreh tested). So below codes are just duplicates of original code (OLD_CODE) but Zohreh wanted to test out Mix_key.\r\n    # rpa_isp_corrected = isp_corrected\r\n\r\n    # rpa_isp_corrected[\"ISP_ADDRESS\"] = rpa_isp_corrected[\"BuildingName\"].map(str) + \" ,\" + rpa_isp_corrected[\"HouseNo\"].map(str) + \" ,\" + rpa_isp_corrected[\"StreetType\"].map(str) + \" ,\" + rpa_isp_corrected[\"StreetName\"].map(str) + \" ,\" + rpa_isp_corrected[\"Postcode\"].map(str)+ \" ,\" + rpa_isp_corrected[\"City\"].map(str)+ \" ,\" + rpa_isp_corrected[\"State\"].map(str)\r\n    # # rpa_isp_corrected[\"ISP_ADDRESS\"] = rpa_isp_corrected[\"BuildingName\"].map(str) + \" ,\" + rpa_isp_corrected[\"HouseNo\"].map(str) + \" ,\" + rpa_isp_corrected[\"StreetType\"].map(str) + \" ,\" + rpa_isp_corrected[\"StreetName\"].map(str) + \" ,\" + rpa_isp_corrected[\"Postcode\"].map(str)+ \" ,\" + rpa_isp_corrected[\"City\"].map(str)+ \" ,\" + rpa_isp_corrected[\"State\"].map(str)+ \" ,\" + rpa_isp_corrected[\"Street_1\"].map(str)+ \" ,\" + rpa_isp_corrected[\"Combined_Building\"].map(str)\r\n\r\n    \r\n    # rpa_isp_corrected['TM_HOUSE_NO1']= rpa_isp_corrected['HouseNo'].str.pad(width=10)\r\n    \r\n    # New_Columns = rpa_isp_corrected.drop_duplicates(subset=['BuildingName','HouseNo','StreetType','Street_1','Postcode','STD_CITY','State'],\r\n    #                                                       keep='first')\r\n    \r\n    # distinct_mdu_p1 = New_Columns[['HNUM_STRT_TM','TM_HOUSE_NO1','HouseNo','BuildingName',\r\n    #                                   'StreetType','StreetName',\r\n    #                                   'Section','Postcode','City',\r\n    #                                   'State','ServiceType', 'ISP_ADDRESS']]\r\n    \r\n    #     distinct_mdu_p1['HOUSE_NO_TM'] = distinct_mdu_p1['TM_HOUSE_NO1'].astype(str)\r\n    # distinct_mdu_p1 = distinct_mdu_p1.drop('TM_HOUSE_NO1',axis=1)\r\n  \r\n    # distinct_mdu_p1 = distinct_mdu_p1[distinct_mdu_p1['BuildingName'].notnull()]\r\n    # print(distinct_mdu_p1.shape)\r\n    \r\n    # distinct_mdu_p1[\"Mix_key\"] = distinct_mdu_p1[\"StreetName\"].map(str) + \" ,\" + distinct_mdu_p1[\"HNUM_STRT_TM\"].map(str)\r\n    # distinct_mdu_p1 = distinct_mdu_p1.drop_duplicates(subset=['Mix_key'], keep='first')\r\n    \r\n    # # Rerrange columns\r\n    # distinct_mdu_p1 = distinct_mdu_p1[['HOUSE_NO_TM', 'BuildingName', 'StreetType', 'StreetName', 'Section',\r\n    #                                       'Postcode', 'City', 'State', 'ServiceType', 'ISP_ADDRESS','HNUM_STRT_TM'\r\n    #                                       ,'Mix_key' ]] \r\n\r\n    # #distinct_mdu_p1.to_csv('Distinct_Fields_P1_MDU_Mar.csv',index=False)\r\n    # wr.s3.to_csv(df = distinct_mdu_p1, path = rpa_bucket_path + 'historical_folder/Distinct_Fields_P1_MDU_' + str(curr_date) + '.csv', index = False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: added historical_folder and added curr_date\r\n    # wr.s3.to_csv(df = distinct_mdu_p1, path = 's3://astro-groupdata-prod-target/rpa/' + 'Distinct_Fields_P1_MDU.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: remove curr_date to allow for automation\r\n    # wr.s3.to_csv(df = distinct_mdu_p1, path = 's3://astro-groupdata-prod-source/sftp/rpa/' + 'Distinct_Fields_P1_MDU.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: remove curr_date to allow for automation\r\n\r\n ###============= Original Mapping Codes down here================\r\n    \r\n    rpa_isp_corrected = isp_corrected\r\n\r\n    rpa_isp_corrected[\"ISP_ADDRESS\"] = rpa_isp_corrected[\"BuildingName\"].map(str) + \" ,\" + rpa_isp_corrected[\"HouseNo\"].map(str) + \" ,\" + rpa_isp_corrected[\"StreetType\"].map(str) + \" ,\" + rpa_isp_corrected[\"StreetName\"].map(str) + \" ,\" + rpa_isp_corrected[\"Postcode\"].map(str)+ \" ,\" + rpa_isp_corrected[\"City\"].map(str)+ \" ,\" + rpa_isp_corrected[\"State\"].map(str)+ \" ,\" + rpa_isp_corrected[\"Street_1\"].map(str)+ \" ,\" + rpa_isp_corrected[\"Combined_Building\"].map(str)\r\n    \r\n    rpa_isp_corrected['TM_HOUSE_NO1']= rpa_isp_corrected['HouseNo'].str.pad(width=10)\r\n    \r\n    New_Columns = rpa_isp_corrected.drop_duplicates(subset=['BuildingName','HouseNo','StreetType','Street_1','Postcode','STD_CITY','State'],\r\n                                                          keep='first')\r\n    \r\n    distinct_mdu_p1 = New_Columns[['HNUM_STRT_TM','TM_HOUSE_NO1','HouseNo','BuildingName',\r\n                                      'StreetType','StreetName',\r\n                                      'Section','Postcode','City',\r\n                                      'State','ServiceType', 'ISP_ADDRESS']]\r\n    \r\n    distinct_mdu_p1['HOUSE_NO_TM'] = distinct_mdu_p1['TM_HOUSE_NO1'].astype(str)\r\n    distinct_mdu_p1 = distinct_mdu_p1.drop('TM_HOUSE_NO1',axis=1)\r\n    \r\n    # Rerrange columns\r\n    distinct_mdu_p1 = distinct_mdu_p1[['HOUSE_NO_TM', 'BuildingName', 'StreetType', 'StreetName', 'Section',\r\n                                          'Postcode', 'City', 'State', 'ServiceType', 'ISP_ADDRESS','HNUM_STRT_TM'\r\n                                          ]]\r\n    \r\n    distinct_mdu_p1 = distinct_mdu_p1[distinct_mdu_p1['BuildingName'].notnull()]\r\n    print(distinct_mdu_p1.shape)\r\n    #distinct_mdu_p1.to_csv('Distinct_Fields_P1_MDU_Mar_V1.csv',index=False)\r\n    wr.s3.to_csv(df = distinct_mdu_p1, path = rpa_bucket_path + 'historical_folder/Distinct_Fields_P1_MDU_' + str(curr_date) + '.csv.gz', index = False, compression='gzip') ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: added historical_folder and added curr_date. 22/11/22: added gzip compression\r\n    wr.s3.to_csv(df = distinct_mdu_p1, path = 's3://astro-groupdata-prod-target/rpa/' + 'Distinct_Fields_P1_MDU.csv.gz',index=False, compression='gzi[') ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: remove curr_date to allow for automation. 22/11/22: added gzip compression\r\n    wr.s3.to_csv(df = distinct_mdu_p1, path = 's3://astro-groupdata-prod-source/sftp/rpa/' + 'Distinct_Fields_P1_MDU.csv.gz',index=False, compression='gzip') ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: remove curr_date to allow for automation. 22/11/22: added gzip compression\r\n    \r\n    ### --- 18/11/22: originally saved the below files as 'old_code' because Zohreh introduced the mix_key into this notebook to strengthen the mapping, but it reduced P1P2 numbers too much that we decided to go back to the original mapping method and save it above as the normal files\r\n    # wr.s3.to_csv(df = distinct_mdu_p1, path = rpa_bucket_path + 'historical_folder/Distinct_Fields_P1_MDU_old_code_' + str(curr_date) + '.csv', index = False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: added historical_folder and added curr_date\r\n    # wr.s3.to_csv(df = distinct_mdu_p1, path = 's3://astro-groupdata-prod-target/rpa/' + 'Distinct_Fields_P1_MDU_old_code.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: removed date to allow for automation\r\n    # wr.s3.to_csv(df = distinct_mdu_p1, path = 's3://astro-groupdata-prod-source/sftp/rpa/' + 'Distinct_Fields_P1_MDU_old_code.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: removed date to allow for automation\r\n\r\nelse:\r\n    print('Not TM')\r\n\r\n# Clean HNUM_STRT_TM column\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected['HNUM_STRT_TM'].str.replace('nan ','', case = False)\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected['HNUM_STRT_TM'].str.replace('[,.]','', case = False)\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected[\"HNUM_STRT_TM\"].str.replace(\" \",\"\")\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected[\"HNUM_STRT_TM\"].str.replace(\",\",\"\")\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected[\"HNUM_STRT_TM\"].str.replace(\"\\.\",\"\")\r\n\r\n# Capitalize HNUM_STRT_TM\r\nisp_corrected['HNUM_STRT_TM'] = isp_corrected['HNUM_STRT_TM'].str.upper() \r\n\r\n# Remove nulls in HNUM_STRT and HNUM_STRT_TM\r\nisp_corrected = isp_corrected[isp_corrected.HNUM_STRT_TM.notnull()]\r\n\r\nprint(isp_corrected.shape)\r\n\r\n\r\n##========================================================= SDU P1 ==============================================================\r\n\r\n# Add new column in isp_corrected -- HouseNo + G_Street_Name_1 + G_City = HNUM_STRT_TM\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected[\"HouseNo\"].map(str) + \" ,\" + isp_corrected[\"Street_1\"].map(str) + \" ,\" + isp_corrected[\"STD_CITY\"].map(str)\r\nisp_corrected.head()\r\n\r\n# Clean HNUM_STRT_TM column\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected['HNUM_STRT_TM'].str.replace('nan ','', case = False)\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected['HNUM_STRT_TM'].str.replace('[,.]','', case = False)\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected[\"HNUM_STRT_TM\"].str.replace(\" \",\"\")\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected[\"HNUM_STRT_TM\"].str.replace(\",\",\"\")\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected[\"HNUM_STRT_TM\"].str.replace(\"\\.\",\"\")\r\n\r\n# Capitalize HNUM_STRT_TM\r\nisp_corrected['HNUM_STRT_TM'] = isp_corrected['HNUM_STRT_TM'].str.upper() \r\n\r\n# Remove nulls in HNUM_STRT and HNUM_STRT_TM\r\nisp_corrected = isp_corrected[isp_corrected.HNUM_STRT_TM.notnull()]\r\n\r\n## ### RPA Base generation - to be saved in RPA bucket\r\n#### Getting the ISP fields\r\n#from datetime import datetime\r\n#now = datetime.now()\r\n## Capitalize HNUM_STRT_TM\r\nif ISP_Name == 'TM':\r\n\r\n    ### ----- below codes are commented out as we eventually decided to go with the old mapping key which is HNUM_STRT_TM instead of Mix_key (Zohreh tested). So below codes are just duplicates of original code (OLD_CODE) but Zohreh wanted to test out Mix_key.\r\n    # rpa_isp_corrected = isp_corrected\r\n\r\n    # rpa_isp_corrected = isp_corrected\r\n\r\n    # rpa_isp_corrected[\"ISP_ADDRESS\"] = rpa_isp_corrected[\"BuildingName\"].map(str) + \" ,\" + rpa_isp_corrected[\"HouseNo\"].map(str) + \" ,\" + rpa_isp_corrected[\"StreetType\"].map(str) + \" ,\" +                                 rpa_isp_corrected[\"StreetName\"].map(str) + \" ,\" +                                 rpa_isp_corrected[\"Postcode\"].map(str)+ \" ,\" +                                rpa_isp_corrected[\"City\"].map(str)+ \" ,\" +                                 rpa_isp_corrected[\"State\"].map(str)\r\n    \r\n    # rpa_isp_corrected['TM_HOUSE_NO1']= rpa_isp_corrected['HouseNo'].str.pad(width=10)\r\n    \r\n    # New_Columns = rpa_isp_corrected.drop_duplicates(subset=['HouseNo','StreetType','Street_1','Postcode','STD_CITY','State'],\r\n    #                                                       keep='first')\r\n    \r\n    # distinct_sdu_p1 = New_Columns[['HNUM_STRT_TM','TM_HOUSE_NO1','HouseNo','BuildingName',\r\n    #                                   'StreetType','StreetName',\r\n    #                                   'Section','Postcode','City',\r\n    #                                   'State','ServiceType', 'ISP_ADDRESS']]\r\n    \r\n    \r\n    \r\n    # distinct_sdu_p1['HOUSE_NO_TM'] = distinct_sdu_p1['TM_HOUSE_NO1'].astype(str)\r\n    # distinct_sdu_p1 = distinct_sdu_p1.drop('TM_HOUSE_NO1',axis=1)\r\n    \r\n    # distinct_sdu_p1[\"Mix_key\"] = distinct_sdu_p1[\"BuildingName\"].map(str) + \" ,\" + distinct_sdu_p1[\"HNUM_STRT_TM\"].map(str)\r\n    # distinct_sdu_p1 = distinct_sdu_p1.drop_duplicates(subset=['Mix_key'], keep='first')\r\n       \r\n    \r\n    # # Rerrange columns\r\n    # distinct_sdu_p1 = distinct_sdu_p1[['HOUSE_NO_TM', 'BuildingName', 'StreetType', 'StreetName', 'Section',\r\n    #                                       'Postcode', 'City', 'State', 'ServiceType', 'ISP_ADDRESS','HNUM_STRT_TM'\r\n    #                                       ,'Mix_key' ]]\r\n    \r\n    # print(distinct_sdu_p1.shape)\r\n\r\n    #distinct_sdu_p1.to_csv('Distinct_Fields_P1_SDU_Mar.csv',index=False)\r\n    # wr.s3.to_csv(df = distinct_mdu_p1, path = rpa_bucket_path + 'historical_folder/Distinct_Fields_P1_SDU_' + str(curr_date) + '.csv', index = False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: added historical_folder and added curr_date\r\n    # wr.s3.to_csv(df = distinct_mdu_p1, path = 's3://astro-groupdata-prod-target/rpa/' + 'Distinct_Fields_P1_SDU.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: remove curr_date to allow for automation\r\n    # wr.s3.to_csv(df = distinct_mdu_p1, path = 's3://astro-groupdata-prod-source/sftp/rpa/' + 'Distinct_Fields_P1_SDU.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: remove curr_date to allow for automation\r\n    \r\n###============= Original Mapping Codes =================down here=====================================================================================\r\n    \r\n    rpa_isp_corrected = isp_corrected\r\n\r\n    rpa_isp_corrected[\"ISP_ADDRESS\"] = rpa_isp_corrected[\"BuildingName\"].map(str) + \" ,\" +                                 rpa_isp_corrected[\"HouseNo\"].map(str) + \" ,\" +                                 rpa_isp_corrected[\"StreetType\"].map(str) + \" ,\" +                                 rpa_isp_corrected[\"StreetName\"].map(str) + \" ,\" +                                 rpa_isp_corrected[\"Postcode\"].map(str)+ \" ,\" +                                rpa_isp_corrected[\"City\"].map(str)+ \" ,\" +                                 rpa_isp_corrected[\"State\"].map(str)+ \" ,\" +                                 rpa_isp_corrected[\"Street_1\"].map(str)+ \" ,\" +                                 rpa_isp_corrected[\"Combined_Building\"].map(str)\r\n    \r\n    rpa_isp_corrected['TM_HOUSE_NO1']= rpa_isp_corrected['HouseNo'].str.pad(width=10)\r\n    \r\n    New_Columns = rpa_isp_corrected.drop_duplicates(subset=['HouseNo','StreetType','Street_1','Postcode','STD_CITY','State'],\r\n                                                          keep='first')\r\n    \r\n    distinct_sdu_p1 = New_Columns[['HNUM_STRT_TM','TM_HOUSE_NO1','HouseNo','BuildingName',\r\n                                      'StreetType','StreetName',\r\n                                      'Section','Postcode','City',\r\n                                      'State','ServiceType', 'ISP_ADDRESS']]\r\n    \r\n    distinct_sdu_p1['HOUSE_NO_TM'] = distinct_sdu_p1['TM_HOUSE_NO1'].astype(str)\r\n    distinct_sdu_p1 = distinct_sdu_p1.drop('TM_HOUSE_NO1',axis=1)\r\n    \r\n    # Rerrange columns\r\n    distinct_sdu_p1 = distinct_sdu_p1[['HOUSE_NO_TM', 'BuildingName', 'StreetType', 'StreetName', 'Section',\r\n                                           'Postcode', 'City', 'State', 'ServiceType', 'ISP_ADDRESS','HNUM_STRT_TM'\r\n                                           ]]\r\n    \r\n    print(distinct_sdu_p1.shape)\r\n    #distinct_sdu_p1.to_csv('Distinct_Fields_P1_SDU_Mar_V1.csv',index=False)\r\n    wr.s3.to_csv(df = distinct_sdu_p1, path = rpa_bucket_path + 'historical_folder/Distinct_Fields_P1_SDU_' + str(curr_date) + '.csv.gz', index = False, compression='gzip') ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: added historical_folder and added curr_date. 22/11/22: added gzip compression\r\n    wr.s3.to_csv(df = distinct_sdu_p1, path = 's3://astro-groupdata-prod-target/rpa/' + 'Distinct_Fields_P1_SDU.csv.gz',index=False, compression='gzip') ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: remove curr_date to allow for automation. 22/11/22: added gzip compression\r\n    wr.s3.to_csv(df = distinct_sdu_p1, path = 's3://astro-groupdata-prod-source/sftp/rpa/' + 'Distinct_Fields_P1_SDU.csv.gz',index=False, compression='gzip') ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: remove curr_date to allow for automation. 22/11/22: added gzip compression\r\n    \r\n    ### --- 18/11/22: originally saved the below files as 'old_code' because Zohreh introduced the mix_key into this notebook to strengthen the mapping, but it reduced P1P2 numbers too much that we decided to go back to the original mapping method and save it above as the normal files\r\n    # wr.s3.to_csv(df = distinct_sdu_p1, path = rpa_bucket_path + 'Distinct_Fields_P1_SDU_old_code_20221013.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename\r\n    # wr.s3.to_csv(df = distinct_sdu_p1, path = 's3://astro-groupdata-prod-target/rpa/' + 'Distinct_Fields_P1_SDU_old_code_20221013.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename\r\n    # wr.s3.to_csv(df = distinct_sdu_p1, path = 's3://astro-groupdata-prod-source/sftp/rpa/' + 'Distinct_Fields_P1_SDU_old_code_20221013.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename\r\n    \r\n    \r\nelse:\r\n    print('Not TM')\r\n\r\n\r\n\r\n###==================================================sdu p2=============================================================\r\n\r\n#Add new column in isp_corrected -- HouseNo + G_Street_Name_1 + G_City = HNUM_STRT_TM\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected[\"Street_1\"].map(str) + \" ,\" + isp_corrected[\"STD_CITY\"].map(str)\r\n\r\n# Clean HNUM_STRT_TM column\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected['HNUM_STRT_TM'].str.replace('nan ','', case = False)\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected['HNUM_STRT_TM'].str.replace('[,.]','', case = False)\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected[\"HNUM_STRT_TM\"].str.replace(\" \",\"\")\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected[\"HNUM_STRT_TM\"].str.replace(\",\",\"\")\r\nisp_corrected[\"HNUM_STRT_TM\"] = isp_corrected[\"HNUM_STRT_TM\"].str.replace(\"\\.\",\"\")\r\n\r\n# Capitalize HNUM_STRT_TM\r\nisp_corrected['HNUM_STRT_TM'] = isp_corrected['HNUM_STRT_TM'].str.upper() \r\n\r\n# Remove nulls in HNUM_STRT and HNUM_STRT_TM\r\nisp_corrected = isp_corrected[isp_corrected.HNUM_STRT_TM.notnull()]\r\n\r\n## ### RPA Base generation - to be saved in RPA bucket\r\n#\r\n#### Getting the ISP fields\r\n#from datetime import datetime\r\n#now = datetime.now()\r\n## Capitalize HNUM_STRT_TM\r\n\r\nif ISP_Name == 'TM':\r\n\r\n    ### ----- below codes are commented out as we eventually decided to go with the old mapping key which is HNUM_STRT_TM instead of Mix_key (Zohreh tested). So below codes are just duplicates of original code (OLD_CODE) but Zohreh wanted to test out Mix_key.    \r\n    # rpa_isp_corrected = isp_corrected\r\n\r\n    # rpa_isp_corrected[\"ISP_ADDRESS\"] = rpa_isp_corrected[\"BuildingName\"].map(str) + \" ,\" + rpa_isp_corrected[\"HouseNo\"].map(str) + \" ,\" + rpa_isp_corrected[\"StreetType\"].map(str) + \" ,\" + rpa_isp_corrected[\"StreetName\"].map(str) + \" ,\" + rpa_isp_corrected[\"Postcode\"].map(str)+ \" ,\" + rpa_isp_corrected[\"City\"].map(str)+ \" ,\" + rpa_isp_corrected[\"State\"].map(str)\r\n    \r\n    # rpa_isp_corrected['TM_HOUSE_NO1']= rpa_isp_corrected['HouseNo'].str.pad(width=10)\r\n    \r\n    # New_Columns = rpa_isp_corrected.drop_duplicates(subset=['HouseNo','StreetType','Street_1','Postcode','STD_CITY','State'],\r\n    #                                                       keep='first')\r\n    \r\n    # distinct_sdu_p2 = New_Columns[['HNUM_STRT_TM','TM_HOUSE_NO1','HouseNo','BuildingName',\r\n    #                                   'StreetType','StreetName',\r\n    #                                   'Section','Postcode','City',\r\n    #                                   'State','ServiceType', 'ISP_ADDRESS']]\r\n    \r\n    \r\n    \r\n    # distinct_sdu_p2['HOUSE_NO_TM'] = distinct_sdu_p2['TM_HOUSE_NO1'].astype(str)\r\n    # distinct_sdu_p2 = distinct_sdu_p2.drop('TM_HOUSE_NO1',axis=1)\r\n    \r\n    # distinct_sdu_p2[\"Mix_key\"] = distinct_sdu_p2[\"BuildingName\"].map(str) + \" ,\" + distinct_sdu_p2[\"HNUM_STRT_TM\"].map(str)\r\n        \r\n    # distinct_sdu_p2 = distinct_sdu_p2.drop_duplicates(subset=['Mix_key'], keep='first')\r\n\r\n\r\n    \r\n    # # Rerrange columns\r\n    # distinct_sdu_p2 = distinct_sdu_p2[['HOUSE_NO_TM', 'BuildingName', 'StreetType', 'StreetName', 'Section',\r\n    #                                       'Postcode', 'City', 'State', 'ServiceType', 'ISP_ADDRESS','HNUM_STRT_TM',\r\n    #                                   'Mix_key'\r\n    #                                       ]]\r\n    \r\n    # print(distinct_sdu_p2.shape)\r\n\r\n    # #distinct_sdu_p2.to_csv('Distinct_Fields_P2_SDU_Mar.csv',index=False)\r\n\r\n    # wr.s3.to_csv(df = distinct_sdu_p2, path = rpa_bucket_path + 'Distinct_Fields_P2_SDU_20221013.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename\r\n    # wr.s3.to_csv(df = distinct_sdu_p2, path = 's3://astro-groupdata-prod-target/rpa/' + 'Distinct_Fields_P2_SDU_20221013.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename\r\n    # wr.s3.to_csv(df = distinct_sdu_p2, path = 's3://astro-groupdata-prod-source/sftp/rpa/' + 'Distinct_Fields_P2_SDU_20221013.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename\r\n    \r\n###============= Original Mapping Codes =================down here=====================================================================================\r\n    \r\n    rpa_isp_corrected = isp_corrected\r\n\r\n    rpa_isp_corrected[\"ISP_ADDRESS\"] = rpa_isp_corrected[\"BuildingName\"].map(str) + \" ,\" + rpa_isp_corrected[\"HouseNo\"].map(str) + \" ,\" + rpa_isp_corrected[\"StreetType\"].map(str) + \" ,\" + rpa_isp_corrected[\"StreetName\"].map(str) + \" ,\" + rpa_isp_corrected[\"Postcode\"].map(str)+ \" ,\" + rpa_isp_corrected[\"City\"].map(str)+ \" ,\" + rpa_isp_corrected[\"State\"].map(str)+ \" ,\" + rpa_isp_corrected[\"Street_1\"].map(str)+ \" ,\" + rpa_isp_corrected[\"Combined_Building\"].map(str)\r\n    \r\n    rpa_isp_corrected['TM_HOUSE_NO1']= rpa_isp_corrected['HouseNo'].str.pad(width=10)\r\n    \r\n    New_Columns = rpa_isp_corrected.drop_duplicates(subset=['HouseNo','StreetType','Street_1','Postcode','STD_CITY','State'],\r\n                                                          keep='first')\r\n    \r\n    distinct_sdu_p2 = New_Columns[['HNUM_STRT_TM','TM_HOUSE_NO1','HouseNo','BuildingName',\r\n                                      'StreetType','StreetName',\r\n                                      'Section','Postcode','City',\r\n                                      'State','ServiceType', 'ISP_ADDRESS']]\r\n    \r\n    distinct_sdu_p2['HOUSE_NO_TM'] = distinct_sdu_p2['TM_HOUSE_NO1'].astype(str)\r\n    distinct_sdu_p2 = distinct_sdu_p2.drop('TM_HOUSE_NO1',axis=1)\r\n    \r\n    # Rerrange columns\r\n    distinct_sdu_p2 = distinct_sdu_p2[['HOUSE_NO_TM', 'BuildingName', 'StreetType', 'StreetName', 'Section',\r\n                                           'Postcode', 'City', 'State', 'ServiceType', 'ISP_ADDRESS','HNUM_STRT_TM'\r\n                                           ]]\r\n    \r\n    print(distinct_sdu_p2.shape)\r\n    #distinct_sdu_p2.to_csv('Distinct_Fields_P2_SDU_Mar_V1.csv',index=False)\r\n    wr.s3.to_csv(df = distinct_sdu_p2, path = rpa_bucket_path + 'historical_folder/Distinct_Fields_P2_SDU_' + str(curr_date) + '.csv.gz', index = False, compression='gzip') ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: added historical_folder and added curr_date. 22/11/22: added gzip compression\r\n    wr.s3.to_csv(df = distinct_sdu_p2, path = 's3://astro-groupdata-prod-target/rpa/' + 'Distinct_Fields_P2_SDU.csv.gz',index=False, compression='gzip') ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: remove curr_date to allow for automation. 22/11/22: added gzip compression\r\n    wr.s3.to_csv(df = distinct_sdu_p2, path = 's3://astro-groupdata-prod-source/sftp/rpa/' + 'Distinct_Fields_P2_SDU.csv.gz',index=False, compression='gzip') ## --> 13/10/2022 Amzar: added \"_20221013\" to filename # 18/11/22: remove curr_date to allow for automation. 22/11/22: added gzip compression\r\n    \r\n    ### --- 18/11/22: originally saved the below files as 'old_code' because Zohreh introduced the mix_key into this notebook to strengthen the mapping, but it reduced P1P2 numbers too much that we decided to go back to the original mapping method and save it above as the normal files\r\n    # wr.s3.to_csv(df = distinct_sdu_p2, path = rpa_bucket_path + 'Distinct_Fields_P2_SDU_old_code_20221013.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename\r\n    # wr.s3.to_csv(df = distinct_sdu_p2, path = 's3://astro-groupdata-prod-target/rpa/' + 'Distinct_Fields_P2_SDU_old_code_20221013.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename\r\n    # wr.s3.to_csv(df = distinct_sdu_p2, path = 's3://astro-groupdata-prod-source/sftp/rpa/' + 'Distinct_Fields_P2_SDU_old_code_20221013.csv',index=False) ## --> 13/10/2022 Amzar: added \"_20221013\" to filename\r\n    \r\nelse:\r\n    print('Not TM')\r\n\r\nusage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\nprint('[debug] memory usage is (Megabytes):')\r\nprint(usage)"
}