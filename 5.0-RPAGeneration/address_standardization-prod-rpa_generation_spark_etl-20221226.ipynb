{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "# Glue Studio Notebook\n",
    "You are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n",
    "\n",
    "## Available Magics\n",
    "|          Magic              |   Type       |                                                                        Description                                                                        |\n",
    "|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n",
    "| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n",
    "| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n",
    "| %region                     |  String      |  Specify the AWS region in which to initialize a session                                                                                                  |\n",
    "| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n",
    "| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n",
    "| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n",
    "| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n",
    "| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n",
    "| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n",
    "| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X                                                                            |\n",
    "| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0)                                |\n",
    "| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n",
    "| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n",
    "| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n",
    "| %etl                        |  String      |   Changes the session type to Glue ETL.                                                                                                                   |\n",
    "| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n",
    "| %stop_session               |              |  Stops the current session.                                                                                                                               |\n",
    "| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n",
    "| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Glue Interactive Sessions Kernel\n",
      "For more information on available magic commands, please type %help in any new cell.\n",
      "\n",
      "Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
      "Installed kernel version: 0.35 \n",
      "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::741993363917:role/AWSGlueServiceRole\n",
      "Trying to create a Glue session for the kernel.\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 5\n",
      "Session ID: 0cf836e0-7fcf-43f0-ac3b-87bd40b14084\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 0.35\n",
      "--enable-glue-datacatalog true\n",
      "Waiting for session 0cf836e0-7fcf-43f0-ac3b-87bd40b14084 to get into ready status...\n",
      "Session 0cf836e0-7fcf-43f0-ac3b-87bd40b14084 has been created\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20221118\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from datetime import datetime\n",
    "# now = datetime.now()\n",
    "\n",
    "## Amzar 18/11/2022 -> added a datetime key for tracking file creation\n",
    "# curr_date = str(datetime.today().strftime('%Y%m%d'))\n",
    "curr_date = '20221118' # temporary assignment\n",
    "print(curr_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "ISP_Name = 'TM'\n",
    "\n",
    "# distinct_mdu_p1_read_path = 's3://astro-groupdata-prod-source/rpa/Distinct_Fields_P1_MDU_old_code_20221013.csv' #18/11/22: commented out & added lines to read from 'automated' files (plus removed the old_code part for distinct file) \n",
    "# mdu_p1_read_path = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_mdu/UAMS_Format_stndrd_TM_P1_MDU20221014.csv' # 18/11/22: commented out & changed to latest address UAMS Format file\n",
    "\n",
    "distinct_mdu_p1_read_path = 's3://astro-groupdata-prod-target/rpa/Distinct_Fields_P1_MDU.csv.gz' # 22/11/22: added .gz to read_path\n",
    "mdu_p1_read_path = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_mdu/UAMS_Format_stndrd_TM_P1_MDU_20221118.csv' # need to automate\n",
    "\n",
    "distinct_sdu_p1_read_path = 's3://astro-groupdata-prod-target/rpa/Distinct_Fields_P1_SDU.csv.gz' # 22/11/22: added .gz to read_path\n",
    "sdu_p1_read_path = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_sdu/UAMS_Format_stndrd_TM_P1_SDU_20221118.csv'  # need to automate\n",
    "\n",
    "distinct_sdu_p2_read_path = 's3://astro-groupdata-prod-target/rpa/Distinct_Fields_P2_SDU.csv.gz' # 22/11/22: added .gz to read_path\n",
    "sdu_p2_read_path = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_sdu/UAMS_Format_stndrd_TM_P2_SDU_20221118.csv'  # need to automate\n",
    "# new_p1_save_path = args['new_p1_save_path']\n",
    "# new_p2_save_path = args['new_p2_save_path']\n",
    "\n",
    "# print(distinct_mdu_p1_read_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Distinct mdu p1 read\n",
    "\n",
    "distinct_mdu_p1 = glueContext.create_dynamic_frame_from_options(\n",
    "\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths' : [distinct_mdu_p1_read_path]},\n",
    "    format = 'csv',\n",
    "    format_options = {'withHeader':True}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Distinct sdu p1 read \n",
    "distinct_sdu_p1 = glueContext.create_dynamic_frame_from_options(\n",
    "\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths' : [distinct_sdu_p1_read_path]},\n",
    "    format = 'csv',\n",
    "    format_options = {'withHeader':True}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Distinct sdu p2 read\n",
    "distinct_sdu_p2 = glueContext.create_dynamic_frame_from_options(\n",
    "\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths' : [distinct_sdu_p2_read_path]},\n",
    "    format = 'csv',\n",
    "    format_options = {'withHeader':True}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#MDU P1\n",
    "P1_MDU = glueContext.create_dynamic_frame_from_options(\n",
    "\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths' : [mdu_p1_read_path]},\n",
    "    format = 'csv',\n",
    "    format_options = {'withHeader':True}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SDU P1\n",
    "P1_SDU  = glueContext.create_dynamic_frame_from_options(\n",
    "\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths' : [sdu_p1_read_path]},\n",
    "    format = 'csv',\n",
    "    format_options = {'withHeader':True}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SDU P2\n",
    "P2_SDU  = glueContext.create_dynamic_frame_from_options(\n",
    "\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths' : [sdu_p2_read_path]},\n",
    "    format = 'csv',\n",
    "    format_options = {'withHeader':True}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_mdu_p1 = distinct_mdu_p1.toDF()\n",
    "distinct_sdu_p1 = distinct_sdu_p1.toDF()\n",
    "distinct_sdu_p2 = distinct_sdu_p2.toDF()\n",
    "P1_MDU = P1_MDU.toDF()\n",
    "P1_SDU = P1_SDU.toDF()\n",
    "P2_SDU = P2_SDU.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----first do P1 MDU-----\n",
      "3281168 181324\n",
      "Shape after merging (kl new fields 1) : 71167945\n",
      "this is kl new fields cols : ['HNUM_STRT_TM', 'Account_No', 'service_add_objid', 'House_No', 'AREA', 'Mix_key', 'BuildingName', 'StreetType', 'StreetName', 'Section', 'City', 'State', 'Postcode', 'ServiceType']\n",
      "+--------------------------+\n",
      "|count(DISTINCT Account_No)|\n",
      "+--------------------------+\n",
      "|                    181324|\n",
      "+--------------------------+\n",
      "\n",
      "unique acc_no before de-dupe (KL New fields 2) None\n",
      "+--------------------------+\n",
      "|count(DISTINCT ACCOUNT_ID)|\n",
      "+--------------------------+\n",
      "|                    181324|\n",
      "+--------------------------+\n",
      "\n",
      "unique acc_no after de-dupe (KL New fields 3) None\n",
      "180916\n",
      "180916\n",
      "180916\n"
     ]
    }
   ],
   "source": [
    "#======================================================distinct mdu p1 ==================================\n",
    "print('-----first do P1 MDU-----')\n",
    "\n",
    "#Fakhrul - 18/10/22 - same as above, helping amzar out here, removed mix key bcs we wanna run without mix key\n",
    "## Select columns to reduce computational time\n",
    "distinct_mdu_p1 = distinct_mdu_p1.select(['BuildingName','StreetType','StreetName','Section','City',\n",
    "                                   'State','Postcode','ServiceType','HNUM_STRT_TM'])\n",
    "                                   \n",
    "P1_MDU = P1_MDU.withColumn(\"Mix_key\", f.concat_ws(\" ,\", f.col(\"Street_1_New\") + f.col(\"HNUM_STRT_TM\")) )\n",
    "    \n",
    "P1_MDU = P1_MDU.select(['Account_No', 'service_add_objid', 'House_No','AREA', 'HNUM_STRT_TM', 'Mix_key'])\n",
    "\n",
    "P1_MDU = P1_MDU.withColumn('AREA', f.regexp_replace('AREA', 'AA.{3}', '') )\n",
    "# P1_MDU[\"AREA\"]= np.where(P1_MDU[\"AREA\"].astype(str).str.startswith(\"AA\"), '', P1_MDU[\"AREA\"])\n",
    "\n",
    "P1_MDU = P1_MDU.withColumn( 'Account_No',  f.col('Account_No').cast('string') ).withColumn( 'Account_No',  f.regexp_replace('Account_No', '\\.0', '') )\n",
    "\n",
    "ISP_UNIQUE_WS = distinct_mdu_p1\n",
    "STRT_P1 = P1_MDU\n",
    "\n",
    "print(ISP_UNIQUE_WS.count(), STRT_P1.count()) # 3195117 179663\n",
    "\n",
    "#revision - 24/8/22 zohreh amzar see if it works\n",
    "#KL_New_fields1 = STRT_P1.merge(ISP_UNIQUE_WS,on='Mix_key', how = 'left')\n",
    "# KL_New_fields1 = STRT_P1.merge(ISP_UNIQUE_WS,on='HNUM_STRT_TM', how = 'left')\n",
    "KL_New_fields1 = STRT_P1.join(ISP_UNIQUE_WS, on='HNUM_STRT_TM', how = 'left')\n",
    "\n",
    "print('Shape after merging (kl new fields 1) :', KL_New_fields1.count()) # 70821640\n",
    "print('this is kl new fields cols :', KL_New_fields1.columns)\\\n",
    "\n",
    "# Creating new location flag for postcodes with both urban and rural\n",
    "\n",
    "KL_New_fields1 = KL_New_fields1.withColumn('address_type', when(f.col('BuildingName').isNull(), 'SDU').when(f.col('BuildingName').isNotNull(), 'MDU').otherwise('none') )\n",
    "\n",
    "## Add ASTRO_BLOCK column for RPA\n",
    "KL_New_fields1 = KL_New_fields1.withColumn('ASTRO_BLOCK', f.lit(''))\n",
    "KL_New_fields1 = KL_New_fields1.withColumn('ASTRO_CONDO_NAME', f.col('BuildingName'))\n",
    "\n",
    "### Selecting and renaming columns\n",
    "#revision - 24/8/22 zohreh amzar disabling this below to try the below of below\n",
    "#revision - 18/10/22 fakhrul amzar disabling this to try below\n",
    "#KL_New_fields2 = KL_New_fields1[['Account_No','service_add_objid','address_type','House_No','ASTRO_CONDO_NAME',\n",
    "                                #'ASTRO_BLOCK','AREA','BuildingName','StreetType','StreetName',\n",
    "                                #'Section','City','State','Postcode','ServiceType','HNUM_STRT_TM_y']]\n",
    "\n",
    "KL_New_fields2 = KL_New_fields1.select(['Account_No','service_add_objid','address_type','House_No','ASTRO_CONDO_NAME',\n",
    "                                'ASTRO_BLOCK','AREA','BuildingName','StreetType','StreetName',\n",
    "                                'Section','City','State','Postcode','ServiceType','HNUM_STRT_TM'])\n",
    "\n",
    "## De-dupe on Account_No, keep first (doesn't seem to be a clear reason why we keep first, so I'm gonna ignore that for now\n",
    "# KL_New_fields3 = KL_New_fields2.drop_duplicates(subset= 'Account_No', keep = 'first')\n",
    "KL_New_fields3 = KL_New_fields2.dropDuplicates(subset=['Account_No'])\n",
    "print('KL_New_fields3 after dedupe on Acc No :', KL_New_fields3.filter(f.col('ServiceType').isNotNull()).select('HNUM_STRT_TM').count()) # \n",
    "\n",
    "#revision - 24/8/22 zohreh amzar disabling this below to try the below of below\n",
    "#revision - 18/10/22 fakhrul amzar disabling this below\n",
    "#KL_New_fields3 = KL_New_fields3[KL_New_fields3['HNUM_STRT_TM_y'].notnull()]\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('HNUM_STRT_TM').isNotNull())\n",
    "print('KL_New_fields3 after removing null HNUM_STRT_TM :', KL_New_fields3.filter(f.col('ServiceType').isNotNull()).select('HNUM_STRT_TM').count()) \n",
    "\n",
    "## rename columns using pyspark method\n",
    "KL_New_fields3 = KL_New_fields3.toDF(*['ACCOUNT_ID', 'CRM_OBJID', 'DTYPE', 'ASTRO_HOUSE_NO', 'ASTRO_CONDO_NAME', 'ASTRO_BLOCK', 'ASTRO_AREA', 'TM_CONDO', \n",
    "                                        'TM_STREET_TYPE', 'TM_STREET_NAME', 'TM_AREA', 'TM_CITY', 'TM_STATE', 'TM_POSTCODE', 'ISP_INDICATOR', 'HNUM_STRT_TM'])\n",
    "\n",
    "print('unique acc_no before de-dupe (KL New fields 2)', KL_New_fields2.select(f.countDistinct('Account_No')).show()) # 179659\n",
    "print('unique acc_no after de-dupe (KL New fields 3)', KL_New_fields3.select(f.countDistinct('ACCOUNT_ID')).show()) # 179296\n",
    "\n",
    "# Rearranging column and pad house number\n",
    "KL_New_fields3 = KL_New_fields3.select('ACCOUNT_ID','CRM_OBJID', 'DTYPE', 'ASTRO_HOUSE_NO', 'ASTRO_CONDO_NAME','ASTRO_BLOCK',\n",
    "                                 'ASTRO_AREA','TM_CONDO','TM_STREET_TYPE','TM_STREET_NAME',\n",
    "                                 'TM_AREA', 'TM_CITY', 'TM_STATE', 'TM_POSTCODE','ISP_INDICATOR' )\n",
    "\n",
    "KL_New_fields3 = KL_New_fields3.withColumn('ASTRO_HOUSE_NO', f.lpad(f.col('ASTRO_HOUSE_NO'), 10, ' ') )\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('ISP_INDICATOR') != '')\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('ISP_INDICATOR').isNotNull())\n",
    "\n",
    "print('ISP INDICATOR not null:', KL_New_fields3.count()) # 1639699\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('TM_STATE').isNotNull())\n",
    "\n",
    "print('TM STATE not null:',KL_New_fields3.count()) # 1629700\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('TM_POSTCODE').isNotNull())\n",
    "\n",
    "print('Final count:', KL_New_fields3.count()) # 1629700\n",
    "\n",
    "KL_New_fields3 = KL_New_fields3.withColumn('TM_POSTCODE', f.col('TM_POSTCODE').cast('string')).withColumn('TM_POSTCODE', f.regexp_replace('TM_POSTCODE', '\\.0', '') )\n",
    "KL_New_fields3 = KL_New_fields3.withColumn( 'TM_POSTCODE', f.substring(f.col('TM_POSTCODE'), 1, 5) )\n",
    "KL_New_fields3 = KL_New_fields3.withColumn('TM_POSTCODE', f.lpad(f.col('TM_POSTCODE'), 5, '0') )\n",
    "# KL_New_fields3['TM_POSTCODE']= KL_New_fields3['TM_POSTCODE'].astype(str).replace('00000', '', regex=True)\n",
    "\n",
    "rpa_p1_mdu = KL_New_fields3\n",
    "\n",
    "## ----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# print(distinct_sdu_p1.filter(f.col('ServiceType').isNull()).count()) # 0\n",
    "print(distinct_sdu_p1.filter(f.col('ServiceType') == '').count())\n",
    "# filter(f.col('ISP_INDICATOR') != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----now for P1 SDU-----\n",
      "checking p1 sdu:  1671222\n",
      "3281168 1671222\n",
      "Shape after merging (kl new fields 1) : 1671290\n",
      "count of non-null left join : 72\n",
      "this is kl new fields cols : ['HNUM_STRT_TM', 'Account_No', 'service_add_objid', 'House_No', 'AREA', 'Mix_key', 'BuildingName', 'StreetType', 'StreetName', 'Section', 'City', 'State', 'Postcode', 'ServiceType']\n",
      "KL_New_fields3 after dedupe on Acc No : 4\n",
      "KL_New_fields3 after removing null HNUM_STRT_TM : 4\n",
      "+--------------------------+\n",
      "|count(DISTINCT Account_No)|\n",
      "+--------------------------+\n",
      "|                   1671222|\n",
      "+--------------------------+\n",
      "\n",
      "unique acc_no before de-dupe (KL New fields 2) None\n",
      "+--------------------------+\n",
      "|count(DISTINCT ACCOUNT_ID)|\n",
      "+--------------------------+\n",
      "|                   1671222|\n",
      "+--------------------------+\n",
      "\n",
      "unique acc_no after de-dupe & remove nulls (KL New fields 3) None\n",
      "ISP INDICATOR not null: 4\n",
      "TM STATE not null: 4\n",
      "Final count: 4\n"
     ]
    }
   ],
   "source": [
    "## ======================# P1 SDU =================================\n",
    "print('-----now for P1 SDU-----')\n",
    "print('checking p1 sdu: ', P1_SDU.count()) # 1,639,756 rows\n",
    "\n",
    "## Select columns to reduce computational time\n",
    "#distinct_sdu_p1 = distinct_sdu_p1[['BuildingName','StreetType','StreetName','Section','City',\n",
    "                                   #'State','Postcode','ServiceType','HNUM_STRT_TM','Mix_key']]\n",
    "                                   \n",
    "#Fakhrul - 18/10/22 - same as above, helping amzar out here, removed mix key bcs we wanna run without mix key\n",
    "distinct_sdu_p1 = distinct_sdu_p1.select(['BuildingName','StreetType','StreetName','Section','City',\n",
    "                                   'State','Postcode','ServiceType','HNUM_STRT_TM'])\n",
    "                                   \n",
    "P1_SDU = P1_SDU.withColumn(\"Mix_key\", f.concat_ws(\" ,\", f.col(\"Combined_Building\") + f.col(\"HNUM_STRT_TM\")) )\n",
    "    \n",
    "P1_SDU = P1_SDU.select(['Account_No', 'service_add_objid', 'House_No','AREA', 'HNUM_STRT_TM', 'Mix_key'])\n",
    "\n",
    "P1_SDU = P1_SDU.withColumn('AREA', f.regexp_replace('AREA', 'AA.{3}', '') )\n",
    "# P1_SDU[\"AREA\"]= np.where(P1_SDU[\"AREA\"].astype(str).str.startswith(\"AA\"), '', P1_SDU[\"AREA\"])\n",
    "\n",
    "P1_SDU = P1_SDU.withColumn( 'Account_No',  f.col('Account_No').cast('string') ).withColumn( 'Account_No',  f.regexp_replace('Account_No', '\\.0', '') )\n",
    "\n",
    "ISP_UNIQUE_WS = distinct_sdu_p1\n",
    "STRT_P1 = P1_SDU\n",
    "\n",
    "print(ISP_UNIQUE_WS.count(), STRT_P1.count()) # 9162197 1639756\n",
    "\n",
    "#revision - 24/8/22 zohreh amzar see if it works\n",
    "#KL_New_fields1 = STRT_P1.merge(ISP_UNIQUE_WS,on='Mix_key', how = 'left')\n",
    "# KL_New_fields1 = STRT_P1.merge(ISP_UNIQUE_WS,on='HNUM_STRT_TM', how = 'left')\n",
    "KL_New_fields1 = STRT_P1.join(ISP_UNIQUE_WS, on='HNUM_STRT_TM', how = 'left')\n",
    "\n",
    "print('Shape after merging (kl new fields 1) :', KL_New_fields1.count()) # 2440211\n",
    "print('count of non-null left join :', KL_New_fields1.filter(f.col('ServiceType').isNotNull()).select('HNUM_STRT_TM').count()) # \n",
    "print('this is kl new fields cols :', KL_New_fields1.columns)\n",
    "\n",
    "# Creating new location flag for postcodes with both urban and rural\n",
    "from pyspark.sql.functions import when\n",
    "KL_New_fields1 = KL_New_fields1.withColumn('address_type', when(f.col('BuildingName').isNull(), 'SDU').when(f.col('BuildingName').isNotNull(), 'MDU').otherwise('none') )\n",
    "\n",
    "## Add ASTRO_BLOCK column for RPA\n",
    "KL_New_fields1 = KL_New_fields1.withColumn('ASTRO_BLOCK', f.lit(''))\n",
    "KL_New_fields1 = KL_New_fields1.withColumn('ASTRO_CONDO_NAME', f.col('BuildingName'))\n",
    "\n",
    "### Selecting and renaming columns\n",
    "#revision - 24/8/22 zohreh amzar disabling this below to try the below of below\n",
    "#revision - 18/10/22 fakhrul amzar disabling this to try below\n",
    "#KL_New_fields2 = KL_New_fields1[['Account_No','service_add_objid','address_type','House_No','ASTRO_CONDO_NAME',\n",
    "                                #'ASTRO_BLOCK','AREA','BuildingName','StreetType','StreetName',\n",
    "                                #'Section','City','State','Postcode','ServiceType','HNUM_STRT_TM_y']]\n",
    "\n",
    "KL_New_fields2 = KL_New_fields1.select(['Account_No','service_add_objid','address_type','House_No','ASTRO_CONDO_NAME',\n",
    "                                'ASTRO_BLOCK','AREA','BuildingName','StreetType','StreetName',\n",
    "                                'Section','City','State','Postcode','ServiceType','HNUM_STRT_TM'])\n",
    "\n",
    "## De-dupe on Account_No, keep first (doesn't seem to be a clear reason why we keep first, so I'm gonna ignore that for now\n",
    "# KL_New_fields3 = KL_New_fields2.drop_duplicates(subset= 'Account_No', keep = 'first')\n",
    "KL_New_fields3 = KL_New_fields2.dropDuplicates(subset=['Account_No'])\n",
    "print('KL_New_fields3 after dedupe on Acc No :', KL_New_fields3.filter(f.col('ServiceType').isNotNull()).select('HNUM_STRT_TM').count()) # \n",
    "\n",
    "#revision - 24/8/22 zohreh amzar disabling this below to try the below of below\n",
    "#revision - 18/10/22 fakhrul amzar disabling this below\n",
    "#KL_New_fields3 = KL_New_fields3[KL_New_fields3['HNUM_STRT_TM_y'].notnull()]\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('HNUM_STRT_TM').isNotNull())\n",
    "print('KL_New_fields3 after removing null HNUM_STRT_TM :', KL_New_fields3.filter(f.col('ServiceType').isNotNull()).select('HNUM_STRT_TM').count()) # \n",
    "\n",
    "## rename columns using pyspark method\n",
    "KL_New_fields3 = KL_New_fields3.toDF(*['ACCOUNT_ID', 'CRM_OBJID', 'DTYPE', 'ASTRO_HOUSE_NO', 'ASTRO_CONDO_NAME', 'ASTRO_BLOCK', 'ASTRO_AREA', 'TM_CONDO', \n",
    "                                        'TM_STREET_TYPE', 'TM_STREET_NAME', 'TM_AREA', 'TM_CITY', 'TM_STATE', 'TM_POSTCODE', 'ISP_INDICATOR', 'HNUM_STRT_TM'])\n",
    "\n",
    "print('unique acc_no before de-dupe (KL New fields 2)', KL_New_fields2.select(f.countDistinct('Account_No')).show()) # 1639729\n",
    "print('unique acc_no after de-dupe & remove nulls (KL New fields 3)', KL_New_fields3.select(f.countDistinct('ACCOUNT_ID')).show()) # 1639699\n",
    "\n",
    "# Rearranging column and pad house number\n",
    "KL_New_fields3 = KL_New_fields3.select('ACCOUNT_ID','CRM_OBJID', 'DTYPE', 'ASTRO_HOUSE_NO', 'ASTRO_CONDO_NAME','ASTRO_BLOCK',\n",
    "                                 'ASTRO_AREA','TM_CONDO','TM_STREET_TYPE','TM_STREET_NAME',\n",
    "                                 'TM_AREA', 'TM_CITY', 'TM_STATE', 'TM_POSTCODE','ISP_INDICATOR' )\n",
    "\n",
    "KL_New_fields3 = KL_New_fields3.withColumn('ASTRO_HOUSE_NO', f.lpad(f.col('ASTRO_HOUSE_NO'), 10, ' ') )\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('ISP_INDICATOR') != '')\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('ISP_INDICATOR').isNotNull())\n",
    "\n",
    "print('ISP INDICATOR not null:', KL_New_fields3.count()) # 1639699\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('TM_STATE').isNotNull())\n",
    "\n",
    "print('TM STATE not null:',KL_New_fields3.count()) # 1629700\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('TM_POSTCODE').isNotNull())\n",
    "\n",
    "print('Final count:', KL_New_fields3.count()) # 1629700\n",
    "\n",
    "KL_New_fields3 = KL_New_fields3.withColumn('TM_POSTCODE', f.col('TM_POSTCODE').cast('string')).withColumn('TM_POSTCODE', f.regexp_replace('TM_POSTCODE', '\\.0', '') )\n",
    "KL_New_fields3 = KL_New_fields3.withColumn( 'TM_POSTCODE', f.substring(f.col('TM_POSTCODE'), 1, 5) )\n",
    "KL_New_fields3 = KL_New_fields3.withColumn('TM_POSTCODE', f.lpad(f.col('TM_POSTCODE'), 5, '0') )\n",
    "# KL_New_fields3['TM_POSTCODE']= KL_New_fields3['TM_POSTCODE'].astype(str).replace('00000', '', regex=True)\n",
    "\n",
    "rpa_p1_sdu = KL_New_fields3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----now for P2 SDU-----\n",
      "756968 1766822\n",
      "Shape after merging (kl new fields 1) : 62332947\n",
      "this is kl new fields cols : ['HNUM_STRT_TM', 'Account_No', 'service_add_objid', 'House_No', 'AREA', 'Mix_key', 'BuildingName', 'StreetType', 'StreetName', 'Section', 'City', 'State', 'Postcode', 'ServiceType']\n",
      "+--------------------------+\n",
      "|count(DISTINCT Account_No)|\n",
      "+--------------------------+\n",
      "|                   1766822|\n",
      "+--------------------------+\n",
      "\n",
      "unique acc_no before de-dupe (KL New fields 2) None\n",
      "+--------------------------+\n",
      "|count(DISTINCT ACCOUNT_ID)|\n",
      "+--------------------------+\n",
      "|                   1766822|\n",
      "+--------------------------+\n",
      "\n",
      "unique acc_no after de-dupe (KL New fields 3) None\n",
      "1766817\n",
      "1766817\n",
      "1766817\n"
     ]
    }
   ],
   "source": [
    "## ======================# P2 SDU =================================\n",
    "\n",
    "print('-----now for P2 SDU-----')\n",
    "## Select columns to reduce computational time\n",
    "#distinct_sdu_p2 = distinct_sdu_p2[['BuildingName','StreetType','StreetName','Section','City',\n",
    "                                   #'State','Postcode','ServiceType','HNUM_STRT_TM','Mix_key']]\n",
    "                                   \n",
    "#Fakhrul - 18/10/22 - same as above, helping amzar out here, removed mix key bcs we wanna run without mix key\n",
    "distinct_sdu_p2 = distinct_sdu_p2.select(['BuildingName','StreetType','StreetName','Section','City',\n",
    "                                   'State','Postcode','ServiceType','HNUM_STRT_TM'])\n",
    "                                   \n",
    "P2_SDU = P2_SDU.withColumn(\"Mix_key\", f.concat_ws(\" ,\", f.col(\"Combined_Building\") + f.col(\"HNUM_STRT_TM\")) )\n",
    "    \n",
    "P2_SDU = P2_SDU.select(['Account_No', 'service_add_objid', 'House_No','AREA', 'HNUM_STRT_TM', 'Mix_key'])\n",
    "\n",
    "P2_SDU = P2_SDU.withColumn('AREA', f.regexp_replace('AREA', 'AA.{3}', '') )\n",
    "# P2_SDU[\"AREA\"]= np.where(P2_SDU[\"AREA\"].astype(str).str.startswith(\"AA\"), '', P2_SDU[\"AREA\"])\n",
    "\n",
    "P2_SDU = P2_SDU.withColumn( 'Account_No',  f.col('Account_No').cast('string') ).withColumn( 'Account_No',  f.regexp_replace('Account_No', '\\.0', '') )\n",
    "\n",
    "# ## this step is important for P2 SDU especially since got many duplicates. Amzar 18/10/2022 -> added dedupe to the variable name\n",
    "P2_SDU_dedupe = P2_SDU.drop_duplicates() # 1759949\n",
    "distinct_sdu_p2_dedupe = distinct_sdu_p2.drop_duplicates() # 725325 (if on HNUM_STRT_TM, only 201932 are left)\n",
    "\n",
    "\n",
    "ISP_UNIQUE_WS = distinct_sdu_p2_dedupe # Amzar 18/10/2022 -> added dedupe to the variable name\n",
    "STRT_P1 = P2_SDU_dedupe # Amzar 18/10/2022 -> added dedupe to the variable name\n",
    "print(ISP_UNIQUE_WS.count(), STRT_P1.count()) # 725325 1759949\n",
    "\n",
    "#revision - 24/8/22 zohreh amzar see if it works\n",
    "#KL_New_fields1 = STRT_P1.merge(ISP_UNIQUE_WS,on='Mix_key', how = 'left')\n",
    "# KL_New_fields1 = STRT_P1.merge(ISP_UNIQUE_WS,on='HNUM_STRT_TM', how = 'left')\n",
    "KL_New_fields1 = STRT_P1.join(ISP_UNIQUE_WS, on='HNUM_STRT_TM', how = 'left')\n",
    "\n",
    "print('Shape after merging (kl new fields 1) :', KL_New_fields1.count()) # 60,132,254 rows\n",
    "print('this is kl new fields cols :', KL_New_fields1.columns)\n",
    "\n",
    "# Creating new location flag for postcodes with both urban and rural\n",
    "KL_New_fields1 = KL_New_fields1.withColumn('address_type', when(f.col('BuildingName').isNull(), 'SDU').when(f.col('BuildingName').isNotNull(), 'MDU').otherwise('none') )\n",
    "\n",
    "## Add ASTRO_BLOCK column for RPA\n",
    "KL_New_fields1 = KL_New_fields1.withColumn('ASTRO_BLOCK', f.lit(''))\n",
    "KL_New_fields1 = KL_New_fields1.withColumn('ASTRO_CONDO_NAME', f.col('BuildingName'))\n",
    "\n",
    "### Selecting and renaming columns\n",
    "#revision - 24/8/22 zohreh amzar disabling this below to try the below of below\n",
    "#revision - 18/10/22 fakhrul amzar disabling this to try below\n",
    "#KL_New_fields2 = KL_New_fields1[['Account_No','service_add_objid','address_type','House_No','ASTRO_CONDO_NAME',\n",
    "                                #'ASTRO_BLOCK','AREA','BuildingName','StreetType','StreetName',\n",
    "                                #'Section','City','State','Postcode','ServiceType','HNUM_STRT_TM_y']]\n",
    "\n",
    "KL_New_fields2 = KL_New_fields1.select(['Account_No','service_add_objid','address_type','House_No','ASTRO_CONDO_NAME',\n",
    "                                'ASTRO_BLOCK','AREA','BuildingName','StreetType','StreetName',\n",
    "                                'Section','City','State','Postcode','ServiceType','HNUM_STRT_TM'])\n",
    "\n",
    "## De-dupe on Account_No, keep first (doesn't seem to be a clear reason why we keep first, so I'm gonna ignore that for now\n",
    "# KL_New_fields3 = KL_New_fields2.drop_duplicates(subset= 'Account_No', keep = 'first')\n",
    "KL_New_fields3 = KL_New_fields2.dropDuplicates(subset=['Account_No'])\n",
    "print('KL_New_fields3 after dedupe on Acc No :', KL_New_fields3.filter(f.col('ServiceType').isNotNull()).select('HNUM_STRT_TM').count()) # \n",
    "\n",
    "#revision - 24/8/22 zohreh amzar disabling this below to try the below of below\n",
    "#revision - 18/10/22 fakhrul amzar disabling this below\n",
    "#KL_New_fields3 = KL_New_fields3[KL_New_fields3['HNUM_STRT_TM_y'].notnull()]\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('HNUM_STRT_TM').isNotNull())\n",
    "print('KL_New_fields3 after removing null HNUM_STRT_TM :', KL_New_fields3.filter(f.col('ServiceType').isNotNull()).select('HNUM_STRT_TM').count()) # \n",
    "\n",
    "## rename columns using pyspark method\n",
    "KL_New_fields3 = KL_New_fields3.toDF(*['ACCOUNT_ID', 'CRM_OBJID', 'DTYPE', 'ASTRO_HOUSE_NO', 'ASTRO_CONDO_NAME', 'ASTRO_BLOCK', 'ASTRO_AREA', 'TM_CONDO', \n",
    "                                        'TM_STREET_TYPE', 'TM_STREET_NAME', 'TM_AREA', 'TM_CITY', 'TM_STATE', 'TM_POSTCODE', 'ISP_INDICATOR', 'HNUM_STRT_TM'])\n",
    "\n",
    "print('unique acc_no before de-dupe (KL New fields 2)', KL_New_fields2.select(f.countDistinct('Account_No')).show()) # 1,759,944\n",
    "print('unique acc_no after de-dupe (KL New fields 3)', KL_New_fields3.select(f.countDistinct('ACCOUNT_ID')).show()) # 1,759,816\n",
    "\n",
    "# Rearranging column and pad house number\n",
    "KL_New_fields3 = KL_New_fields3.select('ACCOUNT_ID','CRM_OBJID', 'DTYPE', 'ASTRO_HOUSE_NO', 'ASTRO_CONDO_NAME','ASTRO_BLOCK',\n",
    "                                 'ASTRO_AREA','TM_CONDO','TM_STREET_TYPE','TM_STREET_NAME',\n",
    "                                 'TM_AREA', 'TM_CITY', 'TM_STATE', 'TM_POSTCODE','ISP_INDICATOR' )\n",
    "\n",
    "KL_New_fields3 = KL_New_fields3.withColumn('ASTRO_HOUSE_NO', f.lpad(f.col('ASTRO_HOUSE_NO'), 10, ' ') )\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('ISP_INDICATOR') != '')\n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('ISP_INDICATOR').isNotNull())\n",
    "\n",
    "print('ISP INDICATOR not null:', KL_New_fields3.count()) # \n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('TM_STATE').isNotNull())\n",
    "\n",
    "print('TM STATE not null:',KL_New_fields3.count()) # \n",
    "KL_New_fields3 = KL_New_fields3.filter(f.col('TM_POSTCODE').isNotNull())\n",
    "\n",
    "print('Final count:', KL_New_fields3.count()) # \n",
    "\n",
    "KL_New_fields3 = KL_New_fields3.withColumn('TM_POSTCODE', f.col('TM_POSTCODE').cast('string')).withColumn('TM_POSTCODE', f.regexp_replace('TM_POSTCODE', '\\.0', '') )\n",
    "KL_New_fields3 = KL_New_fields3.withColumn( 'TM_POSTCODE', f.substring(f.col('TM_POSTCODE'), 1, 5) )\n",
    "KL_New_fields3 = KL_New_fields3.withColumn('TM_POSTCODE', f.lpad(f.col('TM_POSTCODE'), 5, '0') )\n",
    "# KL_New_fields3['TM_POSTCODE']= KL_New_fields3['TM_POSTCODE'].astype(str).replace('00000', '', regex=True)\n",
    "\n",
    "rpa_p2_sdu = KL_New_fields3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 (after union MDU & SDU): 180920\n",
      "P1 (after dedupe on ACC_ID, keep MDU first): 180920\n",
      "checking new p1 after correcting:  180920\n",
      "P2: 1766817\n",
      "checking new p2 after correcting:  1766817\n"
     ]
    }
   ],
   "source": [
    "## ----------------------------------------------------------\n",
    "\n",
    "# ### Combining RPA format file to P1 and P2\n",
    "\n",
    "#======================================================new tm p1 ==================================\n",
    "new_p1 = rpa_p1_mdu.union(rpa_p1_sdu)\n",
    "print('P1 (after union MDU & SDU):',new_p1.count()) # 1805286\n",
    "\n",
    "## use this pyspark method to create a 'row number' that increases post-union: https://stackoverflow.com/questions/51200217/how-to-create-sequential-number-column-in-pyspark-dataframe\n",
    "new_p1 = new_p1.withColumn(\"row_idx\", f.row_number().over(Window.orderBy(f.monotonically_increasing_id())))\n",
    "## then use this pyspark method to de-dupe ACCOUNT_ID, keep first: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\n",
    "window = Window.partitionBy('ACCOUNT_ID').orderBy(f.col(\"row_idx\").asc())\n",
    "new_p1 = new_p1.withColumn('row', f.row_number().over(window)).filter(f.col('row') == 1).drop('row')\n",
    "\n",
    "print('P1 (after dedupe on ACC_ID, keep MDU first):',new_p1.count()) # 1805286\n",
    "\n",
    "#revision - 29/8/22 fakhrul adding cleaning code here as postcode is messed up due to decimal\n",
    "# print('checking new p1 before correcting: ', new_p1.info())\n",
    "# print(new_p1[['TM_POSTCODE']].head())\n",
    "\n",
    "new_p1 = new_p1.withColumn('TM_POSTCODE', f.col('TM_POSTCODE').cast('string') ).withColumn( 'TM_POSTCODE',  f.regexp_replace('TM_POSTCODE', '\\.0', '') )\n",
    "new_p1 = new_p1.withColumn( 'TM_POSTCODE', f.substring(f.col('TM_POSTCODE'), 1, 5) )\n",
    "new_p1 = new_p1.withColumn('TM_POSTCODE', f.lpad(f.col('TM_POSTCODE'), 5, '0') )\n",
    "\n",
    "print('checking new p1 after correcting: ', new_p1.count()) # 1805286\n",
    "# print(new_p1[['TM_POSTCODE']].head())\n",
    "\n",
    "#=====================================================new tm p2=========================\n",
    "\n",
    "new_p2 = rpa_p2_sdu ## Amzar 18/10/2022 -> changed the position of this line of code\n",
    "\n",
    "print('P2:',new_p2.count()) # 1747675 ## Amzar 18/10/2022 -> changed the position of this line of code \n",
    "\n",
    "# print('checking new p1 before correcting: ', new_p2.info())\n",
    "# print(new_p2[['TM_POSTCODE']].head())\n",
    "\n",
    "new_p2 = new_p2.withColumn('TM_POSTCODE', f.col('TM_POSTCODE').cast('string') ).withColumn( 'TM_POSTCODE',  f.regexp_replace('TM_POSTCODE', '\\.0', '') )\n",
    "new_p2 = new_p2.withColumn( 'TM_POSTCODE', f.substring(f.col('TM_POSTCODE'), 1, 5) )\n",
    "new_p2 = new_p2.withColumn('TM_POSTCODE', f.lpad(f.col('TM_POSTCODE'), 5, '0') )\n",
    "\n",
    "print('checking new p2 after correcting: ', new_p2.count()) # 1745117\n",
    "# print(new_p2[['TM_POSTCODE']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## ----------------------------------------------------------\n",
    "\n",
    "## Save in source bucket, RPA folder - s3://astro-groupdata-prod-source/sftp/rpa/ \n",
    "\n",
    "# 18/11/22: added line to create standard filename for easy automation + edited existing line to store historic files\n",
    "new_p1.coalesce(1).write.csv('s3://astro-groupdata-prod-source/rpa/New_P1_TM_Format_glue_spark.csv.gz', mode='overwrite', header=True, compression='gzip') \n",
    "new_p1.write.csv('s3://astro-groupdata-prod-source/rpa/historical_folder/new_p1p2/New_P1_TM_Format_glue_spark_'+str(curr_date)+'.csv.gz', mode='overwrite', header=True, compression='gzip')\n",
    "\n",
    "# 18/11/22: added line to create standard filename for easy automation + edited existing line to store historic files\n",
    "new_p2.coalesce(1).write.csv('s3://astro-groupdata-prod-source/rpa/New_P2_TM_Format_glue_spark.csv.gz', mode='overwrite', header=True, compression='gzip') \n",
    "new_p2.write.csv('s3://astro-groupdata-prod-source/rpa/historical_folder/new_p1p2/New_P2_TM_Format_glue_spark_'+str(curr_date)+'.csv.gz', mode='overwrite', header=True, compression='gzip')\n",
    "\n",
    "#new_p1.to_csv('New_P1_TM_Format.csv',index=False)\n",
    "# wr.s3.to_csv(df = new_p1, path = new_p1_save_path + 'New_P1_TM_Format_20221014.csv', index = False) ## --> 14/10/2022 Amzar: added \"_20221014\" to filename\n",
    "# wr.s3.to_csv(df = new_p1, path = 's3://astro-groupdata-prod-source/sftp/rpa/' + 'New_P1_TM_Format_20221018.csv', index = False) ## --> 14/10/2022 Amzar: added \"_20221014\" to filename\n",
    "# 18/11/22: commented out --> new_p1.coalesce(1).write.csv('s3://astro-groupdata-prod-source/rpa/historical_folder/New_P1_TM_Format_old_code_20221018_glue_spark.csv.gz', mode='overwrite', header=True, compression='gzip')\n",
    "\n",
    "#new_p2.to_csv('New_P2_TM_Format.csv',index=False)\n",
    "# wr.s3.to_csv(df = new_p2, path = new_p2_save_path + 'New_P2_TM_Format_20221014.csv', index = False) ## --> 14/10/2022 Amzar: added \"_20221014\" to filename\n",
    "# wr.s3.to_csv(df = new_p2, path = 's3://astro-groupdata-prod-source/sftp/rpa/' + 'New_P2_TM_Format_20221018.csv', index = False) ## --> 14/10/2022 Amzar: added \"_20221014\" to filename\n",
    "# 18/11/22: commented out --> new_p2.coalesce(1).write.csv('s3://astro-groupdata-prod-source/rpa/New_P2_TM_Format_20221018_glue_spark.csv.gz', mode='overwrite', header=True, compression='gzip')\n",
    "\n",
    "# usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "# print('[debug] memory usage is (Megabytes):')\n",
    "# print(usage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro-env",
   "language": "python",
   "name": "astro-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
