{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "%additional_python_modules\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "## do some configurations - run this cell before the next cell which activates the Spark Context\n%additional_python_modules awswrangler\n# %number_of_workers 10\n# %spark_conf spark.driver.maxResultSize=2G",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "Additional python modules to be included:\nawswrangler\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": [
				{
					"name": "stdout",
					"text": "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::741993363917:role/AWSGlueServiceRole\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 754eb60a-4d28-43cc-93eb-7e2b56009615\nApplying the following default arguments:\n--glue_kernel_version 0.35\n--enable-glue-datacatalog true\n--additional-python-modules awswrangler\nWaiting for session 754eb60a-4d28-43cc-93eb-7e2b56009615 to get into ready status...\nSession 754eb60a-4d28-43cc-93eb-7e2b56009615 has been created\n\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 567a891c-08c9-4385-a7a4-585e5e019800\nStopped session.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Trying to tag this spark session part so we can track costs\n- as of 25/12/2022, still haven't managed to get this tracking/tagging of Notebook costs to work",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "job.get_tags()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "AttributeError: 'Job' object has no attribute 'get_tags'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"collapsed": true,
				"jupyter": {
					"outputs_hidden": true
				},
				"tags": [],
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\nAvailable Magic Commands\n\n## Sessions Magics\n%help | Return a list of descriptions and input types for all magic commands. \n%profile | String | Specify a profile in your aws configuration to use as the credentials provider.\n%region | String | Specify the AWS region in which to initialize a session | Default from ~/.aws/configure\n%idle_timeout | Int | The number of minutes of inactivity after which a session will timeout. The default idle timeout value is 2880 minutes (48 hours).\n%session_id | Returns the session ID for the running session. \n%session_id_prefix | String | Define a String that will precede all session IDs in the format [session_id_prefix]-[session_id]. If a session ID is not provided, a random UUID will be generated.\n%status | Returns the status of the current Glue session including its duration, configuration and executing user / role.\n%list_sessions | Lists all currently running sessions by name and ID.\n%stop_session | Stops the current session.\n%glue_version | String | The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0. The default value is 2.0.\n%streaming | String | Changes the session type to Glue Streaming. \n%etl | String | Changes the session type to Glue ETL. \n\n## Glue Config Magics\n%%configure | Dictionary | A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics.\n%iam_role | String | Specify an IAM role ARN to execute your session with. | Default from ~/.aws/configure\n%number_of_workers | int | The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too. The default number_of_workers is 5.\n%worker_type | String | Standard, G.1X, or G.2X. number_of_workers must be set too. The default worker_type is G.1X.\n%security_config | String | Define a Security Configuration to be used with this session. \n%connections | List | Specify a comma separated list of connections to use in the session.\n%additional_python_modules | List | Comma separated list of additional Python modules to include in your cluster (can be from Pypi or S3).\n%extra_py_files | List | Comma separated list of additional Python files From S3.\n%extra_jars | List | Comma separated list of additional Jars to include in the cluster.\n%spark_conf | String | Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n\n## Action Magics\n%%sql | String | Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code. \n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "glueContext.list_sessions()",
			"metadata": {
				"collapsed": true,
				"jupyter": {
					"outputs_hidden": true
				},
				"tags": [],
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "AttributeError: 'GlueContext' object has no attribute 'list_sessions'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%list_sessions",
			"metadata": {
				"collapsed": true,
				"jupyter": {
					"outputs_hidden": true
				},
				"tags": [],
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "The first 3 sessions are:\n18216618-8c98-45a8-b0ba-dbcf4f54508e\na2a8fbd1-cc6c-4e9c-91b2-1ee65e0f1ed2\na57c624b-1968-44a1-be49-1142b86f9f62\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%status",
			"metadata": {
				"collapsed": true,
				"jupyter": {
					"outputs_hidden": true
				},
				"tags": [],
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "Session ID: 18216618-8c98-45a8-b0ba-dbcf4f54508e\nStatus: READY\nDuration: 45.914276 seconds\nRole: arn:aws:iam::741993363917:role/AWSGlueServiceRole\nCreatedOn: 2022-12-19 09:01:28.999000+00:00\nGlueVersion: 2.0\nWorker Type: G.1X\nNumber of Workers: 5\nRegion: ap-southeast-1\nApplying the following default arguments:\n--glue_kernel_version 0.35\n--enable-glue-datacatalog true\nArguments Passed: ['--glue_kernel_version: 0.35', '--enable-glue-datacatalog: true']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "tags_dict = {'Technical_Owner' : 'Amzar', 'Project' : 'Address_standardization' }\n# response = glue_client.tag_resource(TagsToAdd=tags_dict)\nresponse = job.tag_resource(TagsToAdd=tags_dict)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "AttributeError: 'Job' object has no attribute 'tag_resource'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure tags = {'Technical_Owner': 'Amzar'}",
			"metadata": {
				"collapsed": true,
				"jupyter": {
					"outputs_hidden": true
				},
				"tags": [],
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "You are already connected to session 18216618-8c98-45a8-b0ba-dbcf4f54508e. Your change will not reflect in the current session, but it will affect future new sessions. \n\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "The following exception was encountered while parsing the configurations provided: invalid syntax (<unknown>, line 1) \nTraceback (most recent call last):\n  File \"/home/jupyter-user/.local/lib/python3.7/site-packages/aws_glue_interactive_sessions_kernel/glue_pyspark/GlueKernel.py\", line 278, in configure\n    configs = ast.literal_eval(configs_json)\n  File \"/usr/lib64/python3.7/ast.py\", line 46, in literal_eval\n    node_or_string = parse(node_or_string, mode='eval')\n  File \"/usr/lib64/python3.7/ast.py\", line 35, in parse\n    return compile(source, filename, mode, PyCF_ONLY_AST)\n  File \"<unknown>\", line 1\n    tags = {'Technical_Owner': 'Amzar'}\n         ^\nSyntaxError: invalid syntax\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "sys.argv+=['--JOB_NAME', 'amzar-address_standardization-prod-p1p2_mdu_mapping_tm_spark_etl']\nargs = getResolvedOptions(sys.argv,\n                          ['JOB_NAME'])\nprint(args)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'job_bookmark_option': 'job-bookmark-disable', 'job_bookmark_from': None, 'job_bookmark_to': None, 'JOB_ID': None, 'JOB_RUN_ID': None, 'SECURITY_CONFIGURATION': None, 'encryption_type': None, 'enable_data_lineage': None, 'RedshiftTempDir': None, 'TempDir': None, 'JOB_NAME': 'amzar-address_standardization-prod-p1p2_mdu_mapping_tm_spark_etl'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Enhanced P1P2 MDU Mapping for TM\nNotebook created on 13/12/22. Codes copied from Qubole Zeppelin Notebook: https://us.qubole.com/notebooks#home?id=142238&type=my-notebooks&location=Users/amzar_maarof@astro.com.my/Broadband/AddressStandardization \n- There were some edits made to fit the reading in of the new std bases & to reduce no of columns read in\n- run these next 2 cells before any cell below them in this Notebook",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import functions as f\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.functions import when\n\nfrom pyspark.sql.window import Window\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import *\n\nimport pandas as pd\nimport numpy as np\nimport awswrangler as wr # seems like the generate_UAMS_Format function may need this for some reason... Maybe coz I'm using some pandas functions...?\n\n# # ------ automating the generation of start and end dates \nfrom datetime import datetime\nfrom datetime import timedelta\n# Refer to https://stackoverflow.com/questions/9724906/python-date-of-the-previous-month\nimport time\n# curr_date = str(datetime.today().strftime('%Y%m%d'))\ncurr_date = '20221213'\nprint(curr_date)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "20221213\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### set read_paths\nastro_new_std_path = 's3://astro-groupdata-prod-pipeline/address_standardization/astro_new_standardized/astro_new_standardized.csv.gz'\ntm_new_std_path = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_new_standardized/TM_New_Standardized.csv.gz'\ntemporary_save_path = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_mdu/historical_folder/'\nprint(temporary_save_path)\nuams_mdu_path = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_mdu/'\nISP_Name = 'TM'\n\n## define some functions for repetitive actions\ndef clean_HNUM_STRT(df, hnum_strt_col_string):\n    ## Capitalize & clean HNUM_STRT column. Replace specific strings (eg. '[,.]' ) with empty string (\"\")\n    df = df.withColumn(hnum_strt_col_string, f.upper(f.trim(f.col(hnum_strt_col_string))) )\n    df = df.withColumn(hnum_strt_col_string, f.regexp_replace(hnum_strt_col_string, 'NAN |', '') )\n    df = df.withColumn(hnum_strt_col_string, f.regexp_replace(hnum_strt_col_string, '[,.]', '') )\n    df = df.withColumn(hnum_strt_col_string, f.regexp_replace(hnum_strt_col_string, ' ', '') )\n    df = df.withColumn(hnum_strt_col_string, f.regexp_replace(hnum_strt_col_string, '\\.', '') )\n    df = df.withColumn(hnum_strt_col_string, f.regexp_replace(hnum_strt_col_string, ',', '') )\n    return df",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "s3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_mdu/historical_folder/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Read in Astro & TM New Std Bases",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "now = datetime.now()\n\n# read in uploaded astro_new_std (last run 20221014). But note that the count is lower in this one compared to 20220812 -- previous versions of this notebook used older Astro New Std\nastro_new_std = spark.read.csv(astro_new_std_path, header=True)\nprint('original columns in astro new std:', astro_new_std.columns)\n## select relevant columns to reduce runtimes -- \nastro_new_std = astro_new_std.select('ACCOUNT_NO', 'ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', \n                                     'Street_2', 'AREA', 'CITY', 'STATE', 'COUNTRY', 'POSTCODE', 'match', 'Combined_Building', 'ASTRO_CITY', 'ASTRO_STATE', 'cust_location', 'service_add_objid', 'STD_CITY', 'HOUSE_NO', \n                                     'Token_Sort_Ratio', 'rank', 'Standard_Building_Name', 'Address_ID', 'Street_Type', 'Building_Coalesced', 'AREA_Coalesced', 'CITY_Coalesced', 'POSTCODE_Coalesced', \n                                     'STATE_Coalesced', 'Source')\n\n## Trim/strip the whitespaces in the column & make sure it's UPPER CASE\nastro_new_std = astro_new_std.withColumn('ASTRO_STATE', f.upper(f.trim(astro_new_std.ASTRO_STATE)))\n# z.show(astro_new_std.select('ASTRO_STATE_trim').distinct())\nprint('astro_new_std count', astro_new_std.select('ASTRO_STATE').count()) # 4675485\n\n## ensuring ACCOUNT_NO is string\nastro_new_std = astro_new_std.withColumn( 'ACCOUNT_NO', f.col('ACCOUNT_NO').cast('string') )\nastro_new_std = astro_new_std.withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n\n# First, convert all Combined_Building values to UPPER CASE, then trim/strip the whitespaces & filter out null Combined_Building values as these might not be MDU\n# astro_kv = astro_new_std.withColumn('Combined_Building', f.upper(f.trim(f.col('Combined_Building'))))\n# astro_kv1 = astro_kv.filter('Combined_Building IS NOT NULL').filter(~f.col('Combined_Building').isin(['NAN', '-', '.', '', ' ', '/']))\n# print('astro_kv1 count', astro_kv1.count(), 'astro_kv1 unique acc_no', astro_kv1.select(f.countDistinct('ACCOUNT_NO')).show()) # 710887 , 601794\n\n# First, convert all Building_Coalesced values to UPPER CASE, then trim/strip the whitespaces & filter out null Building_Coalesced values as these might not be MDU (used this one coz got higher counts than Combined_Building\nastro_kv = astro_new_std.withColumn('Building_Coalesced', f.upper(f.trim(f.col('Building_Coalesced'))))\nastro_kv = astro_kv.filter('Building_Coalesced IS NOT NULL').filter(~f.col('Building_Coalesced').isin(['NAN', '-', '.', '', ' ', '/']))\n# print('notnull Building_Coalesced count', astro_kv1.count(), 'notnull Building_Coalesced unique acc_no', astro_kv1.select(f.countDistinct('ACCOUNT_NO')).show()) # 713550 , 603025\n\n# ## save a temporary intermediate file (coz from experimentations in this notebook, found that using CSV takes a long time to run each cell + can cause the cluster to crash. Thus, save an ORC intermediate file to avoid these problems)\nastro_kv.write.orc(temporary_save_path+'{0}/Astro_MDUonly.orc'.format(curr_date), mode=\"overwrite\", compression='snappy')\n\nprint(astro_kv.columns)\n\n## if needed, delete original files that were read in to free up memory\ndel astro_new_std\ndel astro_kv\n\nend = datetime.now()\nprint(str(end - now)) # 0:01:24.578301\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "original columns in astro new std: ['ACCOUNT_NO', 'ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'CITY', 'STATE', 'COUNTRY', 'POSTCODE', 'match', 'Combined_Building', 'ASTRO_CITY', 'ASTRO_STATE', 'cust_location', 'service_add_objid', 'STD_CITY', 'HOUSE_NO', 'Token_Sort_Ratio', 'rank', 'Standard_Building_Name', 'Address_ID', 'Street_Type', 'Building_Coalesced', 'AREA_Coalesced', 'CITY_Coalesced', 'POSTCODE_Coalesced', 'STATE_Coalesced', 'Source']\nastro_new_std count 4675485\n['ACCOUNT_NO', 'ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'CITY', 'STATE', 'COUNTRY', 'POSTCODE', 'match', 'Combined_Building', 'ASTRO_CITY', 'ASTRO_STATE', 'cust_location', 'service_add_objid', 'STD_CITY', 'HOUSE_NO', 'Token_Sort_Ratio', 'rank', 'Standard_Building_Name', 'Address_ID', 'Street_Type', 'Building_Coalesced', 'AREA_Coalesced', 'CITY_Coalesced', 'POSTCODE_Coalesced', 'STATE_Coalesced', 'Source']\n0:01:24.578301\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "now = datetime.now()\n# had to re-use 20220707 coz the columns in 20221013 are different to the columns used in the codes I've already written in this notebook\ntm_new_std = spark.read.csv(tm_new_std_path, header=True)\n# print('tm_new_std count', tm_new_std.select('ORIGINAL_ADDRESS').count()) # 11315437 addresses\n\n## select relevant columns to reduce runtimes\n# OLD code for TM_NEW_STD 20220707 --> tm_new_std = tm_new_std.select('ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'CITY11', 'STATE12', 'COUNTRY', 'POSTCODE14', 'match', 'STD_CITY', 'BuildingName', 'City18', 'FloorNo', 'HouseNo', 'Postcode21', 'Section', 'ServiceType', 'State24', 'StreetName', 'StreetType', 'Postcode_Length', 'ADDRESS', 'ADDRESS_KEY', 'Combined_Building', 'Source', 'Standard_Building_Name',  'ADDRESS_MDU', 'Address_Type')\ntm_new_std = tm_new_std.select('ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', \n                               'CITY9', 'STATE10', 'COUNTRY', 'POSTCODE12', 'match', 'STD_CITY', 'BuildingName', 'City16', 'FloorNo', 'HouseNo', 'Postcode19', 'Section', 'ServiceType', 'State22', 'StreetName', \n                               'StreetType', 'Postcode_Length', 'ADDRESS', 'ADDRESS_KEY', 'Combined_Building', 'Source', 'Standard_Building_Name',  'ADDRESS_MDU', 'Address_Type', 'TM_Street', \n                               'Street_coalesced', 'CITY_coalesced') # --> code to be used for TM New Std of 20221013\n# tm_new_std.columns\n\n## renaming columns which have numbers in them (same col name but diff CASE)\ntm_new_std = tm_new_std.withColumnRenamed('CITY9', 'CITY_GAPI').withColumnRenamed('STATE10', 'STATE_GAPI').withColumnRenamed('POSTCODE12', 'POSTCODE_GAPI').withColumnRenamed('City16', 'City').withColumnRenamed('Postcode19', 'Postcode').withColumnRenamed('State22', 'State')\n\n# Trim/strip the whitespaces in the column & make sure it's UPPER CASE\ntm_new_std = tm_new_std.withColumn('STATE_GAPI', f.upper(f.trim(tm_new_std.STATE_GAPI)))\n\n# First, convert all Combined_Building values to UPPER CASE, then trim/strip the whitespaces\ntm_kv = tm_new_std.withColumn('Combined_Building', f.upper(f.trim(f.col('Combined_Building'))))\n\n# Filter out null Combined_Building values as these might not be MDU\ntm_kv = tm_kv.filter('Combined_Building IS NOT NULL').filter(~f.col('Combined_Building').isin(['NAN', '-', '.', '', ' ', '/']))\nprint('notnull combined_building count',tm_kv.count(), 'notnull combined_building unique ori_addr',tm_kv.select(f.countDistinct('ORIGINAL_ADDRESS')).show()) # 3744359 rows, 988728 unique original_addr (excluding HouseNo)\n\n# ## save a temporary intermediate file (coz from experimentations in this notebook, found that using CSV takes a long time to run each cell + can cause the cluster to crash. Thus, save an ORC intermediate file to avoid these problems)\ntm_kv.write.orc(temporary_save_path+'{0}/TM_MDUonly.orc'.format(curr_date), mode=\"overwrite\", compression='snappy')\n\nprint(tm_kv.columns)\n\ndel tm_new_std\ndel tm_kv\n\nend = datetime.now()\nprint(str(end - now)) # 0:04:59.891710",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------------------+\n|count(DISTINCT ORIGINAL_ADDRESS)|\n+--------------------------------+\n|                          988728|\n+--------------------------------+\n\nnotnull combined_building count 3744359 notnull combined_building unique ori_addr None\n['ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'CITY_GAPI', 'STATE_GAPI', 'COUNTRY', 'POSTCODE_GAPI', 'match', 'STD_CITY', 'BuildingName', 'City', 'FloorNo', 'HouseNo', 'Postcode', 'Section', 'ServiceType', 'State', 'StreetName', 'StreetType', 'Postcode_Length', 'ADDRESS', 'ADDRESS_KEY', 'Combined_Building', 'Source', 'Standard_Building_Name', 'ADDRESS_MDU', 'Address_Type', 'TM_Street', 'Street_coalesced', 'CITY_coalesced']\n0:04:59.891710\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Implement the Enhanced Flow for Astro MDUs\n\n--------------------- Flow Idea from P2 MDU Mapping Test_1.1 ---------------------\n\n- Potential Flow for extracting BlockNo (BLOCK_extracted): fix HOUSE_NO that were converted to date --> remove extra special characters (,.) in Combined_Building & at end of HOUSE_NO --> extract after the word BLOCK/BLOK from ORIGINAL_ADDRESS --> extract after the word BLOCK/BLOK from HOUSE_NO --> extract from character before dash in HOUSE_NO --> extract from character before dashes in ORIGINAL_ADDRESS\n- Potential Overall Flow: extract BlockNo --> create new test new_building_name column --> remove FTTH --> standardize CONDO, APT, P/PURI --> remove FLAT, PPR, PANGSAPURI --> (consider new column when appending extracted BlockNo) if the word 'BLOK/BLOCK' is in BuildingName (typically TM Homepass), remove BLOK/BLOCK + character after the word, then append extracted BlockNo to front of new_building_name. If it DOESN'T have BLOK/BLOCK (typically Astro addr), then just append the extracted BlockNo to the front of new_building_name (new_block_building_name) --> consider making a Set out of the words in new_building_name (avoid repeat words) and alphabetically order words --> use new_block_building_name in mapping key\n\n### Recreate extract_house_NO function from Step 2.1 in PySpark Code directly first",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# # read Astro & TM files back in as orc - this seems to cause the rest of the steps to run faster\n# # also decided to rename astro_kv1 to astro_kv_clean directly and do the same for TM\nastro_kv_clean = spark.read.orc(temporary_save_path+'{0}/Astro_MDUonly.orc'.format(curr_date))\n\n## ensure upper case & trim for ORI_ADDR\nastro_kv_clean = astro_kv_clean.withColumn('ORIGINAL_ADDRESS', f.upper(f.trim(f.col('ORIGINAL_ADDRESS'))) )\n\n## sub HOUSE_NO into HOUSE_NO_old\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO_old', f.upper(f.trim(f.col('HOUSE_NO'))) )\n\n### extract House No -- this function uses rlike as the filter\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', \n                 when( f.col('ORIGINAL_ADDRESS').rlike('(\\s|,|^)(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})', 0) ) # (E-A1-A2-A45 / E2-1A-2A-4A / 1-2-3-4 / Z-A-B-C / G/1/11/22)\n                 .when( f.col('ORIGINAL_ADDRESS').rlike('(\\s|,|^|BLO?C?K)(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<=\\s|,|^|BLO?C?K)(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})', 0) ) # (A1-A2-A45 / 1A-2A-4A / 1-2-3 / A-B-C / G/1/11 / B2,-2-23) # 30/10/22: added 'BLOK' to lookbehind coz found 'BLOKF-16-07'\n                 .when( f.col('ORIGINAL_ADDRESS').rlike('(JA?LA?N\\s|JA?LA?N \\b\\w+\\b\\s|JA?LA?N \\b\\w+\\b \\b\\w+\\b\\s)(\\s|,|^)(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})(\\s|,)'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<!JA?LA?N\\s)(?<!JA?LA?N \\w{2,10}\\s)(?<!JA?LA?N \\w{2,10} \\w{2,10}\\s)(\\w{1,4})\\s?(-|,-|\\/)\\s?(\\w{1,4})', 0) ) # (A2-A45 / 2A-4A / 2-3 / B-C / ...) but with JALAN names before them # 29/10/22: only for this code-line, added lookbehind regex in both rlike & extract (but they differ) for JALAN because there are streetnames like PJS 1/21 which might accidentally be taken as HouseNo\n                 .when( f.col('ORIGINAL_ADDRESS').rlike('(\\s|,|^)(\\w{1,4})\\s?(-|,-)\\s?(\\w{1,4})(\\s|,)'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(\\w{1,4})\\s?(-|,-)\\s?(\\w{1,4})', 0) ) # (A2-A45 or 2A-4A or 2-3 or B-C, ...)\n                 .when( f.col('ORIGINAL_ADDRESS').rlike('(\\s|,|^)\\d{1,4}\\s?[A-Z](\\s|,)'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '\\d{1,4}\\s?[A-Z]', 0) ) # (265 A / 265A)\n                 .when( f.col('ORIGINAL_ADDRESS').rlike('(?<!BLO?C?K\\s?,?)(\\s|,|^)([A-Z])\\s?\\d{1,4}(\\s|,)'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<!BLO?C?K\\s?,?)(?<=\\s|,|^)([A-Z])\\s?\\d{1,4}', 0) ) # A32 / A 32\n                 .when( f.col('ORIGINAL_ADDRESS').rlike('(NO|UNIT|LOT)\\.?\\s?\\w{1,6}(\\s|,)'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<=NO|UNIT|LOT)\\.?\\s?\\w{1,6}', 0) ) # (NO 5 / NO. 5 / UNIT 5 / LOT 1405) # 'LOT' was added on 29/10/2022\n                 .when( f.col('ORIGINAL_ADDRESS').rlike('(\\s|,|^)(\\d{1,4})(,|(\\s*,)|\\s)'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<!JA?LA?N\\s)(?<!JA?LA?N \\w{2,10}\\s)(?<!JA?LA?N \\w{2,10} \\w{2,10}\\s)(?<!BLO?C?K\\s?,?)(?<!TINGKAT\\s?,?)(?<!FASA\\s?,?)(?<!LEVEL\\s?,?)(?<=\\s|,|^)(\\d{1,4})', 0) ) # ,234 / 234 (but no TINGKAT, FASA, or LEVEL in front). # 30/10/22: added lookbehind regex to ignore the number after JALAN\n                #  .when( f.col('ORIGINAL_ADDRESS').rlike('(\\s|,)(\\d{1,4})(,|(\\s*,)|\\s)'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<!BLO?C?K\\s?,?)(?<=\\s|,)(\\d{1,4})', 0) ) # ,234, or 234, (original)\n                # (?<!TINGKAT\\s?)(?<!BLO?C?K\\s?,?)(?<!PR) --> this last one is to avoid extracting the 1 out of the word 'PR1MA')\n                 .otherwise('no House NO') \n                 )\n                #  (?<!JA?LA?N)(?<!JA?LA?N \\w+)(?<!JA?LA?N \\w+ \\w+),?",
			"metadata": {
				"trusted": true
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "now=datetime.now()\n\n#### ------------------------ Extracting BlockNo Flow (BLOCK_extracted) ------------------------\n### do for Astro New Std first -- to run this whole cell as CSV: 7-8 mins. As ORC: takes 5.5 mins \n# astro_kv_clean: # 436640 rows, ../ unique acc_no\n\n## trim + uppercase\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.upper(f.trim(f.col('HOUSE_NO'))))\n\n## Fix HOUSE_NO. Replace specific strings (eg. \"BLOK\" ) with empty string (\"\"). 29/10/2022: added line to convert slashes to dashes\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"\\/\",\"-\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), 'NAN\\s?',''))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), ' ','')) # 29/10/22: remove spaces as from what I've seen, there shouldn't be any spaces in HouseNo\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), '[\\[\\]\\.,\"]',''))\n# astro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"[\",'')) # maybe it's this code which is causing the issue\n# astro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"]\",\"\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"'\",\"\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"NO\\s?\",\"\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"LOT\\s?\",\"\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"UNIT\\s?\",\"\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"BLO?C?K\\s?\",\"\"))\n## somehow the above block of code is giving a weird error - \"java.util.regex.PatternSyntaxException: Unclosed character class near index 0 [\" which doesn't allow me to show or save the table. So I made changes to it and commented out the code with the \"[\" and \"]\"\n\n# -- takes 5 mins to run just to run this portion as CSV\n## Fix HOUSE_NO that were converted to date --> tested code for PySpark, looks like it works\n# 29/10/2022: removed the lines that deal with non-upper-cased months (e.g Jan, Feb) since we are making everything uppercased in line above. This is to reduce runtimes & lines of code\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"JAN-\",\"01-\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-JAN\",\"-01\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"FEB-\",\"02-\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-FEB\",'-02'))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"MAR-\",'03-'))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-MAR\",\"-03\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"APR-\",\"04-\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-APR\",\"-04\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"MAY-\",\"05-\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-MAY\",\"-05\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"JUN-\",\"06-\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-JUN\",\"-06\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"JUL-\",\"07-\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-JUL\",\"-07\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"AUG-\",'08-'))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-AUG\",\"-08\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"SEP-\",\"09-\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-SEP\",\"-09\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"OCT-\",\"10-\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-OCT\",\"-10\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"NOV-\",\"11-\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-NOV\",\"-11\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"DEC-\",\"12-\"))\nastro_kv_clean = astro_kv_clean.withColumn('HOUSE_NO', f.regexp_replace(f.col('HOUSE_NO'), \"-DEC\",\"-12\"))\n\n# Fix HOUSE_NO that are converted to date (DD/MM/YYYY format) --> tested code for PySpark. Looks like it works\n# Filter date HOUSE_NO\ndate_house = astro_kv_clean.filter(f.regexp_extract('HOUSE_NO', '^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$', 0) != '' ) \n# Spliting the HOUSE_NO\ndate_house = date_house.withColumn('block_date',  f.substring(date_house.HOUSE_NO, 1, 2))\ndate_house = date_house.withColumn('floor',  f.substring(date_house.HOUSE_NO, 4, 2))\ndate_house = date_house.withColumn('unit',  f.substring(date_house.HOUSE_NO, 9, 2))\n# Combine the split HOUSE_NO with dashes: '-'\ndate_house = date_house.withColumn('HOUSE_NO_ASTRO', f.concat_ws('-', date_house.block_date, date_house.floor, date_house.unit))\n# Remove additional column created to combine HOUSE_NO\ndate_house = date_house.drop(*['block_date','floor','unit'])\n# print(date_house.select('ACCOUNT_NO').count(), date_house.select(f.countDistinct('ACCOUNT_NO')).show()) # 0 rows, 0 unique acc_no\n\n# Filter not date HOUSE_NO\nnot_date_house = astro_kv_clean.filter( f.regexp_extract('HOUSE_NO', '^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$', 0) == '' )\nnot_date_house = not_date_house.withColumn('HOUSE_NO_ASTRO', f.col('HOUSE_NO'))\n# print(not_date_house.select('ACCOUNT_NO').count(), not_date_house.select(f.countDistinct('ACCOUNT_NO')).show()) # 716792  rows, 605387 unique acc_no\n\n# Filter for null HOUSE_NO\nnull_house_no = astro_kv_clean.filter( f.col('HOUSE_NO').isNull() )\nnull_house_no = null_house_no.withColumn('HOUSE_NO_ASTRO', f.col('HOUSE_NO'))\n# print(null_house_no.select('ACCOUNT_NO').count(), null_house_no.select(f.countDistinct('ACCOUNT_NO')).show()) # this gives 0 rows & 0 unique acc_no\n\n# Append the dfs --> Original appending (just date_house & not_date_house) removes the rows where HOUSE_NO is NULL. But I think we should keep those cases because we can still map them as P2 MDU\nastro_kv_clean_1 = date_house.union(not_date_house).union(null_house_no)\n\n## pad the values so it won't convert to date again. Note, the null HOUSE_NO will still stay null after this padding step.\nastro_kv_clean_1 = astro_kv_clean_1.withColumn('HOUSE_NO', f.lpad(astro_kv_clean_1['HOUSE_NO_ASTRO'], 10, ' '))\n\n# -- takes 1-3 mins to run as CSV, 1.5mins as ORC\n\n## remove commas, dots (other special characters?) in Combined_Building. # 13/12/22: changed to Building_Coalesced\nastro_kv_clean_1 = astro_kv_clean_1.withColumn('Building_Coalesced', f.regexp_replace('Building_Coalesced', '[\\.,\"]', '') )\nastro_kv_clean_1 = astro_kv_clean_1.withColumn('Building_Coalesced', f.regexp_replace('Building_Coalesced', \"'\", \"\") )\n\n## create BLOCK_extracted by: extract after the word BLOCK/BLOK from Combined_Building --> extract after the word BLOCK/BLOK from ORIGINAL_ADDRESS --> extract after the word BLOCK/BLOK from HOUSE_NO --> extract from character before dash in HUSE_NO --> extract from character before dashes in ORIGINAL_ADDRESS\n## 29/10/22: in bottom 3 code-lines (extracting from strings with dashes) changed the '\\w+' to \\w{1,3} to enforce exctraction of only alphanumeric words of length 1-3\n# 13/12/22: changed Combined_Building to Building_Coalesced\nfrom pyspark.sql.functions import when\nastro_kv_clean_1 = astro_kv_clean_1.withColumn('BLOCK_extracted', \n                    when( f.col('Building_Coalesced').rlike('BLO?C?K \\w{1,3}'), f.regexp_extract( f.col('Building_Coalesced'), '(?<=BLO?C?K.?\\s)\\w+', 0) ) # check for BLOCK regex in Building_Coalesced (BLK 2A) & get the first 'word' after it ('2A')\n                    .when( f.col('ORIGINAL_ADDRESS').rlike('BLO?C?K \\w{1,3}'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<=BLO?C?K.?\\s)\\w+', 0) ) # check for BLOCK regex in ORIGINAL_ADDRESS (BLK 2A) & get the first 'word' after it ('2A')\n                    .when( f.col('HOUSE_NO').rlike('BLO?C?K \\w{1,3}'), f.regexp_extract( f.col('HOUSE_NO'), '(?<=BLO?C?K.?\\s)\\w+', 0) ) # check for BLOCK regex in HOUSE_NO (BLK 2A) & get the first 'word' after it ('2A')\n                    \n                    .when( f.col('HOUSE_NO').rlike('(?<=,*)\\w{1,3}(?=,?-\\w{1,3}-)'), f.regexp_extract( f.col('HOUSE_NO'), '(?<=,*)\\w{1,3}(?=,?-\\w{1,3}-)', 0) )  # check for HOUSE_NO that has 2 dashes using regex (B-2-1) & get the 1st 1st character ('B'). Use * in lookbehind instead of + because HOUSE_NO probably won't have commas before the number anymore. \n                    .when( f.col('ORIGINAL_ADDRESS').rlike('(?<=,*)\\w{1,3}(?=,?-\\w{1,3}-)'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<=,*)\\w{1,3}(?=,?-\\w{1,3}-)', 0) )  # check for ORIGINAL_ADDRESS that has 2 dashes using regex (B-2-1) & get the 1st character ('B'). \n                    .when( f.col('HOUSE_NO').rlike('\\w{1,3}\\s?-\\w{1,3}'), f.regexp_extract( f.col('HOUSE_NO'), '\\w{1,3}(?=-\\w+)', 0) ) # check for HOUSE_NO that has 1 dash using regex (B-2) & only take 1st character from start of string from HOUSE_NO ('B'). \n                    # .when( f.col('HOUSE_NO').rlike('.+-.+'), 'noBlock' ) # maybe BlockNo not extracted from EDW step?\n                    # .when( f.col('ORIGINAL_ADDRESS').rlike('BLO?C?K'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<=BLO?C?K\\s)\\w+', 0) ) # check for BLOCK regex in ORIGINAL_ADDRESS,\n                    # .when( f.col('HOUSE_NO').isNull(), f.col('HOUSE_NO') ) # just returns null for next step\n                    .otherwise( 'noBlock' ) # cases where no Block No was extracted in EDW Step\n                    )\n\n## save a temporary intermediate file (coz subsequent steps still caused spark cluster to crash, even when using astro_kv1 ORC file)\nastro_kv_clean_1.write.orc(temporary_save_path+'{0}/Astro_newMDUcleaningflow.orc'.format(curr_date), mode=\"overwrite\", compression='snappy')\n\ndel astro_kv_clean_1 # -- if required\n\nend = datetime.now()\nprint(str(end - now)) ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "now = datetime.now()\n\n# read file back in as orc - this seems to cause most of cleaning steps to run faster\nastro_kv_clean_1 = spark.read.orc(temporary_save_path+'{0}/Astro_newMDUcleaningflow.orc'.format(curr_date))\n# print(astro_kv_clean_1.select('ACCOUNT_NO').count(), astro_kv_clean_1.select(f.countDistinct('ACCOUNT_NO')).show()) # 716792 rows & 605387 unique account_NO\n\n#### ------------------------------------------------ POTENTIAL OVERALL FLOW ------------------------------------------------\n# Astro New Std Building cleaning -- takes 5-10 mins as CSV, 5 mins as ORC (in previous run, when I saved an intermediate ORC file after BLOCK_extracted step, this only took 1 sec though...)\n\n# - Potential Overall Flow: extract BlockNo --> create new test new_building_name column --> remove FTTH --> standardize CONDO --> remove BLOCK + 'word' after it -->  remove FLAT, PPR, PANGSAPURI, APARTMENT --> further clean BLOCK_extracted column before appending --> (For P1 MDU, consider new column for this step, maybe new_block_building_name) append the extracted BlockNo to the front of new_building_name --> consider making a Set out of the words in new_building_name (avoid repeat words) and alphabetically order words (create joined_set) --> use joined_set in mapping key\n\n## BlockNo extracted in earlier step\n## create new_building_name column for testing\n# astro_kv_clean_2 = astro_kv_clean_1.withColumn('new_building_name', f.col('Combined_Building') ) # 13/12/22: commented out this line and used Building_Coalesced instead\nastro_kv_clean_2 = astro_kv_clean_1.withColumn('new_building_name', f.col('Building_Coalesced') )\n\n## Remove 'FTTH' from Combined Building\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', 'FTTH\\s', '') ) # removes any 'FTTH ' at start of line\n\n## remove BLOCK from new_building_name\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', 'BLO?C?K\\s?\\w{1,3}(?=\\s[A-Z]*|$)', '') ) # removes any 'BLK/BLOK/BLOCK 13A' where the word/character set after BLOK is between 1-3 characters. Does not remove if the word after BLOK is more than 3 characters e.g won't remove 'BLK/BLOK/BLOCK TERATAI'\n\n## standardize CONDO --> remove FLAT, PPR, PANGSAPURI, APARTMENT \nastro_kv_clean_2 = astro_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', '[CK]ONDO(?=\\s[A-Z]*|$)', 'CONDOMINIUM') ) # replaces C/KONDO with 'CONDOMINIUM'\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', 'KONDOMINIUM', 'CONDOMINIUM') ) # replaces KONDOMINIUM with 'CONDOMINIUM'\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', '\\s?FLAT\\s?', '') ) # removes any 'FLAT'\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', '\\s?PPR\\s?', '') ) # removes any 'PPR'\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', '\\s?AP\\w*T\\s?', '') ) # removes any word that starts with AP and ends with T (APARTMENT) \nastro_kv_clean_2 = astro_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', '\\s?P.*PURI\\s?', '') ) # removes any word that starts with P and ends with PURI (PANGSAPURI)\n\n## clean BLOCK_extracted column before appending (coz still got 'noBlock' and weird values where it's the condo name then a comma then only the Block No) # -- takes 1 min to run\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('BLOCK_extracted', \n                    when( f.col('BLOCK_extracted').contains(','), f.regexp_extract( f.col('BLOCK_extracted'), '(?<=,).+', 0) )\n                    .when( f.col('BLOCK_extracted') == 'noBlock', '')\n                    .otherwise( f.col('BLOCK_extracted') ) \n                    )\n\n## -- 3-8 mins to run as CSV, 3 mins as ORC (in previous run, when I saved an intermediate ORC file after BLOCK_extracted step, this only took 1 sec though...)\n## create new column which appends BLOCK_extracted to new_building_name (P1 MDU) but keep new_building_name BLOK-less for P2 MDU\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('new_block_building_name', f.concat_ws(' ', astro_kv_clean_2.BLOCK_extracted, astro_kv_clean_2.new_building_name) )\n\n## trim & uppercase\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('new_building_name', f.trim(f.upper(astro_kv_clean_2.new_building_name)) )\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('new_block_building_name', f.trim(f.upper(astro_kv_clean_2.new_block_building_name)) )\n\n## make a set of the words in each row then alphabetically arrange (similar to fuzzy's TokenSetRatio: https://github.com/seatgeek/fuzzywuzzy/blob/master/fuzzywuzzy/fuzz.py)\n# relevant pyspark docs: https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html, https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.array_sort.html, https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.array_distinct.html\n# astro_kv_clean_2 = astro_kv_clean_2.withColumn('sorted_set', sorted(f.collect_set(f.split(f.col('new_block_building_name'), pattern=' ', limit=-1))) )\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('sorted_set', f.array_sort(f.array_distinct(f.split(f.col('new_block_building_name'), pattern=' '))) )\n\n## join the values in sorted_set back together into a string column\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('joined_set', f.concat_ws(\" \", f.col(\"sorted_set\")) )\n\n### ------------------------------ final cleaning + coalescing (use columns read in that were already coalesced ------------------------------\n\n## ensuring the columns that will be used in the mapping key (P1P2 MDU/SDU) are trimmed + upper case. 13/12/22: changed some of these cols to use the Coalesced versions\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('HOUSE_NO', f.upper(f.trim(f.col('HOUSE_NO')))) ## THIS STEP would cancel out the padding done when in code cell that cleaned HOUSE_NO above but since we need to add the HOUSE_NO into the mapping key, maybe it's best to do the padding later on again. Or consider only padding if the file has to go through CSV again?\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('joined_set', f.upper(f.trim(f.col('joined_set'))))\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('Street_1', f.upper(f.trim(f.col('Street_1'))))\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('CITY_Coalesced', f.upper(f.trim(f.col('CITY_Coalesced'))))\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('AREA_Coalesced', f.upper(f.trim(f.col('AREA_Coalesced'))))\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('POSTCODE_Coalesced', f.upper(f.trim(f.col('POSTCODE_Coalesced'))))\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('STATE_Coalesced', f.upper(f.trim(f.col('STATE_Coalesced'))))\n# astro_kv_clean_2 = astro_kv_clean_2.withColumn('STD_CITY', f.upper(f.trim(f.col('STD_CITY'))))\n# 12/10/2022, added AREA after deciding I want to compare P1 MDU numbers when HNUM_STRT has AREA vs does not have AREA\n# astro_kv_clean_2 = astro_kv_clean_2.withColumn('AREA', f.upper(f.trim(f.col('AREA')))) \n# astro_kv_clean_2 = astro_kv_clean_2.withColumn('AREA/SECTION', f.upper(f.trim(f.col('AREA/SECTION')))) ## 13/12/22: did not read in this column anymore\n\n## 12/10/2022 - coalesce the multiple AREA columns # 13/12/22: commented out this line (not nescessary anymore as the new std base now has AREA_coalesced directly\n# astro_kv_clean_2 = astro_kv_clean_2.withColumn('AREA_coalesced', f.coalesce(f.col('AREA'), f.col('AREA/SECTION')) ) \n\n## Combine address columns(HOUSE_NO,...) to generate a new column named COMBINED_ADD -- this is the 'standardized' address. # 13/12/22: used the Coalesced cols instead\n# astro_kv_clean_2 = astro_kv_clean_2.withColumn('COMBINED_ADD', f.concat_ws(' ,', astro_kv_clean_2[\"HOUSE_NO\"],  astro_kv_clean_2[\"Combined_Building\"], astro_kv_clean_2[\"Street_1\"], astro_kv_clean_2[\"AREA\"], astro_kv_clean_2[\"POSTCODE\"], astro_kv_clean_2[\"STD_CITY\"], astro_kv_clean_2[\"ASTRO_STATE\"]) )\n# astro_kv_clean_2 = astro_kv_clean_2.withColumn('COMBINED_ADD', f.concat_ws(' ,', astro_kv_clean_2[\"HOUSE_NO\"],  astro_kv_clean_2[\"Combined_Building\"], astro_kv_clean_2[\"Street_1\"], astro_kv_clean_2[\"AREA_coalesced\"], astro_kv_clean_2[\"POSTCODE\"], astro_kv_clean_2[\"STD_CITY\"], astro_kv_clean_2[\"ASTRO_STATE\"]) ) ## 12/10/2022 - used AREA_coalesced instead of AREA\nastro_kv_clean_2 = astro_kv_clean_2.withColumn('COMBINED_ADD', f.concat_ws(' ,', astro_kv_clean_2[\"HOUSE_NO\"],  astro_kv_clean_2[\"Building_Coalesced\"], astro_kv_clean_2[\"Street_1\"], astro_kv_clean_2[\"AREA_coalesced\"], astro_kv_clean_2[\"POSTCODE_Coalesced\"], astro_kv_clean_2[\"CITY_Coalesced\"], astro_kv_clean_2[\"STATE_Coalesced\"]) ) ## 12/10/2022 - used AREA_coalesced instead of AREA\n\n## Pad the HOUSE_NO column not to change to date again --- THIS STEP was already done when cleaning HOUSE_NO above\n# astro_kv_clean_2 = astro_kv_clean_2.withColumn( 'ASTRO_HOUSE_NO1', f.lpad(f.col('HOUSE_NO').cast('string'), 10, ' ') )\n\n## save a temporary intermediate file (coz quite a lot of the previous cleaning steps caused spark cluster to crash when they were run on CSV files)\nastro_kv_clean_2.write.orc(temporary_save_path+'{0}/Astro_newMDUcleaningflow_1.orc'.format(curr_date), mode=\"overwrite\", compression='snappy')\n\ndel astro_kv_clean_2\n\nend = datetime.now()\nprint(str(end - now)) ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Implement the New Flow for ISP MDUs",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "now = datetime.now()\n\n#### ------------------------ Extracting BlockNo Flow (BLOCK_extracted) ------------------------\n### now do for TM New Std -- takes 38 mins to run as ORC\n## Note for TM, no need to extract HouseNo as they have it in their own column + they don't have a 'full' concatenated address column\n# tm_kv_clean: 1483206 rows, 43821 unique address_key, 717389 unique original_addr (excluding HouseNo)\n\n# # read Astro & TM files back in as orc - this seems to cause the rest of the steps to run faster\n# # also decided to rename astro_kv1 to astro_kv_clean directly and do the same for TM\ntm_kv_clean = spark.read.orc(temporary_save_path+'{0}/TM_MDUonly.orc'.format(curr_date))\n\n# print(tm_kv_clean.select('Combined_Building').count(), tm_kv_clean.select(f.countDistinct('ORIGINAL_ADDRESS')).show()) # 3,475,286 rows, 103824 unique address_key, 972413 unique original_addr (excluding HouseNo)\n\n## trim + uppercase\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.upper(f.trim(f.col('HouseNo'))))\n\n## Fix HouseNo. Replace specific strings (eg. \"BLOK\" ) with empty string (\"\"). 29/10/2022: added line to convert slashes to dashes\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"\\/\",\"-\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), 'NAN\\s?',''))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), ' ','')) # 29/10/22: remove spaces as from what I've seen, there shouldn't be any spaces in HouseNo\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), '[\\[\\]\\.,\"]',''))\n# tm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"[\",'')) # maybe it's this code which is causing the issue\n# tm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"]\",\"\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"'\",\"\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"NO\\s?\",\"\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"LOT\\s?\",\"\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"UNIT\\s?\",\"\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"BLO?C?K\\s?\",\"\"))\n## somehow the above block of code is giving a weird error - \"java.util.regex.PatternSyntaxException: Unclosed character class near index 0 [\" which doesn't allow me to show or save the table. So I made changes to it and commented out the code with the \"[\" and \"]\"\n\n# -- takes 5 mins to run just to run this portion as CSV\n## Fix HouseNo that were converted to date --> tested code for PySpark, looks like it works\n# 29/10/2022: removed the lines that deal with non-upper-cased months (e.g Jan, Feb) since we are making everything uppercased in line above. This is to reduce runtimes & lines of code\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JAN-\",\"01-\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JAN\",\"-01\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"FEB-\",\"02-\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-FEB\",'-02'))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"MAR-\",'03-'))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-MAR\",\"-03\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"APR-\",\"04-\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-APR\",\"-04\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"MAY-\",\"05-\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-MAY\",\"-05\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JUN-\",\"06-\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JUN\",\"-06\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"JUL-\",\"07-\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-JUL\",\"-07\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"AUG-\",'08-'))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-AUG\",\"-08\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"SEP-\",\"09-\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-SEP\",\"-09\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"OCT-\",\"10-\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-OCT\",\"-10\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"NOV-\",\"11-\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-NOV\",\"-11\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"DEC-\",\"12-\"))\ntm_kv_clean = tm_kv_clean.withColumn('HouseNo', f.regexp_replace(f.col('HouseNo'), \"-DEC\",\"-12\"))\n\n# Fix HouseNo that are converted to date (DD/MM/YYYY format) --> tested code for PySpark. Looks like it works\n# Filter date HouseNo\ndate_house = tm_kv_clean.filter(f.regexp_extract('HouseNo', '^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$', 0) != '' ) \n# Spliting the HOUSE_NO\ndate_house = date_house.withColumn('block_date',  f.substring(date_house.HouseNo, 1, 2))\ndate_house = date_house.withColumn('floor',  f.substring(date_house.HouseNo, 4, 2))\ndate_house = date_house.withColumn('unit',  f.substring(date_house.HouseNo, 9, 2))\n# Combine the split HouseNo with dashes: '-'\ndate_house = date_house.withColumn('HOUSE_NO_TM', f.concat_ws('-', date_house.block_date, date_house.floor, date_house.unit))\n# Remove additional column created to combine HouseNo\ndate_house = date_house.drop(*['block_date','floor','unit']) # 0 rows\n\n# Filter not date HouseNo\nnot_date_house = tm_kv_clean.filter( f.regexp_extract('HouseNo', '^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$', 0) == '' )\nnot_date_house = not_date_house.withColumn('HOUSE_NO_TM', f.col('HouseNo'))\n\n# Append the 2 df again --> for TM, we don't need to append null HOUSE_NO because there are no nulls in that column\ntm_kv_clean_1 = date_house.union(not_date_house)\n# print(tm_kv_clean_1.select('ORIGINAL_ADDRESS').count(), tm_kv_clean_1.select(f.countDistinct('ORIGINAL_ADDRESS')).show()) tm_kv_clean_1: 3407094 rows, 974034 unique original_addr (excluding HouseNo)\n\n## pad the values so it won't convert to date again -- note this step is done slightly different compared to the same padding step in code cell for Astro HOUSE_NO cleaning above as I keep HOUSE_NO_TM as the padded column. This is so I can compare the house numbers from TM file & Astro file after P1P2 mapping\ntm_kv_clean_1 = tm_kv_clean_1.withColumn('HOUSE_NO_TM', f.lpad(tm_kv_clean_1['HOUSE_NO_TM'], 10, ' '))\n\n# -- takes 20s as ORC\n\n## remove commas, dots (other special characters?) in Combined_Building\ntm_kv_clean_1 = tm_kv_clean_1.withColumn('Combined_Building', f.regexp_replace('Combined_Building', '[\\.,\"]', '') )\ntm_kv_clean_1 = tm_kv_clean_1.withColumn('Combined_Building', f.regexp_replace('Combined_Building', \"'\", \"\") )\n\n## create BLOCK_extracted by: extract after the word BLOCK/BLOK from Combined_Building --> extract after the word BLOCK/BLOK from ORIGINAL_ADDRESS --> extract after the word BLOCK/BLOK from HOUSE_NO_TM --> extract from character before dash in HUSE_NO --> extract from character before dashes in ORIGINAL_ADDRESS\n## 29/10/22: in bottom 3 code-lines (extracting from strings with dashes) changed the '\\w+' to \\w{1,3} to enforce exctraction of only alphanumeric words of length 1-3\nfrom pyspark.sql.functions import when\ntm_kv_clean_1 = tm_kv_clean_1.withColumn('BLOCK_extracted', \n                    when( f.col('Combined_Building').rlike('BLO?C?K \\w{1,3}'), f.regexp_extract( f.col('Combined_Building'), '(?<=BLO?C?K.?\\s)\\w+', 0) ) # check for BLOCK regex in Combined_Building (BLK 2A) & get the first 'word' after it ('2A')\n                    .when( f.col('ORIGINAL_ADDRESS').rlike('BLO?C?K \\w{1,3}'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<=BLO?C?K.?\\s)\\w+', 0) ) # check for BLOCK regex in ORIGINAL_ADDRESS (BLK 2A) & get the first 'word' after it ('2A')\n                    .when( f.col('HOUSE_NO_TM').rlike('BLO?C?K \\w{1,3}'), f.regexp_extract( f.col('HOUSE_NO_TM'), '(?<=BLO?C?K.?\\s)\\w+', 0) ) # check for BLOCK regex in HOUSE_NO_TM (BLK 2A) & get the first 'word' after it ('2A')\n                    \n                    .when( f.col('HOUSE_NO_TM').rlike('(?<=,*)\\w{1,3}(?=,?-\\w{1,3}-)'), f.regexp_extract( f.col('HOUSE_NO_TM'), '(?<=,*)\\w{1,3}(?=,?-\\w{1,3}-)', 0) )  # check for HOUSE_NO_TM that has 2 dashes using regex (B-2-1) & get the 1st 1st character ('B'). Use * in lookbehind instead of + because HOUSE_NO_TM probably won't have commas before the number anymore. \n                    .when( f.col('ORIGINAL_ADDRESS').rlike('(?<=,*)\\w{1,3}(?=,?-\\w{1,3}-)'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<=,*)\\w{1,3}(?=,?-\\w{1,3}-)', 0) )  # check for ORIGINAL_ADDRESS that has 2 dashes using regex (B-2-1) & get the 1st character ('B'). \n                    .when( f.col('HOUSE_NO_TM').rlike('\\w{1,3}\\s?-\\w{1,3}'), f.regexp_extract( f.col('HOUSE_NO_TM'), '\\w{1,3}(?=-\\w+)', 0) ) # check for HOUSE_NO_TM that has 1 dash using regex (B-2) & only take 1st character from start of string from HOUSE_NO_TM ('B'). \n                    # .when( f.col('HOUSE_NO_TM').rlike('.+-.+'), 'noBlock' ) # maybe BlockNo not extracted from EDW step?\n                    # .when( f.col('ORIGINAL_ADDRESS').rlike('BLO?C?K'), f.regexp_extract( f.col('ORIGINAL_ADDRESS'), '(?<=BLO?C?K\\s)\\w+', 0) ) # check for BLOCK regex in ORIGINAL_ADDRESS,\n                    # .when( f.col('HOUSE_NO_TM').isNull(), f.col('HOUSE_NO_TM') ) # just returns null for next step\n                    .otherwise( 'noBlock' ) # cases where no Block No was extracted in EDW Step\n                    )\n\n## save a temporary intermediate file (coz subsequent steps still caused spark cluster to crash, even when using tm_kv1 ORC file) -- takes 5 mins to run\ntm_kv_clean_1.write.orc(temporary_save_path+'{0}/TM_newMDUcleaningflow.orc'.format(curr_date), mode=\"overwrite\", compression='snappy')\ndel tm_kv_clean_1\n\nend = datetime.now()\nprint(str(end - now)) ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "now=datetime.now()\n\n# read file back in as orc - this seems to cause most of cleaning steps to run faster\ntm_kv_clean_1 = spark.read.orc(temporary_save_path+'{0}/TM_newMDUcleaningflow.orc'.format(curr_date))\n# print(tm_kv_clean_1.select('ORIGINAL_ADDRESS').count(), tm_kv_clean_1.select(f.countDistinct('ORIGINAL_ADDRESS')).show()) # tm_kv_clean_1: 3407094 rows, 972413 unique original_addr (excluding HouseNo)\n\n#### ------------------------------------------------ POTENTIAL OVERALL FLOW ISP ------------------------------------------------\n# TM New Std Building cleaning -- takes 1 sec to run as ORC\n\n# - Potential Overall Flow: extract BlockNo --> create new test new_building_name column --> remove FTTH --> standardize CONDO --> remove BLOCK + 'word' after it --> remove FLAT, PPR, PANGSAPURI, APARTMENT --> further clean BLOCK_extracted column before appending --> (For P1 MDU, consider new column for this step, maybe new_block_building_name) append the extracted BlockNo to the front of new_building_name --> consider making a Set out of the words in new_building_name (avoid repeat words) and alphabetically order words (create joined_set) --> use joined_set in mapping key\n\n## BlockNo extracted in earlier step\n## create new_building_name column for testing\ntm_kv_clean_2 = tm_kv_clean_1.withColumn('new_building_name', f.col('Combined_Building') )\n\n## Remove 'FTTH' from Combined Building\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', 'FTTH\\s', '') ) # removes any 'FTTH ' at start of line\n\n## remove BLOCK from new_building_name\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', 'BLO?C?K\\s?\\w{1,3}(?=\\s[A-Z]*|$)', '') ) # removes any 'BLK/BLOK/BLOCK 13A' where the word/character set after BLOK is between 1-3 characters. Does not remove if the word after BLOK is more than 3 characters e.g won't remove 'BLK/BLOK/BLOCK TERATAI'\n\n## standardize CONDO --> remove FLAT, PPR, PANGSAPURI, APARTMENT \ntm_kv_clean_2 = tm_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', '[CK]ONDO(?=\\s[A-Z]*|$)', 'CONDOMINIUM') ) # replaces C/KONDO with 'CONDOMINIUM'\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', 'KONDOMINIUM', 'CONDOMINIUM') ) # replaces KONDOMINIUM with 'CONDOMINIUM'\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', '\\s?FLAT\\s?', '') ) # removes any 'FLAT'\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', '\\s?PPR\\s?', '') ) # removes any 'PPR'\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', '\\s?AP\\w*T\\s?', '') ) # removes any word that starts with AP and ends with T (APARTMENT) \ntm_kv_clean_2 = tm_kv_clean_2.withColumn('new_building_name', f.regexp_replace('new_building_name', '\\s?P.*PURI\\s?', '') ) # removes any word that starts with P and ends with PURI (PANGSAPURI)\n\n## clean BLOCK_extracted column before appending (coz still got 'noBlock' and weird values where it's the condo name then a comma then only the Block No) # -- takes 1 min to run\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('BLOCK_extracted', \n                    when( f.col('BLOCK_extracted').contains(','), f.regexp_extract( f.col('BLOCK_extracted'), '(?<=,).+', 0) )\n                    .when( f.col('BLOCK_extracted') == 'noBlock', '')\n                    .otherwise( f.col('BLOCK_extracted') ) \n                    )\n\n## -- takes 1 sec to run as ORC\n## create new column which appends BLOCK_extracted to new_building_name (P1 MDU) but keep new_building_name BLOK-less for P2 MDU\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('new_block_building_name', f.concat_ws(' ', tm_kv_clean_2.BLOCK_extracted, tm_kv_clean_2.new_building_name) )\n\n## trim & uppercase\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('new_building_name', f.trim(f.upper(tm_kv_clean_2.new_building_name)) )\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('new_block_building_name', f.trim(f.upper(tm_kv_clean_2.new_block_building_name)) )\n\n## make a set of the words in each row then alphabetically arrange (similar to fuzzy's TokenSetRatio: https://github.com/seatgeek/fuzzywuzzy/blob/master/fuzzywuzzy/fuzz.py)\n# relevant pyspark docs: https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html, https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.array_sort.html, https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.array_distinct.html\n# tm_kv_clean_2 = tm_kv_clean_2.withColumn('sorted_set', sorted(f.collect_set(f.split(f.col('new_block_building_name'), pattern=' ', limit=-1))) )\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('sorted_set', f.array_sort(f.array_distinct(f.split(f.col('new_block_building_name'), pattern=' '))) )\n\n## join the values in sorted_set back together into a string column\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('joined_set', f.concat_ws(\" \", f.col(\"sorted_set\")) )\n\n### final cleaning\n\n## ensuring the columns that will be used in the mapping key (P1P2 MDU/SDU) are trimmed + upper case\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('HOUSE_NO_TM', f.upper(f.trim(f.col('HOUSE_NO_TM')))) ## THIS STEP would cancel out the padding done when in code cell that cleaned HOUSE_NO above but since we need to add the HOUSE_NO_TM into the mapping key, maybe it's best to do the padding later on again. Or consider only padding if the file has to go through CSV again?\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('Street_1', f.upper(f.trim(f.col('Street_1'))))\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('joined_set', f.upper(f.trim(f.col('joined_set'))))\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('STD_CITY', f.upper(f.trim(f.col('STD_CITY'))))\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('AREA', f.upper(f.trim(f.col('AREA')))) \ntm_kv_clean_2 = tm_kv_clean_2.withColumn('Section', f.upper(f.trim(f.col('Section'))))\n\n## 12/10/2022 - coalesce the multiple AREA columns\ntm_kv_clean_2 = tm_kv_clean_2.withColumn('AREA_Coalesced', f.coalesce(f.col('Section'), f.col('AREA')) ) \n\n## Combine address columns(HOUSE_NO_TM,...) to generate a new column named COMBINED_ADD - checked Zohreh's notebook for Step 3.0, apparently this part is not done for ISP. Probs coz ISP will be ISP_ADDRESS, based on homepasses\n# tm_kv_clean_2 = tm_kv_clean_2.withColumn('COMBINED_ADD', f.concat_ws(' ,', tm_kv_clean_2[\"HOUSE_NO_TM\"],  tm_kv_clean_2[\"Combined_Building\"], tm_kv_clean_2[\"Street_1\"], tm_kv_clean_2[\"AREA\"], tm_kv_clean_2[\"POSTCODE\"], tm_kv_clean_2[\"STD_CITY\"], tm_kv_clean_2[\"ASTRO_STATE\"]) )\n\n## Pad the HOUSE_NO_TM column not to change to date again --- THIS STEP was already done when cleaning HOUSE_NO above\n# tm_kv_clean_2 = tm_kv_clean_2.withColumn( 'TM_HOUSE_NO1', f.lpad(f.col('HOUSE_NO_TM').cast('string'), 10, ' ') )\n\nprint('tm_kv_clean_2 columns:', tm_kv_clean_2.columns)\nprint('final tm_kv_clean_2 count:', tm_kv_clean_2.select('joined_set').count())\n\n## save a temporary intermediate file (coz quite a lot of the previous cleaning steps caused spark cluster to crash when they were run on CSV files) -- takes 1.5min to run as ORC\ntm_kv_clean_2.write.orc(temporary_save_path+'{0}/TM_newMDUcleaningflow_1.orc'.format(curr_date), mode=\"overwrite\", compression='snappy')\n\ndel tm_kv_clean_2\n\nend = datetime.now()\nprint(str(end - now)) ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "tm_kv_clean_2 columns: ['ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'CITY_GAPI', 'STATE_GAPI', 'COUNTRY', 'POSTCODE_GAPI', 'match', 'STD_CITY', 'BuildingName', 'City', 'FloorNo', 'HouseNo', 'Postcode', 'Section', 'ServiceType', 'State', 'StreetName', 'StreetType', 'Postcode_Length', 'ADDRESS', 'ADDRESS_KEY', 'Combined_Building', 'Source', 'Standard_Building_Name', 'ADDRESS_MDU', 'Address_Type', 'TM_Street', 'Street_coalesced', 'CITY_coalesced', 'HOUSE_NO_TM', 'BLOCK_extracted', 'new_building_name', 'new_block_building_name', 'sorted_set', 'joined_set', 'AREA_Coalesced']\nfinal tm_kv_clean_2 count: 3676163\n0:01:14.425750\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Start of P1P2 MDU Mapping\nFirst, read in Astro & TM Files post-new MDU Cleaning Flow",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "print(astro_kv_clean_2.count()) # 716792\nprint(tm_kv_clean_2.count()) # 3407094",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "713550\n3676163\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# del astro_kv_clean_2 # - if required\n# read file back in as orc - this seems to cause most of cleaning steps to run faster\nastro_kv_clean_2 = spark.read.orc(temporary_save_path+'{0}/Astro_newMDUcleaningflow_1.orc'.format(curr_date))\n\n# print(astro_kv_clean_2.count()) # 716792\n\n# del tm_kv_clean_2 # - if required\n# read file back in as orc - this seems to cause most of cleaning steps to run faster\ntm_kv_clean_2 = spark.read.orc(temporary_save_path+'{0}/TM_newMDUcleaningflow_1.orc'.format(curr_date))\n# print(tm_kv_clean_2.count()) # 3407094\n\nprint(astro_kv_clean_2.columns)\nprint(tm_kv_clean_2.columns)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "['ACCOUNT_NO', 'ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'CITY', 'STATE', 'COUNTRY', 'POSTCODE', 'match', 'Combined_Building', 'ASTRO_CITY', 'ASTRO_STATE', 'cust_location', 'service_add_objid', 'STD_CITY', 'HOUSE_NO', 'Token_Sort_Ratio', 'rank', 'Standard_Building_Name', 'Address_ID', 'Street_Type', 'Building_Coalesced', 'AREA_Coalesced', 'CITY_Coalesced', 'POSTCODE_Coalesced', 'STATE_Coalesced', 'Source', 'HOUSE_NO_old', 'HOUSE_NO_ASTRO', 'BLOCK_extracted', 'new_building_name', 'new_block_building_name', 'sorted_set', 'joined_set', 'COMBINED_ADD']\n['ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'CITY_GAPI', 'STATE_GAPI', 'COUNTRY', 'POSTCODE_GAPI', 'match', 'STD_CITY', 'BuildingName', 'City', 'FloorNo', 'HouseNo', 'Postcode', 'Section', 'ServiceType', 'State', 'StreetName', 'StreetType', 'Postcode_Length', 'ADDRESS', 'ADDRESS_KEY', 'Combined_Building', 'Source', 'Standard_Building_Name', 'ADDRESS_MDU', 'Address_Type', 'TM_Street', 'Street_coalesced', 'CITY_coalesced', 'HOUSE_NO_TM', 'BLOCK_extracted', 'new_building_name', 'new_block_building_name', 'sorted_set', 'joined_set', 'AREA_Coalesced']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### P1 MDU Mapping",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "###  ----------------- Generate Mapping Key, then do P1 MDU Mapping -----------------\n\nnow = datetime.now()\n\n## Create Mapping Key using the new joined_set for P1 MDU then use the function above to clean it\nastro_kv_clean_2 = astro_kv_clean_2.withColumn( 'HNUM_STRT', f.concat_ws(' ,', f.col(\"HOUSE_NO\"), f.col(\"joined_set\"), f.col(\"STD_CITY\")) )\nastro_kv_clean_2 = clean_HNUM_STRT(astro_kv_clean_2, 'HNUM_STRT')\n\n## Create Mapping Key using the new joined_set for P1 MDU then use the function above to clean it\ntm_kv_clean_2 = tm_kv_clean_2.withColumn( 'HNUM_STRT_TM', f.concat_ws(' ,', f.col(\"HOUSE_NO_TM\"), f.col(\"joined_set\"), f.col(\"STD_CITY\")) )\ntm_kv_clean_2 = clean_HNUM_STRT(tm_kv_clean_2, 'HNUM_STRT_TM')\n\n### Update on 11/10/2022: only those with matching HOUSE_NO should be P1 MDU. If matching block or Building level only, then P2 MDU\n\n## Remove duplicates by generating a set of HNUM_STRT_TM values - code copied from Step 3.0 P1P2 Mapping\nwords = \" \".join(tm_kv_clean_2.agg(f.collect_list(f.col('HNUM_STRT_TM'))).collect()[0][0]).split()\nselection = set(words)\nselection1 = list(selection)\nprint('unique P1 MDU HNUM_STRT_TM', len(selection1)) # 2313517 unique HNUM_STRT_TM \n\n# create the spark DataFrame with defined column type (https://stackoverflow.com/questions/32742004/create-spark-dataframe-can-not-infer-schema-for-type). Then rename the column.\nselection2 = spark.createDataFrame(selection1, StringType())\nselection2 = selection2.withColumnRenamed('value', 'MAPPED_HNUM_STRT_TM')\n\n## pre-join row count:\n# astro_kv_clean_2 =  \n# selection2 = 2313517 \n\n# ## ----------------- MERGE/map for P1 MDU - on 11/10/2022, the HNUM_STRT for P1 MDU is now HOUSE_NO + joined_set + STD_CITY -----------------\n\nMAPPED_STRT_HNUM_Df = astro_kv_clean_2.join(selection2, on = astro_kv_clean_2.HNUM_STRT == selection2.MAPPED_HNUM_STRT_TM, how = 'inner')\nprint('P1 MDU count & unique acc_no post MERGE', MAPPED_STRT_HNUM_Df.select('HNUM_STRT').count(), MAPPED_STRT_HNUM_Df.select(f.countDistinct(f.col('ACCOUNT_NO'))).show()) # 63415 inner join matches but 63275 unique acc\n\nprint(MAPPED_STRT_HNUM_Df.columns)\n\n## -- whole cell takes about 30 sec to run\n## Making sure that joined_set has valid value by filtering things out\n# MAPPED_STRT_HNUM_Df[\"Combined_Building\"] = MAPPED_STRT_HNUM_Df['Combined_Building'].str.replace('[,.]','', case = False) # old code, might not need\n# MAPPED_STRT_HNUM_Df[\"Combined_Building\"] = MAPPED_STRT_HNUM_Df[\"Combined_Building\"].str.replace(\",\",\"\") # old code, might not need\nMAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.filter( f.col('joined_set').isNotNull())\nMAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.filter( f.col('joined_set')!= \"\")\nMAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.filter( f.col('joined_set')!= \" \")\nMAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.filter( f.col('joined_set')!= \"NAN\")\n\nprint('P1 MDU count & unique acc_no post filter out nulls & blanks', MAPPED_STRT_HNUM_Df.select('joined_set').count(), MAPPED_STRT_HNUM_Df.select(f.countDistinct('ACCOUNT_NO')).show()) # 63245 rows, 63106 unique acc left\n# MAPPED_STRT_HNUM_Df.select('joined_set').distinct().show(10) # can uncomment if you want to see unique values in joined_set\n\n## cleaning of Street Name\nMAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.withColumn('Street_1', when( f.col('Street_1') == 'NAN', '').otherwise(f.col('Street_1')) )\n\n## Method to ensure the dropDuplicates removes according to desired order. Refer: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\n# original pd code: MAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.sort_values(by='Source', ascending=False).drop_duplicates(subset=['ACCOUNT_NO','HOUSE_NO'],keep='first').drop_duplicates(subset='ACCOUNT_NO',keep='last') # from pyspark.sql import Window\nwindow = Window.partitionBy(['ACCOUNT_NO','HOUSE_NO']).orderBy(f.col(\"Source\").desc())\nMAPPED_STRT_HNUM_Df_1 = MAPPED_STRT_HNUM_Df.withColumn('row', f.row_number().over(window)).filter(f.col('row') == 1).drop('row')\n# print(MAPPED_STRT_HNUM_Df_1.select('joined_set').count(), MAPPED_STRT_HNUM_Df_1.select(f.countDistinct('ACCOUNT_NO')).show()) #  rows,  unique acc left\n\nwindow = Window.partitionBy('ACCOUNT_NO').orderBy(f.col(\"Source\").asc())\nMAPPED_STRT_HNUM_Df_2 = MAPPED_STRT_HNUM_Df_1.withColumn('row', f.row_number().over(window)).filter(f.col('row') == 1).drop('row')\n# print(MAPPED_STRT_HNUM_Df_2.select('joined_set').count(), MAPPED_STRT_HNUM_Df_2.select(f.countDistinct('ACCOUNT_NO')).show()) #  rows,  unique acc left\n\n## ensuring ACCOUNT_NO is string in both tables\nMAPPED_STRT_HNUM_Df_2 = MAPPED_STRT_HNUM_Df_2.withColumn( 'ACCOUNT_NO', MAPPED_STRT_HNUM_Df_2['ACCOUNT_NO'].cast('string') )\nMAPPED_STRT_HNUM_Df_2 = MAPPED_STRT_HNUM_Df_2.withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n\n## in Astro New Std, there are some null HOUSE_NO. For those cases, we can't regard as P1 MDU. Previously in the date_house step, these cases were filtered out. But I want to move them to P2 MDU\nMAPPED_STRT_HNUM_Df_2 = MAPPED_STRT_HNUM_Df_2.filter('HOUSE_NO is NOT NULL')\nprint('Final print of P1 MDU', MAPPED_STRT_HNUM_Df_2.select('joined_set').count(), MAPPED_STRT_HNUM_Df_2.select(f.countDistinct('ACCOUNT_NO')).show()) # 63106 rows, 63106 unique acc left\n\n## Save P1_MDU first for easier use later on (note this is before preparing the file as UAMS format \nP1_MDU = MAPPED_STRT_HNUM_Df_2 # 59682 rows, 59682 unique acc left\nP1_MDU.write.orc(temporary_save_path+'{0}/P1_MDU.orc'.format(curr_date), mode='overwrite', compression='snappy') # historical\n\n# automated save but select relevant columns only for use in next Job. Also coz you can't save DFs that have arrays in column into CSV\nP1_MDU = P1_MDU.select(['ACCOUNT_NO', 'ORIGINAL_ADDRESS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'COUNTRY', 'POSTCODE', \n                        'Combined_Building', 'ASTRO_STATE', 'service_add_objid', 'STD_CITY', 'HOUSE_NO', 'Standard_Building_Name',\n                        'Street_Type', 'Building_Coalesced', 'AREA_Coalesced', 'CITY_Coalesced', 'POSTCODE_Coalesced', 'STATE_Coalesced', 'Source', 'match',\n                        'joined_set', 'COMBINED_ADD', 'HNUM_STRT', 'MAPPED_HNUM_STRT_TM'])\nP1_MDU.write.csv(uams_mdu_path+'P1_MDU.csv.gz', mode='overwrite', compression='gzip', header=True) # automated\n\n# ------------------ Remove those that are P1 Mapped: Step below needs to be run before P2 MDU Mapping ------------------\n\n## ensuring ACCOUNT_NO is string in both tables\nastro_kv_clean_2 = astro_kv_clean_2.withColumn( 'ACCOUNT_NO', astro_kv_clean_2['ACCOUNT_NO'].cast('string') )\nastro_kv_clean_2 = astro_kv_clean_2.withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n\n## remove the mapped account no from P1 MDU mapping before P2 MDU mapping step\np1_mdu_acc_df = MAPPED_STRT_HNUM_Df_2.select('ACCOUNT_NO') # unique ACC_NO count is same as Final P1 MDU print above\n\n# ## filter out the P1 MDU accounts, leaving the unmapped base for P2 MDU mapping\nastro_kv_clean_3 = astro_kv_clean_2.join(p1_mdu_acc_df, on='ACCOUNT_NO', how='leftanti')\nprint('Unmapped base leftover for P2 MDU Mapping', astro_kv_clean_3.select('ACCOUNT_NO').count(), astro_kv_clean_3.select(f.countDistinct('ACCOUNT_NO')).show())  # 634563 rows & 539919 unique acc_no left\n\nend = datetime.now()\nprint(str(end - now))  # 0:01:11.070039",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "unique P1 MDU HNUM_STRT_TM 2313517\n+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                     63275|\n+--------------------------+\n\nP1 MDU count & unique acc_no post MERGE 63415 None\n['ACCOUNT_NO', 'ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'CITY', 'STATE', 'COUNTRY', 'POSTCODE', 'match', 'Combined_Building', 'ASTRO_CITY', 'ASTRO_STATE', 'cust_location', 'service_add_objid', 'STD_CITY', 'HOUSE_NO', 'Token_Sort_Ratio', 'rank', 'Standard_Building_Name', 'Address_ID', 'Street_Type', 'Building_Coalesced', 'AREA_Coalesced', 'CITY_Coalesced', 'POSTCODE_Coalesced', 'STATE_Coalesced', 'Source', 'HOUSE_NO_old', 'HOUSE_NO_ASTRO', 'BLOCK_extracted', 'new_building_name', 'new_block_building_name', 'sorted_set', 'joined_set', 'COMBINED_ADD', 'HNUM_STRT', 'MAPPED_HNUM_STRT_TM']\n+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                     63106|\n+--------------------------+\n\nP1 MDU count & unique acc_no post filter out nulls & blanks 63245 None\n+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                     63106|\n+--------------------------+\n\nFinal print of P1 MDU 63106 None\n+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                    539919|\n+--------------------------+\n\nUnmapped base leftover for P2 MDU Mapping 634563 None\n0:01:34.763235\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### P2 MDU Mapping",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "###  ----------------- Generate Mapping Key, then do P2 MDU Mapping -----------------\n\nnow=datetime.now()\n\n#### 12/10/2022: P1 MDU is now HouseNo, and for P2 MDU we want the highest number of matches. Thus, use new_building_name which is just cleaned Combined_Building in mapping key (maps at building level, but not block level)\n### final step of flow: use new_building_name in mapping key then clean mapping key -- takes 17 sec\n# can bypass all steps except for generation of HNUM_STRT. \n\n## Create Mapping Key using the new Combined_Building for P2 MDU then use the function above to clean it\nastro_kv_clean_3 = astro_kv_clean_3.withColumn( 'HNUM_STRT', f.concat_ws(' ,', f.col(\"new_building_name\"), f.col(\"STD_CITY\")) )\nastro_kv_clean_3 = clean_HNUM_STRT(astro_kv_clean_3, 'HNUM_STRT')\n\n## Create Mapping Key using the new Combined_Building for P2 MDU then use the function above to clean it\ntm_kv_clean_2 = tm_kv_clean_2.withColumn( 'HNUM_STRT_TM', f.concat_ws(' ,', f.col(\"new_building_name\"), f.col(\"STD_CITY\")) )\ntm_kv_clean_2 = clean_HNUM_STRT(tm_kv_clean_2, 'HNUM_STRT_TM')\n\n## Remove duplicates by generating a set of HNUM_STRT_TM values - code copied from Step 3.0 P1P2 Mapping\nwords = \" \".join(tm_kv_clean_2.agg(f.collect_list(f.col('HNUM_STRT_TM'))).collect()[0][0]).split()\nselection = set(words)\nselection1 = list(selection)\nprint('no of unique HNUM_STRT_TM', len(selection1)) # 87262\n\nfrom pyspark.sql.types import *\n# create the spark DataFrame with defined column type (https://stackoverflow.com/questions/32742004/create-spark-dataframe-can-not-infer-schema-for-type). Then rename the column.\nselection2 = spark.createDataFrame(selection1, StringType())\nselection2 = selection2.withColumnRenamed('value', 'MAPPED_HNUM_STRT_TM')\n\n## pre-join row _count:\n# astro_kv_clean_3 = 634563 rows & 539919 unique acc_no\n# selection2 = 87262\n\n# ## ----------------- MERGE/map for P2 MDU - on 11/10/2022, the HNUM_STRT for P2 MDU is now joined_set + STD_CITY -----------------\nMAPPED_STRT_HNUM_Df = astro_kv_clean_3.join(selection2, on = astro_kv_clean_3.HNUM_STRT == selection2.MAPPED_HNUM_STRT_TM, how = 'inner')\nprint('P2 MDU post-merge', MAPPED_STRT_HNUM_Df.select('HNUM_STRT').count(), MAPPED_STRT_HNUM_Df.select(f.countDistinct(f.col('ACCOUNT_NO'))).show()) # 245679 inner join matches, 210157 unique acc\nprint(MAPPED_STRT_HNUM_Df.columns)\n\n## -- whole cell takes about 30 sec to run\n\n## Making sure that joined_set has valid value by filtering things out\n# MAPPED_STRT_HNUM_Df[\"Combined_Building\"] = MAPPED_STRT_HNUM_Df['Combined_Building'].str.replace('[,.]','', case = False) # old code, might not need\n# MAPPED_STRT_HNUM_Df[\"Combined_Building\"] = MAPPED_STRT_HNUM_Df[\"Combined_Building\"].str.replace(\",\",\"\") # old code, might not need\nMAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.filter( f.col('new_building_name').isNotNull())\nMAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.filter( f.col('new_building_name')!= \"\")\nMAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.filter( f.col('new_building_name')!= \" \")\nMAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.filter( f.col('new_building_name')!= \"NAN\")\n\n# originally when filtering on 'joined_set', it was 243724 rows, 209020 unique acc left. When filtering on new_building_name, numbers are 214195 rows, 180425 unique acc\nprint('P2 MDU after filtering out nulls & blanks in joined_set',MAPPED_STRT_HNUM_Df.select('new_building_name').count(), MAPPED_STRT_HNUM_Df.select(f.countDistinct('ACCOUNT_NO')).show()) \n# MAPPED_STRT_HNUM_Df.select('joined_set').distinct().show(10) # can uncomment if you want to see unique values in joined_set\n\n## cleaning of Street Name\nMAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.withColumn('Street_1', when( f.col('Street_1') == 'NAN', '').otherwise(f.col('Street_1')) )\n\n## Method to ensure the dropDuplicates removes according to desired order. Refer: https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first\n# original pd code: MAPPED_STRT_HNUM_Df = MAPPED_STRT_HNUM_Df.sort_values(by='Source', ascending=False).drop_duplicates(subset=['ACCOUNT_NO','HOUSE_NO'],keep='first').drop_duplicates(subset='ACCOUNT_NO',keep='last') # from pyspark.sql import Window\nwindow = Window.partitionBy(['ACCOUNT_NO','HOUSE_NO']).orderBy(f.col(\"Source\").desc())\nMAPPED_STRT_HNUM_Df_1 = MAPPED_STRT_HNUM_Df.withColumn('row', f.row_number().over(window)).filter(f.col('row') == 1).drop('row')\n# print('2nd print',MAPPED_STRT_HNUM_Df_1.select('joined_set').count(), MAPPED_STRT_HNUM_Df_1.select(f.countDistinct('ACCOUNT_NO')).show()) #  rows,  unique acc left\n\nwindow = Window.partitionBy('ACCOUNT_NO').orderBy(f.col(\"Source\").asc())\nMAPPED_STRT_HNUM_Df_2 = MAPPED_STRT_HNUM_Df_1.withColumn('row', f.row_number().over(window)).filter(f.col('row') == 1).drop('row')\n# print('3rd print',MAPPED_STRT_HNUM_Df_2.select('joined_set').count(), MAPPED_STRT_HNUM_Df_2.select(f.countDistinct('ACCOUNT_NO')).show()) #  rows,  unique acc left\n\n## ensuring ACCOUNT_NO is string in both tables\nMAPPED_STRT_HNUM_Df_2 = MAPPED_STRT_HNUM_Df_2.withColumn( 'ACCOUNT_NO', MAPPED_STRT_HNUM_Df_2['ACCOUNT_NO'].cast('string') )\nMAPPED_STRT_HNUM_Df_2 = MAPPED_STRT_HNUM_Df_2.withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n\n## in Astro New Std, there are some null HOUSE_NO. For P2 MDU, should we still consider them?\nMAPPED_STRT_HNUM_Df_2 = MAPPED_STRT_HNUM_Df_2.filter('HOUSE_NO is NOT NULL')\nprint('Final print of P2 MDU',MAPPED_STRT_HNUM_Df_2.select('joined_set').count(), MAPPED_STRT_HNUM_Df_2.select(f.countDistinct('ACCOUNT_NO')).show()) # 180425 rows, 180425 unique acc\n\n## obtain mapped account no from P2 MDU mapping for removal from overall base in next step\np2_mdu_acc_df = MAPPED_STRT_HNUM_Df_2.select('ACCOUNT_NO') # unique ACC_NO count is same as Final P1 MDU print above\n\n## save P2_MDU file for easier use later on (note this is before preparing the file as UAMS format)\nP2_MDU = MAPPED_STRT_HNUM_Df_2 # 209020 rows, 209020 unique acc left\nP2_MDU.write.orc(temporary_save_path+'{0}/P2_MDU.orc'.format(curr_date), mode='overwrite', compression='snappy') # historical\n\n# automated save but select relevant columns only for use in next Job. Also coz you can't save DFs that have arrays in column into CSV\nP2_MDU = P2_MDU.select(['ACCOUNT_NO', 'ORIGINAL_ADDRESS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'COUNTRY', 'POSTCODE', \n                        'Combined_Building', 'ASTRO_STATE', 'service_add_objid', 'STD_CITY', 'HOUSE_NO', 'Standard_Building_Name',\n                        'Street_Type', 'Building_Coalesced', 'AREA_Coalesced', 'CITY_Coalesced', 'POSTCODE_Coalesced', 'STATE_Coalesced', 'Source', 'match',\n                        'joined_set', 'COMBINED_ADD', 'HNUM_STRT', 'MAPPED_HNUM_STRT_TM'])\nP2_MDU.write.csv(uams_mdu_path+'P2_MDU.csv.gz', mode='overwrite', compression='gzip', header=True) # automated\n\nend = datetime.now()\nprint(str(end - now)) # 0:01:02.965288",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "no of unique HNUM_STRT_TM 87262\n+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                    210157|\n+--------------------------+\n\nP2 MDU post-merge 245679 None\n['ACCOUNT_NO', 'ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'CITY', 'STATE', 'COUNTRY', 'POSTCODE', 'match', 'Combined_Building', 'ASTRO_CITY', 'ASTRO_STATE', 'cust_location', 'service_add_objid', 'STD_CITY', 'HOUSE_NO', 'Token_Sort_Ratio', 'rank', 'Standard_Building_Name', 'Address_ID', 'Street_Type', 'Building_Coalesced', 'AREA_Coalesced', 'CITY_Coalesced', 'POSTCODE_Coalesced', 'STATE_Coalesced', 'Source', 'HOUSE_NO_old', 'HOUSE_NO_ASTRO', 'BLOCK_extracted', 'new_building_name', 'new_block_building_name', 'sorted_set', 'joined_set', 'COMBINED_ADD', 'HNUM_STRT', 'MAPPED_HNUM_STRT_TM']\n+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                    180425|\n+--------------------------+\n\nP2 MDU after filtering out nulls & blanks in joined_set 214195 None\n+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                    180425|\n+--------------------------+\n\nFinal print of P2 MDU 180425 None\n0:01:26.687499\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ## 23/12/2022 - trying to save the file using Pandas methods -- but kept getting errors related to Memory e.g either Spark session would crash or the Glue job session would crash... \n# # --> Instead added a new argument to the spark write.csv function (escape =None) and that file seems to be able to open in Pandas format woohoo!\n\n# now=datetime.now()\n\n# def save_unmapped_base_using_pandas():\n#     # ## filter out the P1 & P2 MDU accounts from the TOTAL base, leaving the unmapped base for P1P2 SDU Mapping in next Job\n#     astro_new_std_all = spark.read.csv(astro_new_std_path, header=True) # count: 4675485\n#     p1_mdu_acc_df = spark.read.orc(temporary_save_path+'{0}/P1_MDU.orc'.format(curr_date)).select('ACCOUNT_NO')\n#     p2_mdu_acc_df = spark.read.orc(temporary_save_path+'{0}/P2_MDU.orc'.format(curr_date)).select('ACCOUNT_NO')\n\n#     ## ensuring ACCOUNT_NO is string in all tables\n#     astro_new_std_all = astro_new_std_all.withColumn( 'ACCOUNT_NO', f.col('ACCOUNT_NO').cast('string') ).withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n#     p1_mdu_acc_df = p1_mdu_acc_df.withColumn( 'ACCOUNT_NO',  f.col('ACCOUNT_NO').cast('string') ).withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n#     p2_mdu_acc_df = p2_mdu_acc_df.withColumn( 'ACCOUNT_NO',  f.col('ACCOUNT_NO').cast('string') ).withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n\n#     # OLD filter method: astro_unmapped = astro_new_std.filter(~f.col('ACCOUNT_NO').isin(p1_mdu_acc_list)).filter(~f.col('ACCOUNT_NO').isin(p2_mdu_acc_list)) # <-- inefficient method of filtering based on a list\n#     ## filter using MERGE\n#     astro_unmapped = astro_new_std_all.join(p1_mdu_acc_df, on='ACCOUNT_NO', how='leftanti').join(p2_mdu_acc_df, on='ACCOUNT_NO', how='leftanti')\n#     astro_unmapped_pd = astro_unmapped.toPandas()\n#     print( 'Unmapped Base, to push to next job', astro_unmapped_pd.shape, astro_unmapped_pd.ACCOUNT_NO.nunique() )\n\n#     ## save this unmapped base as pandas\n#     wr.s3.to_csv(df = astro_unmapped_pd, path = temporary_save_path + '{0}/astro_new_std-mapped_mdu.csv.gz'.format(curr_date),  compression='gzip', index=False)\n#     return\n\n\n# save_unmapped_base_using_pandas()\n\n# end = datetime.now()\n# print(str(end - now)) # ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "0:02:50.424217\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ------------------ Remove those that are P1 & P2 MDU Mapped from the total Astro New Std BASE: Step below needs to be run, then push to a path for the next Job to read\nnow=datetime.now()\n\ndef save_unmapped_base():\n    # ## filter out the P1 & P2 MDU accounts from the TOTAL base, leaving the unmapped base for P1P2 SDU Mapping in next Job\n    astro_new_std_all = spark.read.csv(astro_new_std_path, header=True) # count: 4675485\n    p1_mdu_acc_df = spark.read.orc(temporary_save_path+'{0}/P1_MDU.orc'.format(curr_date)).select('ACCOUNT_NO')\n    p2_mdu_acc_df = spark.read.orc(temporary_save_path+'{0}/P2_MDU.orc'.format(curr_date)).select('ACCOUNT_NO')\n\n    ## ensuring ACCOUNT_NO is string in all tables\n    astro_new_std_all = astro_new_std_all.withColumn( 'ACCOUNT_NO', f.col('ACCOUNT_NO').cast('string') ).withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n    p1_mdu_acc_df = p1_mdu_acc_df.withColumn( 'ACCOUNT_NO',  f.col('ACCOUNT_NO').cast('string') ).withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n    p2_mdu_acc_df = p2_mdu_acc_df.withColumn( 'ACCOUNT_NO',  f.col('ACCOUNT_NO').cast('string') ).withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n\n    # OLD filter method: astro_unmapped = astro_new_std.filter(~f.col('ACCOUNT_NO').isin(p1_mdu_acc_list)).filter(~f.col('ACCOUNT_NO').isin(p2_mdu_acc_list)) # <-- inefficient method of filtering based on a list\n    ## filter using MERGE\n    astro_unmapped = astro_new_std_all.join(p1_mdu_acc_df, on='ACCOUNT_NO', how='leftanti').join(p2_mdu_acc_df, on='ACCOUNT_NO', how='leftanti')\n    print('Unmapped Base, to push to next job', astro_unmapped.select('ACCOUNT_NO').count(), astro_unmapped.select(f.countDistinct('ACCOUNT_NO')).show())  \n    # after filtering out P2 MDU on blank/null joined_set: 4296644 rows & 3918444 unique acc_no left\n    # after filtering out P2 MDU on blank/null new_building_name: 4330080 rows & 3947039 unique acc_no left\n\n    ## save this unmapped base\n    astro_unmapped.write.csv(temporary_save_path+'{0}/astro_new_std-mapped_mdu.csv.gz'.format(curr_date), mode='overwrite', compression='gzip', header=True) # historical base\n    astro_unmapped.write.csv(uams_mdu_path+'astro_new_std-mapped_mdu.csv.gz', mode='overwrite', compression='gzip', header=True) # automated base # 25/12/2022, added escape=None coz the CSV file seems to open alright using Pandas read csv\n    # , quoteAll='true' # <-- extra write.csv argument that I experimented with \n    print(astro_unmapped.columns)\n\n# del astro_new_std_all\n# del astro_unmapped\nsave_unmapped_base()\n\nend = datetime.now()\nprint(str(end - now)) # 0:04:15.377729",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------------+\n|count(DISTINCT ACCOUNT_NO)|\n+--------------------------+\n|                   3947039|\n+--------------------------+\n\nUnmapped Base, to push to next job 4330080 None\n['ACCOUNT_NO', 'ORIGINAL_ADDRESS', 'GEOHASH', 'LOCATION_TYPE', 'TRANSFORMED_ADDRESS', 'NUMBER_OF_RESULTS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'CITY', 'STATE', 'COUNTRY', 'POSTCODE', 'match', 'Combined_Building', 'ASTRO_CITY', 'ASTRO_STATE', 'cust_location', 'service_add_objid', 'STD_CITY', 'HOUSE_NO', 'Token_Sort_Ratio', 'rank', 'Standard_Building_Name', 'Address_ID', 'Street_Type', 'Building_Coalesced', 'AREA_Coalesced', 'CITY_Coalesced', 'POSTCODE_Coalesced', 'STATE_Coalesced', 'Source']\n0:04:22.810360\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Below few cells are for experimenting and trying to figure out why I can't read in the spark-saved CSV in Pandas:\nThe errors I were getting were either: \n- ParserError: ',' expected after '\"'\n- Error tokenising data. C error: EOF inside string starting at line\n\nI continued more experiments on Qubole @ https://us.qubole.com/notebooks#home?id=143539&type=my-notebooks&view=home when testing PySpark and the local Jupyter notebook called \"Studying astro_new_std-mapped_mdu.ipynb\" when testing Pandas code. Using PySpark, I found that the Spark-saved CSV file (using default parameters/args) contained the value '\\\"\\\"' in Combined_Building & Building_Coalesced columns. After studying the documentations for Pyspark when reading & writing csv & for Pandas when using read_csv, I realized that PySpark uses \\ as its default 'escape' argument while pandas has None as its default 'escapechar' argument. \n\nAlso, Spark uses \"\" as its default 'emptyValue' argument which is the STRING representation of an empty value. This sounds like everytime there is a blank ('') string value in the column, Spark by default saves that string as \"\". This combined with the \\ as the default 'escape' argument may explain why I get \\\"\\\" values in some columns where I assume it should be blank. Note: when reading in a CSV file, if you use emptyValue = '\"\"', then it will actually convert null values in string columns to \"\"",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# ### EXPERIMENT---------------- used codes in code cell above\n# astro_new_std_all = spark.read.csv(astro_new_std_path, header=True) # count: 4675485\n# p1_mdu_acc_df = spark.read.orc(temporary_save_path+'{0}/P1_MDU.orc'.format(curr_date)).select('ACCOUNT_NO')\n# p2_mdu_acc_df = spark.read.orc(temporary_save_path+'{0}/P2_MDU.orc'.format(curr_date)).select('ACCOUNT_NO')\n\n# ## ensuring ACCOUNT_NO is string in all tables\n# astro_new_std_all = astro_new_std_all.withColumn( 'ACCOUNT_NO', f.col('ACCOUNT_NO').cast('string') ).withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n# p1_mdu_acc_df = p1_mdu_acc_df.withColumn( 'ACCOUNT_NO',  f.col('ACCOUNT_NO').cast('string') ).withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n# p2_mdu_acc_df = p2_mdu_acc_df.withColumn( 'ACCOUNT_NO',  f.col('ACCOUNT_NO').cast('string') ).withColumn( 'ACCOUNT_NO', f.regexp_replace('ACCOUNT_NO', '\\.0','') )\n\n# # OLD filter method: astro_unmapped = astro_new_std.filter(~f.col('ACCOUNT_NO').isin(p1_mdu_acc_list)).filter(~f.col('ACCOUNT_NO').isin(p2_mdu_acc_list)) # <-- inefficient method of filtering based on a list\n# ## filter using MERGE\n# astro_unmapped = astro_new_std_all.join(p1_mdu_acc_df, on='ACCOUNT_NO', how='leftanti').join(p2_mdu_acc_df, on='ACCOUNT_NO', how='leftanti')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# astro_unmapped.schema",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "StructType(List(StructField(ACCOUNT_NO,StringType,true),StructField(ORIGINAL_ADDRESS,StringType,true),StructField(GEOHASH,StringType,true),StructField(LOCATION_TYPE,StringType,true),StructField(TRANSFORMED_ADDRESS,StringType,true),StructField(NUMBER_OF_RESULTS,StringType,true),StructField(FORMATTED_ADDRESS,StringType,true),StructField(Street_1,StringType,true),StructField(Street_2,StringType,true),StructField(AREA,StringType,true),StructField(CITY,StringType,true),StructField(STATE,StringType,true),StructField(COUNTRY,StringType,true),StructField(POSTCODE,StringType,true),StructField(match,StringType,true),StructField(Combined_Building,StringType,true),StructField(ASTRO_CITY,StringType,true),StructField(ASTRO_STATE,StringType,true),StructField(cust_location,StringType,true),StructField(service_add_objid,StringType,true),StructField(STD_CITY,StringType,true),StructField(HOUSE_NO,StringType,true),StructField(Token_Sort_Ratio,StringType,true),StructField(rank,StringType,true),StructField(Standard_Building_Name,StringType,true),StructField(Address_ID,StringType,true),StructField(Street_Type,StringType,true),StructField(Building_Coalesced,StringType,true),StructField(AREA_Coalesced,StringType,true),StructField(CITY_Coalesced,StringType,true),StructField(POSTCODE_Coalesced,StringType,true),StructField(STATE_Coalesced,StringType,true),StructField(Source,StringType,true)))\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ## filter all rows which have a weird string value\n# problem_rows = spark.createDataFrame([], schema= astro_unmapped.schema)\n# col_to_check = ['Combined_Building', 'ASTRO_CITY', 'ASTRO_STATE', 'STD_CITY', 'HOUSE_NO', 'Street_Type', 'Building_Coalesced', 'AREA_Coalesced', 'CITY_Coalesced', 'POSTCODE_Coalesced', 'STATE_Coalesced', 'Source']\n# for col in col_to_check:\n#      # problem_rows = problem_rows.union( astro_unmapped.filter(f.col(col).contains(\"`\") | f.col(col).contains('\"') | f.col(col).contains('\\') )  )\n#     problem_rows = problem_rows.union( astro_unmapped.filter( f.col(col).contains('\"') | f.col(col).contains('\\') )  )\n    \n# problem_rows.select('ACCOUNT_NO').count()",
			"metadata": {},
			"execution_count": null,
			"outputs": [
				{
					"name": "stdout",
					"text": "14259\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Prepare the P1 & P2 MDU files into UAMS format\n<!-- , go to the next Job\nThe function (generate_uams_format) was defined in the next Job, i.e. amzar-address_standardization-prod-p1p2_mapping_tm @ https://ap-southeast-1.console.aws.amazon.com/gluestudio/home?region=ap-southeast-1#/editor/job/amzar-address_standardization-prod-p1p2_mapping_tm/script -->",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# ## checking columns in one of the tables created in this note\n# # df = spark.read.csv(uams_mdu_path+'astro_new_std-mapped_mdu.csv.gz')\n# df = spark.read.csv(uams_mdu_path+'P2_MDU.csv.gz', header=True)\n# df.columns",
			"metadata": {
				"trusted": true
			},
			"execution_count": 66,
			"outputs": [
				{
					"name": "stdout",
					"text": "['ACCOUNT_NO', 'ORIGINAL_ADDRESS', 'FORMATTED_ADDRESS', 'Street_1', 'Street_2', 'AREA', 'COUNTRY', 'POSTCODE', 'Combined_Building', 'ASTRO_STATE', 'service_add_objid', 'STD_CITY', 'HOUSE_NO', 'Standard_Building_Name', 'Street_Type', 'Building_Coalesced', 'AREA_Coalesced', 'CITY_Coalesced', 'POSTCODE_Coalesced', 'STATE_Coalesced', 'Source', 'match', 'joined_set', 'COMBINED_ADD', 'HNUM_STRT', 'MAPPED_HNUM_STRT_TM']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def extract_street(item): # added on 12/12/22 after I noticed that this function gets defined multiple times in the original script\n    \"\"\"This function is to extract the street type from the column that has the full street name\"\"\"\n    import re\n    street_type = \"\"\n    \n    r1 = \"JALAN|LORONG|CHANGKAT|LAMAN|LAHAT|LEBUH|LEBUHRAYA|LENGKOK|LINGKARAN|PERSIARAN\"\n\n\n    m = re.search(r1,item)\n    if m:\n        street_type = m.group()\n    return street_type      \n\ndef generate_uams_format(input_df, p_base): # copied from amzar-address_standardization-prod-p1p2_mapping_tm Job\n    \"\"\"This function is to generate the P1P2 MDU/SDU files into the UAMS format & get ServiceType before saving\"\"\"\n    \n    STRT_P1 = input_df.copy() # Amzar 9/9/2022 --> added copy() to create an explicit copy\t\n    print(p_base, 'STRT_P1 dataframe shape: ', STRT_P1.shape) # Amzar 9/9/2022 --> added new print statement to see shape of STRT_P1 variable\n    STRT_P1['Street_1'] = STRT_P1['Street_1'].astype(str)\n    STRT_P1.reset_index(inplace=True, drop=True)\n    test = STRT_P1.loc[STRT_P1['Street_1'].apply(lambda x: x.startswith('AA')), :].index # Amzar 9/9/2022 --> added loc statement\n    test = list(test)\n    STRT_P1.loc[test,'Street_1'] = ''\n    STRT_P1.loc[STRT_P1['match']=='Match','Street_2'] = ''\n    STRT_P1[STRT_P1['match']=='Match']\n    \n    ## Apply extract_street function defined above    \n    STRT_P1[\"Street_Type_1\"] = STRT_P1[\"Street_1\"].apply(extract_street)\n    STRT_P1[\"Street_Type_2\"] = STRT_P1[\"Street_2\"].map(str).apply(extract_street)\n    STRT_P1.head()\n    street_type_list = ['JALAN ', 'LORONG ','CHANGKAT ', 'LAMAN ', 'LAHAT ', 'LEBUH ', 'LEBUHRAYA ', 'LENGKOK ','LINGKARAN ', 'PERSIARAN ' ]\n    STRT_P1[\"Street_1_New\"] = STRT_P1[\"Street_1\"].str.replace('|'.join(street_type_list), '')\n    STRT_P1[\"Street_2\"] = STRT_P1[\"Street_2\"].str.upper()\n    STRT_P1[\"Street_2_New\"] = STRT_P1[\"Street_2\"].str.replace('|'.join(street_type_list), '')\n    STRT_P1.head()\n    \n    # Getting the ServiceType\n    service_list = isp_corrected.loc[:, ['ServiceType','HNUM_STRT_TM']].drop_duplicates() # Amzar 9/9/2022 --> added loc\n    service_list[\"ServiceType\"] = service_list[\"ServiceType\"].astype(str).str.upper()\n    service_list = service_list[service_list['ServiceType']!='ERROR']\n    New_fields1 = pd.merge(STRT_P1,service_list,left_on ='HNUM_STRT',right_on='HNUM_STRT_TM', how = 'left')\n    New_fields1.info()\n    #MDU NEW\n    New_fields2 = New_fields1[['ACCOUNT_NO','service_add_objid','ASTRO_HOUSE_NO1',\n                               'Combined_Building','Street_Type_1','Street_1_New','Standard_Building_Name',\n                               'Street_Type_2','Street_2_New','AREA','POSTCODE','STD_CITY','ASTRO_STATE', 'ServiceType','HNUM_STRT_TM']]\n    New_fields2.loc[:, 'Servicable']= str(ISP_Name) # Amzar 9/9/2022 --> added loc \n    # New_fields3 = New_fields2.drop_duplicates(subset= 'ACCOUNT_NO', keep = 'first')\n    New_fields3 = New_fields2.sort_values(['ServiceType']).drop_duplicates(subset= 'ACCOUNT_NO', keep = 'first')\n    \n    astro_cleaned = New_fields3.copy() # Amzar 9/9/2022 --> added copy()\n    print(p_base, 'astro_cleaned shape: ', astro_cleaned.shape, '& New_fields3 shape: ', New_fields3.shape) # Amzar 9/9/2022 --> added new print statement to compare\n\n    # Fix HOUSE_NO that are converted to dat\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['ASTRO_HOUSE_NO1'] \n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"JAN-\",\"01-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-JAN\",\"-01\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"Jan-\",\"01-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-Jan\",\"-01\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"FEB-\",\"02-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-FEB\",'-02', case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"Feb-\",\"02-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-Feb\",\"-02\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"MAR-\",'03-', case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-MAR\",\"-03\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"Mar-\",'03-', case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-Mar\",\"-03\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"APR-\",\"04-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-APR\",\"-04\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"Apr-\",\"04-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-Apr\",\"-04\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"MAY-\",\"05-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-MAY\",\"-05\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"May-\",\"05-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-May\",\"-05\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"JUN-\",\"06-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-JUN\",\"-06\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"Jun-\",\"06-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-Jun\",\"-06\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"JUL-\",\"07-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-JUL\",\"-07\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"Jul-\",\"07-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-Jul\",\"-07\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"AUG-\",'08-', case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-AUG\",\"-08\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"Aug-\",'08-', case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-Aug\",\"-08\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"SEP-\",\"09-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-SEP\",\"-09\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"Sep-\",\"09-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-Sep\",\"-09\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"OCT-\",\"10-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-OCT\",\"-10\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"Oct-\",\"10-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-Oct\",\"-10\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"NOV-\",\"11-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-NOV\",\"-11\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"Nov-\",\"11-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-Nov\",\"-11\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"DEC-\",\"12-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-DEC\",\"-12\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"Dec-\",\"12-\", case = False)\n    astro_cleaned['HOUSE_NO'] = astro_cleaned['HOUSE_NO'].str.replace(\"-Dec\",\"-12\", case = False)\n    \n    # Fix HOUSE_NO that are converted to date (DD/MM/YYYY format)\t\n    # Filter date HOUSE_NO\t\n    date_house = astro_cleaned[astro_cleaned['HOUSE_NO'].str.match('^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$')== True]\n    # Spliting the HOUSE_NO\n    date_house.loc[:, 'block'] = (date_house.HOUSE_NO.str[0:2]) # Amzar 9/9/2022 --> added loc\n    date_house.loc[:, 'floor'] = (date_house.HOUSE_NO.str[3:5]) # Amzar 9/9/2022 --> added loc \n    date_house.loc[:, 'unit'] = (date_house.HOUSE_NO.str[8:10]) # Amzar 9/9/2022 --> added loc\n    # Combine the split HOUSE_NO with ...\n    date_house.loc[:, 'HOUSE_NO_ASTRO'] = date_house['block'] + \"-\" + date_house['floor'] + \"-\" + date_house['unit'] # Amzar 9/9/2022 --> added loc \n    # Filter not date HOUSE_NO\n    not_date_house = astro_cleaned[~(astro_cleaned['HOUSE_NO'].str.match('^([0-2][0-9]|(3)[0-1])(\\/)(((0)[0-9])|((1)[0-2]))(\\/)\\d{4}$')== True)]\n    not_date_house.loc[:, 'HOUSE_NO_ASTRO'] = not_date_house['HOUSE_NO'] # Amzar 9/9/2022 --> added loc \n    not_date_house.head()\n    # Append the 2 df again\n    frame = [date_house,not_date_house]\n    astro_cleaned = pd.concat(frame)\n    astro_cleaned.shape\n    \n    # Remove additional column created to combine HOUSE_NO\n    astro_cleaned = astro_cleaned.drop(['block','floor','unit'],axis=1)\n    astro_cleaned.info()\n    astro_cleaned['ASTRO_HOUSE_NO1']= astro_cleaned['HOUSE_NO_ASTRO'].str.pad(width=10)\n    #MDU NEW\n    astro_cleaned2 = astro_cleaned[['ACCOUNT_NO','service_add_objid', 'ASTRO_HOUSE_NO1', \n                                       'Combined_Building','Street_Type_1','Street_1_New','Street_Type_2',\n                                       'Street_2_New',  'AREA','STD_CITY' ,'POSTCODE', 'ASTRO_STATE',\n                                       'Standard_Building_Name',\n                                       'ServiceType', 'Servicable', 'HNUM_STRT_TM']]\n\n    UAMS_Base = astro_cleaned2.copy() # Amzar 9/9/2022 --> added copy() \n    print(p_base, 'UAMS_BASE df shape: ', UAMS_Base.shape, ' & astro_cleaned2 df shape: ', astro_cleaned2.shape) # Amzar 9/9/2022 --> added new print statement to compare\n    UAMS_Base.loc[:, 'ACCOUNT_NO'] = UAMS_Base.loc[:, 'ACCOUNT_NO'].astype(str) # Amzar 9/9/2022 --> added loc \n    UAMS_Base.loc[:, 'ACCOUNT_NO'] = UAMS_Base.loc[:, 'ACCOUNT_NO'].str.replace('\\.0','', case = False) # Amzar 9/9/2022 --> added loc                                 \n    print(p_base, 'UAMS_Base shape after converting ACC_NO col to str type: ', UAMS_Base.shape) # Amzar 9/9/2022 --> added more text to the print statement\n    \n    return UAMS_Base",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### -------------------------------- READING IN THE FILES for P1P2 MDU, to generate them into UAMS format (added on 14/12/22) --------------------------------\n\ndef p1p2_mdu_generate_uams(df_path, p_base):\n    \n    now = datetime.now()\n\n    # df = wr.s3.read_csv(path = df_path, compression='gzip', dtype=str, usecols = ['ACCOUNT_NO','service_add_objid','HOUSE_NO','Building_Coalesced', 'Street_1', 'Street_2', 'Standard_Building_Name', 'AREA_Coalesced','POSTCODE_Coalesced', 'CITY_Coalesced','STATE_Coalesced', 'joined_set', 'Source', 'MAPPED_HNUM_STRT_TM'])\n    df = spark.read.csv(df_path, header=True)\n    df = df.select(['ACCOUNT_NO','service_add_objid','HOUSE_NO','Building_Coalesced', 'Street_1', 'Street_2', 'Standard_Building_Name', 'AREA_Coalesced','POSTCODE_Coalesced', 'CITY_Coalesced','STATE_Coalesced', 'joined_set', 'Source', 'match', 'HNUM_STRT'])\n    # rename columns to fit the format in the function above (coz I defined it for just the SDU mapping originally)\n    df = df.toDF(*['ACCOUNT_NO','service_add_objid','ASTRO_HOUSE_NO1','Combined_Building', 'Street_1', 'Street_2', 'Standard_Building_Name', 'AREA','POSTCODE', 'STD_CITY','ASTRO_STATE', 'joined_set', 'Source', 'match', 'HNUM_STRT'])\n    \n    # For each p_base, create HNUM_STRT_TM in isp_corrected/tm_kv_clean_2\n    tm_kv_clean_2 = spark.read.orc(temporary_save_path+'{0}/TM_newMDUcleaningflow_1.orc'.format(curr_date))\n    if p_base == 'P1_MDU':\n        tm_kv_clean_2 = tm_kv_clean_2.withColumn( 'HNUM_STRT_TM', f.concat_ws(' ,', f.col(\"HOUSE_NO_TM\"), f.col(\"joined_set\"), f.col(\"STD_CITY\")) )\n    elif p_base == 'P2_MDU':\n        tm_kv_clean_2 = tm_kv_clean_2.withColumn( 'HNUM_STRT_TM', f.concat_ws(' ,', f.col(\"new_building_name\"), f.col(\"STD_CITY\")) )\n    else:\n        print('neither of P1_MDU or P2_MDU base was keyed in')\n        return\n    \n    # Clean, Strip, & Capitalize HNUM_STRT_TM\n    tm_kv_clean_2 = clean_HNUM_STRT(tm_kv_clean_2, 'HNUM_STRT_TM')\n    isp_corrected = tm_kv_clean_2.toPandas() # convert to Pandas as the generate_uams_format function was copied from a Pandas job ---> but I keep getting Error: Interpreter Died: ... so maybe I can't use Pandas?\n    \n    # use function defined above to generate UAMS format & get ServiceType - converted the df table to pandas coz my codes for the generate_uams_format function were copied from a Pandas script                                     \n    UAMS_MDU_Base = generate_uams_format(df.toPandas(), p_base)\n\n    # de-dupe & rename columns using pandas first\n    UAMS_MDU_Base = UAMS_MDU_Base.drop_duplicates(subset=['ACCOUNT_NO'], keep='first')\n    print(p_base, 'UAMS_MDU_Base shape AFTER dedupe on ACC_NO, keep first: ', UAMS_MDU_Base.shape) # Amzar 9/9/2022 --> added more text to the print statement\n    UAMS_MDU_Base = UAMS_MDU_Base.rename({'ASTRO_HOUSE_NO1':'House_No', 'ACCOUNT_NO': 'Account_No'}, axis=1)\n    \n    # convert the pandas DF back to Spark for saving\n    UAMS_MDU_Base = spark.createDataFrame(UAMS_MDU_Base)\n    \n    # save the file\n    UAMS_MDU_Base.write.csv(uams_mdu_path+'UAMS_Format_stndrd_' + str(ISP_Name)+ '_{0}.csv.gz'.format(p_base), mode='overwrite', compression='gzip', header=True)\n    UAMS_MDU_Base.write.csv(uams_mdu_path+'historical_folder/UAMS_Format_stndrd_' + str(ISP_Name)+ '_{0}_{1}.csv.gz'.format(p_base, curr_date), mode='overwrite', compression='gzip', header=True)\n        \n    end = datetime.now()\n    print('Time taken to run for ', str(p_base), str(end - now)) # 0:0\n    return\n\np1_mdu_path = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_mdu/P1_MDU.csv.gz'\np2_mdu_path = 's3://astro-groupdata-prod-pipeline/address_standardization/tm_uams_mdu/P2_MDU.csv.gz'\np1p2_mdu_generate_uams(p1_mdu_path, 'P1_MDU')\np1p2_mdu_generate_uams(p2_mdu_path, 'P2_MDU')\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Error: Interpreter died:\n\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Generating P1P2 MDU TM Distinct Files for RPA use case\n- this cell below seems to work",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "#### ----- P1 & P2 MDU TM Distinct Files\n\ndef prepare_distinct_files_pyspark(p_base):\n    \"\"\"For p_base, key in a string of either 'P1_MDU' or 'P2_MDU' \"\"\"\n    \n    ### codes COPIED from Step 3.0 P1P2 Mapping. Section: RPA Base generation - to be saved in RPA bucket -- takes about 30 sec to run\n    now = datetime.now()\n    rpa_isp_corrected = spark.read.orc(temporary_save_path+'{0}/TM_newMDUcleaningflow_1.orc'.format(curr_date))\n    print('starting rpa_isp_corrected count:', rpa_isp_corrected.count()) ## 3676163\n\n    ## create concatenated ISP_ADDRESS column\n    rpa_isp_corrected = rpa_isp_corrected.withColumn( 'ISP_ADDRESS', f.concat_ws(', ', f.col('BuildingName').cast('string'), f.col('HouseNo').cast('string'), f.col('StreetType').cast('string'), f.col('StreetName').cast('string'), f.col('Postcode').cast('string'), f.col('City').cast('string'), f.col('State').cast('string')) )\n\n    ## create padded house no column (avoids CSV turning this column into dates)\n    rpa_isp_corrected = rpa_isp_corrected.withColumn( 'TM_HOUSE_NO1', f.lpad(f.col('HouseNo').cast('string'), 10, ' ') )\n\n    ## de-dupe and keep first (but looking at the columns to de-dupe on, maybe the keep first is not necessary?)\n    New_Columns = rpa_isp_corrected.dropDuplicates( ['BuildingName','HouseNo','StreetType','Street_1','Postcode','STD_CITY','State'] )\n    print(\"After de-dupe on ['BuildingName','HouseNo','StreetType','Street_1','Postcode','STD_CITY','State']:\", New_Columns.count()) ## 3231452\n    \n    if p_base == 'P1_MDU':\n        ## Create Mapping Key using the new joined_set for P1 MDU\n        New_Columns = New_Columns.withColumn( 'HNUM_STRT_TM', f.concat_ws(' ,', f.col(\"HOUSE_NO_TM\"), f.col(\"joined_set\"), f.col(\"STD_CITY\")) )\n    elif p_base == 'P2_MDU':\n        ## Create Mapping Key using the new_building_name for P2 MDU\n        New_Columns = New_Columns.withColumn( 'HNUM_STRT_TM', f.concat_ws(' ,', f.col(\"new_building_name\"), f.col(\"STD_CITY\")) )\n    else:\n        print('invalid p_base inputted')\n        return\n\n    ## Use function above to capitalize & clean HNUM_STRT_TM column. Also, replace specific strings (eg. '[,.]' ) with empty string (\"\")\n    New_Columns = clean_HNUM_STRT(New_Columns, 'HNUM_STRT_TM')\n        \n    ## select relevant columns --> should I include joined_set, new_block_building_name? Maybe no need coz joined_set is similar to HNUM_STRT_TM and new_block_building_name is similar to BuildingName\n    distinct = New_Columns.select('HNUM_STRT_TM','TM_HOUSE_NO1','BuildingName', 'StreetType','StreetName', 'Section','Postcode','City', 'State','ServiceType', 'ISP_ADDRESS')\n    # OLD CODE: distinct_mdu_p1 = New_Columns[['HNUM_STRT_TM','TM_HOUSE_NO1','HouseNo','BuildingName','StreetType','StreetName','Section','Postcode','City','State','ServiceType', 'ISP_ADDRESS']]\n\n    ## create HOUSE_NO_TM column\n    distinct = distinct.withColumn('HOUSE_NO_TM', f.col('TM_HOUSE_NO1').cast('string'))\n    distinct = distinct.drop(*['TM_HOUSE_NO1'])\n\n    ## removing null BuildingName\n    distinct = distinct.filter(f.col('BuildingName').isNotNull())\n    print('After removing null BuildingName:', distinct.count()) ## 3022508\n\n    ## Zohreh created Mix_key column at some point to increase accuracy. But ignore for now (3/10/2022)\n    # distinct_mdu_p1[\"Mix_key\"] = distinct_mdu_p1[\"StreetName\"].map(str) + \" ,\" + distinct_mdu_p1[\"HNUM_STRT_TM\"].map(str)\n    # distinct_mdu_p1 = distinct_mdu_p1.drop_duplicates(subset=['Mix_key'], keep='first')\n\n    ## Rerrange columns\n    distinct = distinct.select('HOUSE_NO_TM', 'BuildingName', 'StreetType', 'StreetName', 'Section', 'Postcode', 'City', 'State', 'ServiceType', 'ISP_ADDRESS','HNUM_STRT_TM')\n\n    ## Save Distinct Files\n    distinct.write.csv(temporary_save_path+'{0}/Distinct_Fields_{1}.csv.gz'.format(curr_date, p_base), mode='overwrite', compression='gzip')\n    \n    end = datetime.now()\n    print(str(end - now)) # 0:00:27.770097\n    return\n    \nprepare_distinct_files_pyspark('P1_MDU')\nprepare_distinct_files_pyspark('P2_MDU')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "starting rpa_isp_corrected count: 3676163\nAfter de-dupe on ['BuildingName','HouseNo','StreetType','Street_1','Postcode','STD_CITY','State']: 3231452\nAfter removing null BuildingName: 3231452\n0:00:27.770097\nstarting rpa_isp_corrected count: 3676163\nAfter de-dupe on ['BuildingName','HouseNo','StreetType','Street_1','Postcode','STD_CITY','State']: 3231452\nAfter removing null BuildingName: 3231452\n0:00:20.194856\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}